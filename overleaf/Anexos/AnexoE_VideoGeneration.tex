\chapter{Video Generation: Segmentation and Inpainting}
\label{appendix:video_generation}

This appendix describes the complete video generation pipeline, including CLIP-guided video segmentation using SAM2's temporal tracking and text-driven video inpainting using WAN 2.1 VACE. While video processing was explored as an extension of the core image segmentation work, it demonstrates the flexibility of the CLIP+SAM2 approach for temporal data.

\section{Extension to Video Segmentation}

While the primary focus of this thesis is on image segmentation evaluated on PASCAL VOC 2012, the CLIP-guided prompting approach naturally extends to video segmentation by leveraging SAM2's native video tracking capabilities. This extension demonstrates that intelligent semantic guidance can be applied to temporal data while maintaining efficiency and requiring only first-frame analysis.

\subsection{SAM2 Video Architecture Overview}

SAM2 \cite{ravi2024sam2} extends the original Segment Anything Model to video by introducing a memory-based architecture for temporal consistency. The key innovation is a streaming memory mechanism that maintains object identity across frames without requiring per-frame segmentation. The system stores high-confidence predictions in memory banks and retrieves them when processing subsequent frames, enabling adaptation to appearance changes and handling of temporary occlusions.

\subsection{CLIP-Guided Video Segmentation Pipeline}

The video segmentation approach combines CLIP's semantic understanding for initial object detection with SAM2's temporal tracking. The pipeline operates in five stages:

\subsubsection{Stage 1: First Frame CLIP Analysis}

The first frame is extracted and processed by SCLIP with the user-provided vocabulary (e.g., ["horse", "rider", "motorcycle", "person"]). This produces pixel-wise class predictions and confidence scores. Critically, this is the \textit{only} time CLIP processes the video, making the approach computationally efficient compared to per-frame dense prediction.

\subsubsection{Stage 2: Semantic Prompt Extraction}

Using the same intelligent prompting strategy as the image pipeline, 50-300 semantic prompt points are extracted from high-confidence CLIP regions on frame 0. The system identifies connected components, filters by minimum size, and extracts centroid coordinates with associated class labels and confidence scores.

\subsubsection{Stage 3: SAM2 Initialization}

SAM2's video predictor is initialized with the Hiera-Large architecture. Each extracted prompt point is registered as a foreground point on frame 0, associating spatial locations with object classes for temporal tracking.

\subsubsection{Stage 4: Temporal Mask Propagation}

SAM2 propagates masks across the entire video using its memory-based tracking mechanism, involving frame encoding, memory retrieval, mask prediction, and memory updates.

\subsubsection{Stage 5: Video Rendering}

The final stage generates a visualization video by overlaying colored segmentation masks on original frames with 50\% transparency. Class labels are rendered at object centroids with high-contrast text. Frames are encoded using H.264 codec with YUV420p pixel format for universal playback compatibility.

\subsection{Key Implementation Features}

The implementation employs efficient memory management with CPU offloading for video frames and intermediate states, temporary file storage for rendered frames, and single-pass processing to minimize overhead.

The approach performs CLIP analysis only on frame 0, unlike per-frame methods. SAM2's memory-based tracking handles temporal propagation, making video segmentation as efficient as image segmentation plus encoding overhead. The implementation uses CLIP class indices as SAM2 object IDs, creating direct mapping between semantic classes and tracked objects suitable for class-level segmentation.

\subsection{Video Examples and Applications}

Video segmentation is demonstrated on sports footage (MotoGP racing, NBA basketball) where CLIP identifies specific riders/players by name (e.g., "Valentino Rossi", "Stephen Curry") and SAM2 tracks them consistently across frames. Example vocabularies include:

\begin{itemize}
    \item \textbf{MotoGP:} ["motorcycle", "rider", "track", "tire barrier", "Valentino Rossi"]
    \item \textbf{NBA:} ["basketball player", "basketball", "court", "hoop", "Stephen Curry", "LeBron James"]
\end{itemize}

The system successfully tracks these objects through camera motion, pose changes, and partial occlusions, demonstrating the robustness of SAM2's memory-based approach when initialized with semantically meaningful CLIP-guided prompts.

This video extension enables applications such as:
\begin{itemize}
    \item Automated sports highlights generation (tracking specific players/teams)
    \item Video surveillance (detecting and tracking people/vehicles by description)
    \item Content-based video retrieval (finding frames containing described objects)
    \item Video dataset annotation (semi-automated labeling with human verification)
\end{itemize}

\section{Video Inpainting with WAN 2.1 VACE}
\label{sec:appendix_vace_videogen}

While the CLIP-guided video segmentation pipeline generates high-quality segmentation masks across video frames, practical video editing applications require the ability to modify or remove detected objects. This section describes the integration of WAN 2.1 VACE \cite{vace2025} for text-driven video inpainting, enabling complete video editing workflows from segmentation to content generation.

\subsection{Integration Architecture}

The video inpainting pipeline extends the segmentation workflow with three additional stages:

\begin{enumerate}
    \item \textbf{Mask Video Generation:} Convert segmentation results to binary mask videos suitable for VACE input
    \item \textbf{Preview Video Generation:} Create preview videos with gray overlay on tracked regions for user verification
    \item \textbf{VACE Inference:} Apply WAN 2.1 VACE for text-guided inpainting of masked regions
\end{enumerate}

This modular design allows users to inspect segmentation quality before committing to computationally expensive inpainting operations.

\subsection{Mask Video Generation}

SAM2's video propagation produces frame-wise segmentation masks $\{M_t^{(i)}\}_{t=1}^{T}$ for each tracked object $i$ across $T$ frames. For video inpainting, these masks are converted to binary mask videos:

\subsubsection{Binary Mask Extraction}
For a target class specified by the user (e.g., ``person'', ``motorcycle''), all masks belonging to that class are combined into a unified binary mask per frame:

\begin{equation}
M_t^{\text{binary}}(x,y) = \begin{cases}
255 & \text{if } \exists i: c_i = c_{\text{target}} \text{ and } M_t^{(i)}(x,y) = 1 \\
0 & \text{otherwise}
\end{cases}
\end{equation}

where $c_i$ is the class index of object $i$, and $c_{\text{target}}$ is the user-specified target class to inpaint.

The binary mask frames are saved as PNG images and encoded into an MP4 video using FFmpeg with H.264 codec. This ensures compatibility with VACE's video input requirements while maintaining lossless mask boundaries through appropriate codec settings (CRF 23 for perceptually lossless quality).

\subsubsection{Dimension Handling}
SAM2 occasionally produces masks with unexpected dimensions $(1, H, W)$ or $(W, H)$ due to internal representation choices. The implementation includes robust dimension correction:

\begin{verbatim}
if len(mask.shape) == 3 and mask.shape[0] == 1:
    mask = mask[0]  # Remove leading dimension
if mask.shape == (width, height):
    mask = mask.T   # Transpose if dimensions swapped
\end{verbatim}

This preprocessing ensures all masks conform to the expected $(H, W)$ format before aggregation.

\subsection{Preview Video Generation}

Before running VACE inference (which can take several minutes), a preview video is generated to verify segmentation quality. This preview applies a gray overlay (RGB: 128, 128, 128) to tracked regions:

\begin{equation}
I'_t(x,y) = \begin{cases}
[128, 128, 128] & \text{if } M_t^{\text{binary}}(x,y) = 255 \\
I_t(x,y) & \text{otherwise}
\end{cases}
\end{equation}

where $I_t$ is the original frame and $I'_t$ is the preview frame. This gray overlay clearly indicates which regions will be inpainted, allowing users to abort if segmentation quality is insufficient.

\subsection{VACE Inference Configuration}

WAN 2.1 VACE is invoked with the following configuration optimized for high-end GPUs (NVIDIA RTX 4090 24GB VRAM):

\subsubsection{Model Selection}
The implementation uses \texttt{VACE-14B} (14 billion parameters) for high-quality inpainting results. The RTX 4090's 24GB VRAM enables full-resolution processing at 480p-720p with the larger model variant.

\subsubsection{Input Preparation}
VACE requires three inputs:
\begin{itemize}
    \item \textbf{Source video} (\texttt{src\_video}): The preview video with gray overlay, providing visual context about which regions to inpaint
    \item \textbf{Source mask} (\texttt{src\_mask}): Binary mask video indicating regions to fill
    \item \textbf{Text prompt} (\texttt{prompt}): Natural language description of desired content (e.g., ``empty road'', ``green grass field'')
\end{itemize}

\subsubsection{Key Hyperparameters}
VACE inference is configured with the following parameters based on empirical testing:

\begin{table}[h]
\centering
\begin{tabular}{lll}
\hline
\textbf{Parameter} & \textbf{Value} & \textbf{Purpose} \\
\hline
\texttt{frame\_num} & $\min(81, T)$ & VACE requires $4n+1$ frames \\
\texttt{size} & 480p & Resolution (480p/720p supported) \\
\texttt{sample\_solver} & unipc & Diffusion sampler (UniPC for speed) \\
\texttt{sample\_steps} & 50 & Denoising iterations \\
\texttt{sample\_shift} & 16 & Temporal shift for consistency \\
\texttt{sample\_guide\_scale} & 5.0 & Classifier-free guidance strength \\
\texttt{use\_prompt\_extend} & plain & Prompt engineering strategy \\
\texttt{base\_seed} & 2025 & Random seed for reproducibility \\
\hline
\end{tabular}
\caption{VACE hyperparameters for video inpainting. Values chosen to balance quality and computational efficiency on consumer hardware.}
\label{tab:vace_params_annex}
\end{table}

\textbf{Frame count constraint:} VACE's architecture requires $(4n+1)$ frames (e.g., 81, 85, 89). Videos longer than 81 frames are truncated to fit memory constraints, though future work could process longer videos in sliding windows.

\textbf{Sampling strategy:} UniPC sampler provides a good balance between quality and speed, requiring only 50 steps compared to 100+ steps for DDPM. The guidance scale of 5.0 ensures reasonable prompt adherence without over-saturation.

\subsection{Inference Execution}

VACE inference is invoked through the official Python API with directory handling to accommodate relative paths in the VACE codebase. This careful path management ensures VACE can locate its configuration files and model checkpoints while maintaining clean integration with the broader pipeline.

\subsection{Memory Management}

Video inpainting with diffusion models is memory-intensive, posing challenges on consumer GPUs. Several strategies mitigate memory constraints:

\subsubsection{Frame Offloading}
Frames are stored on disk as temporary PNGs rather than kept in GPU memory:

\begin{verbatim}
temp_dir = tempfile.mkdtemp()
for frame_idx in range(num_frames):
    cv2.imwrite(f"{temp_dir}/frame_{frame_idx:06d}.png", frame)
\end{verbatim}

This approach maintains efficient disk usage while ensuring all frames are accessible during VACE processing.

\subsubsection{Resolution Configuration}
The RTX 4090's 24GB VRAM enables processing at 480p-720p resolution. The implementation defaults to 480p for faster processing, with 720p available for higher quality output when needed.

\subsection{Output and Cleanup}

VACE generates the inpainted video as an MP4 file with H.264 encoding. Post-processing steps include:

\begin{enumerate}
    \item \textbf{Format verification:} Check that output video is playable using OpenCV
    \item \textbf{Frame count validation:} Ensure output contains expected number of frames
    \item \textbf{Temporary file cleanup:} Delete intermediate PNG frames and temporary directories
    \item \textbf{Preview retention:} Keep mask and preview videos for reference
\end{enumerate}

The complete pipeline produces three output videos:
\begin{itemize}
    \item \texttt{output\_mask\_<class>.mp4}: Binary mask video
    \item \texttt{output\_preview\_<class>.mp4}: Gray overlay preview
    \item \texttt{output\_inpainted\_<class>.mp4}: Final VACE-generated result
\end{itemize}

\subsection{Practical Considerations}

\subsubsection{Computational Cost}
VACE inference on VACE-1.3B takes approximately 3-8 minutes for 81 frames at 480p on consumer GPUs (GTX 1060), varying with prompt complexity and scene content. This makes interactive editing impractical, but acceptable for offline video processing workflows.

\subsubsection{Prompt Engineering}
VACE's inpainting quality is highly sensitive to prompt phrasing. Effective prompts are:
\begin{itemize}
    \item \textbf{Descriptive:} ``empty asphalt road with white lane markings'' vs. ``road''
    \item \textbf{Contextual:} ``green grass field under blue sky'' vs. ``grass''
    \item \textbf{Consistent:} Describe the \textit{desired result} after removal, not the removed object
\end{itemize}

Poor prompts (e.g., ``remove the person'') often produce artifacts as VACE interprets them literally rather than inferring background context.

\subsubsection{Limitations}
Current implementation limitations include:
\begin{itemize}
    \item \textbf{Length restriction:} 81-frame maximum per VACE batch
    \item \textbf{Single-pass processing:} No iterative refinement or multi-scale generation
    \item \textbf{No instance control:} Cannot selectively inpaint individual instances of a class (e.g., remove only one of three people)
\end{itemize}

\subsection{Complete Pipeline Integration}

The complete text-to-video-editing pipeline combines CLIP-guided segmentation with VACE inpainting:

\begin{enumerate}
    \item \textbf{User input:} ``Remove all motorcycles from the video''
    \item \textbf{CLIP analysis:} Detect ``motorcycle'' on frame 0 using SCLIP dense prediction
    \item \textbf{Prompt extraction:} Extract 5-10 centroid points for detected motorcycles
    \item \textbf{SAM2 tracking:} Propagate motorcycle masks across all frames
    \item \textbf{Mask generation:} Combine all motorcycle masks into binary mask video
    \item \textbf{VACE inpainting:} Generate inpainted video with prompt ``empty road''
\end{enumerate}

This zero-shot workflow requires no manual annotation, no training, and no frame-by-frame user interaction, demonstrating the power of composing foundation models (CLIP, SAM2, VACE) for complex video editing tasks.

\section{Summary}

This appendix demonstrates the extension of CLIP-guided segmentation to video processing, combining first-frame semantic analysis with temporal tracking and optional text-driven inpainting. While video processing was not the primary focus of this thesis (which concentrates on image segmentation evaluation on PASCAL VOC 2012), it validates the approach's applicability to temporal data and opens avenues for future work in zero-shot video editing.
