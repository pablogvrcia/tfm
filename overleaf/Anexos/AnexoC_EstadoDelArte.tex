\chapter{Extended State of the Art: Video Inpainting}
\label{appendix:sota_video}

This appendix provides a detailed review of the evolution of video inpainting methods, complementing the summary provided in Chapter \ref{ch:no_lineal}.

\section{Flow-Based Video Inpainting (2020-2022)}
Early deep learning approaches to video inpainting leveraged optical flow to propagate information from neighboring frames. STTN \cite{zeng2020learning} introduced spatio-temporal transformers that utilize attention mechanisms for implicit relation reasoning among embedded features, facilitating context propagation across frames. However, STTN operated independently on each frame without explicit motion modeling.

FGVC (Flow-Guided Video Completion) \cite{xu2019deep} deployed flow completion networks and propagation algorithms separately, pioneering the use of optical flow for guided inpainting. However, its per-pixel forward flow tracing lacked sub-pixel accuracy, leading to spatial misalignment and blurry results.

E2FGVI (End-to-End Flow-Guided Video Inpainting) \cite{li2022towards} addressed these limitations by proposing an end-to-end trainable framework that jointly optimizes flow completion and content hallucination. Presented at CVPR 2022, E2FGVI achieved state-of-the-art performance with processing speeds of 0.12 seconds per frame on 432×240 videos, nearly 15× faster than previous flow-based methods.

FGT (Flow-Guided Transformer) \cite{zhang2022fgt}, introduced at ECCV 2022, innovatively leverages motion discrepancy exposed by optical flows to instruct attention retrieval in transformers. FGT++ \cite{zhang2023fgt++}, released in 2023 as the journal extension, achieved significantly improved results by combining flow guidance with transformer-based generation.

\section{Hybrid Propagation and Transformer Methods (2023)}
ProPainter \cite{zhou2023propainter}, presented at ICCV 2023, marked a significant advancement by introducing dual-domain propagation that combines the advantages of image warping and feature propagation, exploiting global correspondences reliably. The method proposes a mask-guided sparse video Transformer that achieves high efficiency by discarding unnecessary and redundant tokens. ProPainter outperforms prior methods by a large margin of 1.46 dB in PSNR while maintaining appealing efficiency. This work represents the state-of-the-art in traditional (non-diffusion) video inpainting approaches.

For specialized scenarios, Deep Stereo Video Inpainting \cite{wu2023deep} (CVPR 2023) introduced SVINet, the first approach for stereo video inpainting utilizing deep convolutional neural networks. The key challenge addressed was maintaining stereo consistency between left and right views while performing temporal inpainting.

\section{Diffusion-Based Video Inpainting (2025)}
The emergence of diffusion models has revolutionized video inpainting in 2025, bringing generative capabilities previously limited to images into the video domain. These methods leverage the powerful priors learned by large-scale diffusion models to generate highly realistic and temporally coherent video content.

\subsection{DiffuEraser}
DiffuEraser \cite{diffueraser2025}, introduced in January 2025, adapts stable diffusion architectures to video inpainting by incorporating prior information to provide initialization and weak conditioning, which helps mitigate noisy artifacts and suppress hallucinations. Experimental results demonstrate that this method outperforms state-of-the-art techniques in both content completeness and temporal consistency while maintaining acceptable efficiency.

\subsection{VipDiff}
VipDiff \cite{vipdiff2025}, also released in January 2025, proposes a training-free framework for conditioning pre-trained image-level diffusion models to generate spatially-temporally coherent and diverse video inpainting results. This approach is particularly valuable as it leverages existing image diffusion models without requiring expensive video-specific training.

\subsection{DiTPainter}
DiTPainter \cite{ditpainter2025}, presented in April-May 2025, introduces an efficient diffusion transformer network designed specifically for video inpainting. Unlike previous methods that initialize from large pretrained models, DiTPainter is trained from scratch with a focus on video-specific architectures. The method can address videos with arbitrary lengths and can be applied to video decaptioning and video completion tasks with acceptable time costs. Experiments demonstrate that DiTPainter outperforms existing video inpainting algorithms with higher quality and better spatial-temporal consistency.

\subsection{EraserDiT}
EraserDiT \cite{eraserdit2025}, released in June 2025, synergistically combines the advantages of diffusion models and transformer architectures to maintain long-term temporal consistency while ensuring high-quality inpainting results. The integration of Diffusion Transformers (DiT) represents the cutting edge of video generation technology, addressing the blurring and temporal inconsistencies that plague earlier methods when dealing with large masks.

\section{All-in-One Video Creation and Editing: VACE}
VACE (Video All-in-One Creation and Editing) \cite{vace2025}, presented at ICCV 2025, represents a paradigm shift toward unified video generation frameworks. Developed by Alibaba's Tongyi Lab, VACE integrates multiple video tasks—including reference-to-video generation (R2V), video-to-video editing (V2V), and masked video-to-video editing (MV2V)—into a single all-in-one framework.

\subsection{Technical Architecture}
VACE introduces the Video Condition Unit (VCU), a unified interface that organizes diverse video task inputs (editing, reference, masking) into a consistent representation. Through a Context Adapter structure, the system injects different task concepts into the model using formalized representations of temporal and spatial dimensions, enabling flexible handling of arbitrary video synthesis tasks.

\subsection{WAN 2.1 VACE Implementation}
The WAN 2.1 VACE model is available in two variants: VACE-1.3B and VACE-14B, with the latter containing 14 billion parameters for fine granularity in video detail synthesis. The model supports resolutions from 480p to 720p and can process videos of varying lengths (typically up to 81 frames in the 4n+1 format required by the architecture).

Key capabilities include:
\begin{itemize}
    \item \textbf{Masked Video Inpainting (MV2V):} Intelligently fills missing regions in videos, removes unwanted objects, and achieves high-quality video content reconstruction with temporal consistency
    \item \textbf{Reference-Guided Generation (R2V):} Generates video sequences conditioned on reference images
    \item \textbf{Style Transfer (V2V):} Applies style transformations to existing videos while preserving motion
    \item \textbf{Text-Guided Editing:} Modifies video content based on natural language prompts
\end{itemize}

\subsection{Performance and Availability}
Extensive experiments demonstrate that VACE achieves performance on par with task-specific models across various subtasks, while offering the significant advantage of a unified framework. The model was released in May 2025 and is publicly available on HuggingFace and ModelScope, enabling researchers and practitioners to leverage state-of-the-art video inpainting capabilities.
