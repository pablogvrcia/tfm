\chapter{Technical Foundations: Vision Transformers, CLIP and SAM 2}
\label{appendix:technical_foundations}

This appendix provides detailed technical explanations of the foundational architectures used in this thesis: Vision Transformers (ViT) and CLIP. These explanations support the main methodology presented in Chapter 2 without distracting from the core contributions.

\section{Vision Transformer (ViT) Architecture}
\label{sec:appendix_vit}

The Vision Transformer \cite{dosovitskiy2020image} adapts the Transformer architecture \cite{vaswani2017attention} from NLP to computer vision by treating an image as a sequence of patches.

\subsection{Core Concepts}

\textbf{Core Idea:} Instead of processing images through convolutional layers, ViT:
\begin{enumerate}
    \item Divides the image into fixed-size patches
    \item Linearly embeds each patch
    \item Processes the patch sequence through standard Transformer layers
    \item Uses the output for classification or (in this thesis's case) dense prediction
\end{enumerate}

\textbf{Why this works:} Transformers excel at modeling long-range dependencies through self-attention. By treating spatial patches as sequence elements, ViT can capture relationships between distant image regions more effectively than CNNs with limited receptive fields.

\subsection{Self-Attention Mechanism}

Self-attention allows each element in a sequence to attend to all other elements, computing context-aware representations.

\textbf{Input:} Sequence of $N$ token embeddings $X \in \mathbb{R}^{N \times D}$

\textbf{Learnable Parameters:} Three weight matrices per attention head:
\begin{itemize}
    \item $W_Q \in \mathbb{R}^{D \times d_h}$ (Query projection)
    \item $W_K \in \mathbb{R}^{D \times d_h}$ (Key projection)
    \item $W_V \in \mathbb{R}^{D \times d_h}$ (Value projection)
\end{itemize}
where $d_h = D / H$ is the dimension per head ($H$ = number of heads, typically 8-16).

\textbf{Step-by-step computation:}

\textbf{(1) Project to Queries, Keys, Values:}
\begin{align}
Q &= X W_Q \in \mathbb{R}^{N \times d_h} \quad \text{(What am I looking for?)} \\
K &= X W_K \in \mathbb{R}^{N \times d_h} \quad \text{(What do I contain?)} \\
V &= X W_V \in \mathbb{R}^{N \times d_h} \quad \text{(What information do I provide?)}
\end{align}

\textbf{Intuition:}
\begin{itemize}
    \item Each token $i$ produces a \textit{query} $Q_i$: "What information am I seeking?"
    \item Each token $j$ produces a \textit{key} $K_j$: "What information do I offer?"
    \item Each token $j$ produces a \textit{value} $V_j$: "Here's my actual information"
\end{itemize}

\textbf{(2) Compute Attention Weights:}
\begin{equation}
A = \text{softmax}\left(\frac{Q K^T}{\sqrt{d_h}}\right) \in \mathbb{R}^{N \times N}
\end{equation}

This computes how much token $i$ should attend to token $j$:
\begin{equation}
A_{ij} = \frac{\exp(Q_i \cdot K_j / \sqrt{d_h})}{\sum_{k=1}^{N} \exp(Q_i \cdot K_k / \sqrt{d_h})}
\end{equation}

\textbf{Why $Q K^T$?} It measures compatibility: if token $i$'s query is similar to token $j$'s key (high dot product), then $j$ likely has relevant information for $i$.

\textbf{Why divide by $\sqrt{d_h}$?} Scaling factor prevents dot products from becoming too large in high dimensions, which would cause softmax to produce very peaked distributions (gradient saturation).

\textbf{(3) Weighted Aggregation:}
\begin{equation}
\text{Output} = A V \in \mathbb{R}^{N \times d_h}
\end{equation}

Each output token is a weighted sum of all value vectors:
\begin{equation}
\text{Output}_i = \sum_{j=1}^{N} A_{ij} V_j
\end{equation}

\subsection{Multi-Head Attention}

Instead of one attention mechanism, Transformers use $H$ parallel heads (e.g., $H=8$) to capture different types of relationships.

\textbf{Why multiple heads?} Different heads can specialize:
\begin{itemize}
    \item Head 1 might learn positional relationships ("nearby patches")
    \item Head 2 might learn color similarity
    \item Head 3 might learn semantic relationships ("all sky patches")
\end{itemize}

\textbf{Computation:}
\begin{align}
\text{head}_h &= \text{Attention}(X W_Q^h, X W_K^h, X W_V^h) \in \mathbb{R}^{N \times d_h} \\
\text{MultiHead}(X) &= \text{Concat}(\text{head}_1, \ldots, \text{head}_H) W_O \in \mathbb{R}^{N \times D}
\end{align}

where $W_O \in \mathbb{R}^{D \times D}$ is an output projection matrix.

\subsection{Complete Transformer Layer}

A full Transformer layer combines multi-head self-attention with position-wise feed-forward networks:

\textbf{(1) Multi-Head Self-Attention with Residual:}
\begin{align}
\hat{X} &= \text{LayerNorm}(X) \\
X' &= X + \text{MultiHead}(\hat{X})
\end{align}

\textbf{(2) Feed-Forward Network (MLP) with Residual:}
\begin{align}
\hat{X}' &= \text{LayerNorm}(X') \\
X_{\text{out}} &= X' + \text{MLP}(\hat{X}')
\end{align}

where MLP typically consists of two linear layers with GELU activation:
\begin{equation}
\text{MLP}(x) = W_2 \cdot \text{GELU}(W_1 x + b_1) + b_2
\end{equation}

\textbf{Why residual connections?} They allow gradients to flow directly through the network, enabling training of very deep models (12-24 layers for ViT).

\textbf{Why LayerNorm before (not after)?} Pre-normalization stabilizes training and is the modern standard.

\subsection{ViT-B/16 Complete Pipeline}

\textbf{Architecture specifications:}
\begin{itemize}
    \item Patch size: $P = 16$ pixels
    \item Embedding dimension: $D = 512$
    \item Number of layers: $L = 12$
    \item Number of heads: $H = 8$
    \item MLP hidden dimension: $2048$ (4Ã— expansion)
\end{itemize}

\textbf{Forward pass:}

\textbf{(a) Patch Embedding:}
\begin{equation}
X_{\text{patches}} = \text{LinearProjection}(\text{Flatten}(\text{Patches}(I))) \in \mathbb{R}^{N \times D}
\end{equation}
where $N = H_{\text{img}} W_{\text{img}} / P^2$ (e.g., $N = 224 \times 224 / 16^2 = 196$).

\textbf{(b) Prepend CLS Token:}
\begin{equation}
X_0 = [\text{CLS}; X_{\text{patches}}] \in \mathbb{R}^{(N+1) \times D}
\end{equation}

\textbf{(c) Add Position Embeddings:}
\begin{equation}
X_0 \leftarrow X_0 + E_{\text{pos}}
\end{equation}

\textbf{(d) Process through $L$ Transformer Layers:}
\begin{equation}
X_\ell = \text{TransformerLayer}_\ell(X_{\ell-1}) \quad \text{for } \ell = 1, \ldots, 12
\end{equation}

\textbf{(e) Extract Features:}
\begin{itemize}
    \item \textbf{For classification:} Use CLS token: $X_{12}[0, :]$
    \item \textbf{For dense prediction:} Use all patch tokens: $X_{12}[1:, :]$
\end{itemize}

\section{CLIP Architecture}

CLIP \cite{radford2021learning} trains a Vision Transformer and Text Transformer jointly to align image and text representations in a shared embedding space.

\subsection{Contrastive Learning Objective}

Given a batch of $B$ image-text pairs $(I_i, T_i)$:

\textbf{(1) Encode images and texts:}
\begin{align}
f_i &= \text{ViT}(I_i) \in \mathbb{R}^{D} \quad \text{(use CLS token)} \\
t_i &= \text{TextTransformer}(T_i) \in \mathbb{R}^{D}
\end{align}

\textbf{(2) Normalize:}
\begin{align}
f_i &\leftarrow f_i / \|f_i\|_2 \\
t_i &\leftarrow t_i / \|t_i\|_2
\end{align}

\textbf{(3) Compute similarity matrix:}
\begin{equation}
S_{ij} = f_i \cdot t_j \quad \in [-1, 1]
\end{equation}

\textbf{(4) Contrastive loss:} Maximize $S_{ii}$ (matching pairs) and minimize $S_{ij}$ for $i \neq j$ (non-matching pairs):
\begin{equation}
\mathcal{L} = -\frac{1}{B} \sum_{i=1}^{B} \left[ \log \frac{\exp(S_{ii}/\tau)}{\sum_{j=1}^{B} \exp(S_{ij}/\tau)} + \log \frac{\exp(S_{ii}/\tau)}{\sum_{j=1}^{B} \exp(S_{ji}/\tau)} \right]
\end{equation}

where $\tau$ is a learnable temperature parameter.

\subsection{Key Properties for Open-Vocabulary Segmentation}

After training on 400M image-text pairs, CLIP learns:
\begin{itemize}
    \item \textbf{Shared embedding space:} Both images and text map to the same 512-D space
    \item \textbf{Semantic understanding:} Learned on natural language descriptions, not just class labels
    \item \textbf{Zero-shot capability:} Can recognize unseen categories by comparing to text embeddings
\end{itemize}

This enables open-vocabulary segmentation without category-specific training.

\section{Segment Anything Model 2 (SAM 2) Architecture}
\label{sec:appendix_sam2}

SAM 2 \cite{ravi2024sam2} extends the original Segment Anything Model to video by introducing a memory mechanism for temporal consistency.

\subsection{Memory-Based Tracking Mechanism}

The core innovation of SAM 2 is its ability to maintain object states across frames using a memory bank. The propagation process involves:

\begin{enumerate}
    \item \textbf{Frame encoding:} Each frame $I_t$ is processed by the image encoder to produce visual features $\mathbf{f}_t$.

    \item \textbf{Memory retrieval:} For each tracked object, SAM 2 retrieves relevant memories from previous frames using cross-attention:
    \[
    \mathbf{m}_t = \text{Attention}(\mathbf{q}_t, \{\mathbf{k}_1, \ldots, \mathbf{k}_{t-1}\}, \{\mathbf{v}_1, \ldots, \mathbf{v}_{t-1}\})
    \]
    where $\mathbf{q}_t$ is the current frame query, and $\{\mathbf{k}_i, \mathbf{v}_i\}$ are memory key-value pairs from previous frames.

    \item \textbf{Mask prediction:} The mask decoder combines current features $\mathbf{f}_t$ and retrieved memories $\mathbf{m}_t$ to predict object masks:
    \[
    M_t^{(i)} = \text{Decoder}(\mathbf{f}_t, \mathbf{m}_t^{(i)})
    \]

    \item \textbf{Memory update:} High-confidence predictions are stored in the memory bank for future reference.
\end{enumerate}

This mechanism allows SAM 2 to track objects through occlusions and appearance changes without requiring per-frame re-initialization.
