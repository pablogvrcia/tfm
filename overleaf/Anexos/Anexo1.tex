\chapter{Technical Background: Vision Transformers and Self-Attention}
\label{appendix:technical_background}

This annex provides detailed technical background on Vision Transformers and self-attention mechanisms that underpin SCLIP and our enhancement framework. These details were moved from the main chapters for conciseness, as recommended by thesis advisors.

\section{Vision Transformer Architecture}

The Vision Transformer (ViT) \cite{dosovitskiy2020image} adapts the Transformer architecture from NLP to computer vision by treating images as sequences of patches.

\subsection{Core Concept}

Instead of convolutional layers, ViT:
\begin{enumerate}
\item Divides image into fixed-size patches (e.g., 16×16 pixels)
\item Linearly embeds each patch into a token
\item Processes the patch sequence through standard Transformer layers
\item Uses outputs for classification or (in our case) dense prediction
\end{enumerate}

\textbf{Why this works:} Transformers excel at modeling long-range dependencies through self-attention, capturing relationships between distant image regions more effectively than CNNs with limited receptive fields.

\subsection{Self-Attention Mechanism}

Self-attention allows each element in a sequence to attend to all other elements, computing context-aware representations.

\textbf{Input:} Sequence of $N$ token embeddings $X \in \mathbb{R}^{N \times D}$

\textbf{Learnable parameters per attention head:}
\begin{itemize}
\item $W_Q \in \mathbb{R}^{D \times d_h}$ (Query projection)
\item $W_K \in \mathbb{R}^{D \times d_h}$ (Key projection)
\item $W_V \in \mathbb{R}^{D \times d_h}$ (Value projection)
\end{itemize}
where $d_h = D / H$ is the dimension per head ($H$ = number of heads, typically 8-16).

\textbf{Step-by-step computation:}

\textbf{(1) Project to Queries, Keys, Values:}
\begin{align}
Q &= X W_Q \in \mathbb{R}^{N \times d_h} \quad \text{(What am I looking for?)} \\
K &= X W_K \in \mathbb{R}^{N \times d_h} \quad \text{(What do I contain?)} \\
V &= X W_V \in \mathbb{R}^{N \times d_h} \quad \text{(What information do I provide?)}
\end{align}

\textbf{Intuition:}
\begin{itemize}
\item Each token $i$ produces a \textit{query} $Q_i$: "What information am I seeking?"
\item Each token $j$ produces a \textit{key} $K_j$: "What information do I offer?"
\item Each token $j$ produces a \textit{value} $V_j$: "Here's my actual information"
\end{itemize}

\textbf{(2) Compute Attention Weights:}
\begin{equation}
A = \text{softmax}\left(\frac{Q K^T}{\sqrt{d_h}}\right) \in \mathbb{R}^{N \times N}
\end{equation}

This computes how much token $i$ should attend to token $j$:
\begin{equation}
A_{ij} = \frac{\exp(Q_i \cdot K_j / \sqrt{d_h})}{\sum_{k=1}^{N} \exp(Q_i \cdot K_k / \sqrt{d_h})}
\end{equation}

\textbf{Why $Q K^T$?} It measures compatibility: if token $i$'s query is similar to token $j$'s key (high dot product), then $j$ likely has relevant information for $i$.

\textbf{Why divide by $\sqrt{d_h}$?} Scaling factor prevents dot products from becoming too large in high dimensions, which would cause softmax to produce very peaked distributions (gradient saturation).

\textbf{(3) Weighted Aggregation:}
\begin{equation}
\text{Output} = A V \in \mathbb{R}^{N \times d_h}
\end{equation}

Each output token is a weighted sum of all value vectors:
\begin{equation}
\text{Output}_i = \sum_{j=1}^{N} A_{ij} V_j
\end{equation}

\textbf{Intuition:} Token $i$ aggregates information from all other tokens, weighted by attention scores. If $A_{i,j} = 0.7$ and $A_{i,k} = 0.3$, then output $i$ is 70\% influenced by token $j$ and 30\% by token $k$.

\subsection{Multi-Head Attention}

Instead of one attention mechanism, Transformers use $H$ parallel heads (e.g., $H=8$) to capture different relationship types.

\textbf{Why multiple heads?} Different heads can specialize:
\begin{itemize}
\item Head 1 might learn positional relationships ("nearby patches")
\item Head 2 might learn color similarity
\item Head 3 might learn semantic relationships ("all sky patches")
\end{itemize}

\textbf{Computation:}
\begin{align}
\text{head}_h &= \text{Attention}(X W_Q^h, X W_K^h, X W_V^h) \in \mathbb{R}^{N \times d_h} \\
\text{MultiHead}(X) &= \text{Concat}(\text{head}_1, \ldots, \text{head}_H) W_O \in \mathbb{R}^{N \times D}
\end{align}

where $W_O \in \mathbb{R}^{D \times D}$ is an output projection matrix.

\subsection{Complete Transformer Layer}

A full Transformer layer combines multi-head self-attention with position-wise feed-forward networks:

\textbf{(1) Multi-Head Self-Attention with Residual:}
\begin{align}
\hat{X} &= \text{LayerNorm}(X) \\
X' &= X + \text{MultiHead}(\hat{X})
\end{align}

\textbf{(2) Feed-Forward Network (MLP) with Residual:}
\begin{align}
\hat{X}' &= \text{LayerNorm}(X') \\
X_{\text{out}} &= X' + \text{MLP}(\hat{X}')
\end{align}

where MLP typically consists of two linear layers with GELU activation:
\begin{equation}
\text{MLP}(x) = W_2 \cdot \text{GELU}(W_1 x + b_1) + b_2
\end{equation}

\textbf{Why residual connections?} They allow gradients to flow directly through the network, enabling training of very deep models (12-24 layers for ViT).

\textbf{Why LayerNorm before (not after)?} Pre-normalization stabilizes training and is the modern standard (vs. original post-normalization).

\subsection{Vision Transformer (ViT) Complete Pipeline}

\textbf{Architecture: ViT-B/16 (used in CLIP and SCLIP)}
\begin{itemize}
\item Patch size: $P = 16$ pixels
\item Embedding dimension: $D = 512$
\item Number of layers: $L = 12$
\item Number of heads: $H = 8$
\item MLP hidden dimension: $2048$ (4× expansion)
\end{itemize}

\textbf{Complete forward pass:}

\textbf{(a) Patch Embedding:}
\begin{equation}
X_{\text{patches}} = \text{LinearProjection}(\text{Flatten}(\text{Patches}(I))) \in \mathbb{R}^{N \times D}
\end{equation}
where $N = H_{\text{img}} W_{\text{img}} / P^2$ (e.g., $N = 224 \times 224 / 16^2 = 196$).

\textbf{(b) Prepend CLS Token:}
\begin{equation}
X_0 = [\text{CLS}; X_{\text{patches}}] \in \mathbb{R}^{(N+1) \times D}
\end{equation}

The CLS (classification) token is a learnable embedding that aggregates global image information. For classification, only the CLS token output is used. For dense prediction (our case), we use all patch tokens.

\textbf{(c) Add Position Embeddings:}
\begin{equation}
X_0 \leftarrow X_0 + E_{\text{pos}}
\end{equation}

Position embeddings are learned vectors that encode spatial location, allowing the model to distinguish between patches at different positions.

\textbf{(d) Process through $L$ Transformer Layers:}
\begin{equation}
X_\ell = \text{TransformerLayer}_\ell(X_{\ell-1}) \quad \text{for } \ell = 1, \ldots, 12
\end{equation}

\textbf{(e) Extract Features:}
\begin{itemize}
\item \textbf{For classification:} Use CLS token: $X_{12}[0, :]$
\item \textbf{For dense prediction:} Use all patch tokens: $X_{12}[1:, :]$
\end{itemize}

\section{CLIP: Connecting Vision and Language}

CLIP \cite{radford2021learning} trains a Vision Transformer and Text Transformer jointly to align image and text representations in a shared embedding space.

\subsection{Training Objective (Contrastive Learning)}

Given a batch of $B$ image-text pairs $(I_i, T_i)$:

\textbf{(1) Encode images and texts:}
\begin{align}
f_i &= \text{ViT}(I_i) \in \mathbb{R}^{D} \quad \text{(use CLS token)} \\
t_i &= \text{TextTransformer}(T_i) \in \mathbb{R}^{D}
\end{align}

\textbf{(2) Normalize:}
\begin{align}
f_i &\leftarrow f_i / \|f_i\|_2 \\
t_i &\leftarrow t_i / \|t_i\|_2
\end{align}

\textbf{(3) Compute similarity matrix:}
\begin{equation}
S_{ij} = f_i \cdot t_j \quad \in [-1, 1]
\end{equation}

\textbf{(4) Contrastive loss:} Maximize $S_{ii}$ (matching pairs) and minimize $S_{ij}$ for $i \neq j$ (non-matching pairs):
\begin{equation}
\mathcal{L} = -\frac{1}{B} \sum_{i=1}^{B} \left[ \log \frac{\exp(S_{ii}/\tau)}{\sum_{j=1}^{B} \exp(S_{ij}/\tau)} + \log \frac{\exp(S_{ii}/\tau)}{\sum_{j=1}^{B} \exp(S_{ji}/\tau)} \right]
\end{equation}

where $\tau$ is a learnable temperature parameter.

\textbf{Result:} After training on 400M image-text pairs, CLIP learns:
\begin{itemize}
\item Images of "dogs" are close to text "a photo of a dog"
\item Images of "cars" are close to text "a car on the road"
\item Zero-shot transfer: Can classify/segment unseen categories by comparing to text embeddings
\end{itemize}

\subsection{Why CLIP Enables Open-Vocabulary Segmentation}

\begin{enumerate}
\item \textbf{Shared embedding space:} Both images and text map to the same 512-D space
\item \textbf{Semantic understanding:} Learned on natural language descriptions, not just class labels
\item \textbf{Zero-shot:} Given text "airplane", CLIP can recognize airplanes without airplane-specific training
\end{enumerate}

\section{SCLIP's Cross-layer Self-Attention Modification}

SCLIP modifies CLIP's final transformer layer to improve dense prediction quality:

\textbf{Standard attention:}
\begin{equation}
A_{\text{standard}} = \text{softmax}\left(\frac{QK^T}{\sqrt{d_h}}\right)
\end{equation}

\textbf{SCLIP's Cross-layer Self-Attention (CSA):}
\begin{equation}
A_{\text{CSA}} = \text{softmax}\left(\frac{QQ^T + KK^T}{\sqrt{d_h}}\right)
\end{equation}

\textbf{Why this works better for dense prediction:}

\begin{itemize}
\item \textbf{$QQ^T$:} Measures similarity between queries. If patch $i$ and patch $j$ have similar queries, they likely belong to the same semantic region (e.g., both are "sky"). This encourages \textit{spatial grouping}.

\item \textbf{$KK^T$:} Measures similarity between keys. Provides complementary structural information about which patches should be grouped together.

\item \textbf{Combined effect:} Patches in the same semantic region mutually reinforce each other through both query and key similarities, producing more spatially coherent features.

\item \textbf{Contrast with standard $QK^T$:} Standard attention measures query-key compatibility, which works well for global understanding (classification) but can create noisy, fragmented dense predictions.
\end{itemize}

\section{Implementation Notes}

\subsection{Computational Complexity}

\textbf{Standard self-attention:} $O(N^2 D)$ for $N$ tokens and $D$ dimensions

\textbf{SCLIP's CSA:} Same complexity, just different attention matrix computation

\subsection{Memory Requirements}

\textbf{Attention matrix:} $O(N^2)$ memory for $N \times N$ weights

For 14×14 patch grid ($N=196$): $196 \times 196 = 38,416$ attention weights per head

For 8 heads: $\sim$300KB per layer (float32)

\subsection{Gradients and Training}

SCLIP uses frozen CLIP weights, applying CSA modification only during inference. No gradient computation or backpropagation required, making it truly training-free.

\section{Summary}

This annex provided detailed technical background on:
\begin{itemize}
\item Vision Transformer architecture and self-attention mechanisms
\item Multi-head attention and its role in capturing diverse relationships
\item CLIP's contrastive learning objective and why it enables open-vocabulary tasks
\item SCLIP's Cross-layer Self-Attention modification for improved dense prediction
\end{itemize}

These foundations underpin all enhancements presented in the main thesis (Phases 1, 2A, 2B, 2C).
