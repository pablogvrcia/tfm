\chapter{Implementation Details and Code Repository}
\label{appendix:implementation}

This appendix provides comprehensive technical details about the implementation of our open-vocabulary semantic segmentation system, including software architecture, hyperparameter configurations, and reproducibility information.

\section{Software Environment}

Our implementation is built using the following core dependencies:

\begin{itemize}
    \item \textbf{Python:} 3.8 or higher
    \item \textbf{PyTorch:} 2.0.0+ with CUDA support
    \item \textbf{CLIP:} OpenAI CLIP (git+https://github.com/openai/CLIP.git)
    \item \textbf{SAM 2:} Meta's Segment Anything Model 2
    \item \textbf{Diffusers:} Hugging Face diffusers library (Stable Diffusion v2)
    \item \textbf{OpenCV:} Image processing utilities (cv2)
    \item \textbf{NumPy, PIL, tqdm:} Scientific computing and utilities
\end{itemize}

All experiments were conducted on an NVIDIA GeForce GTX 1060 6GB Max-Q mobile GPU. Due to the limited 6GB VRAM constraint, careful memory management and optimization were required. The dense SCLIP + SAM2 approach was adapted to work within this memory budget through FP16 precision, sequential processing, and model variant selection.

\section{Code Repository Structure}

The complete implementation is organized into modular components:

\subsection{Main Entry Points}

\begin{itemize}
    \item \texttt{main.py}: Unified CLI for both segmentation approaches
    \begin{itemize}
        \item Dense SCLIP + SAM2 refinement (default)
        \item Proposal-based SAM2+CLIP (--use-proposals flag)
        \item Supports segmentation, removal, replacement, and style transfer
    \end{itemize}

    \item \texttt{run\_sclip\_benchmarks.py}: Benchmark evaluation script
    \begin{itemize}
        \item PASCAL VOC 2012 evaluation
        \item COCO-Stuff 164K evaluation
        \item Configurable SCLIP modes (dense, hybrid with SAM)
        \item Metrics computation and visualization
    \end{itemize}

    \item \texttt{pipeline.py}: Proposal-based segmentation pipeline
    \begin{itemize}
        \item OpenVocabSegmentationPipeline class
        \item SAM2 automatic mask generation
        \item CLIP-based mask scoring
        \item Multi-scale voting and adaptive selection
    \end{itemize}
\end{itemize}

\subsection{Core Models}

\textbf{models/sclip\_segmentor.py} - Dense SCLIP + SAM2 Implementation
\begin{itemize}
    \item SCLIPSegmentor class (primary contribution)
    \item Cross-layer Self-Attention (CSA) feature extraction
    \item Dense semantic prediction
    \item Prompted SAM2 refinement with majority voting
    \item Unified \texttt{segment()} interface
\end{itemize}

\textbf{models/sclip\_features.py} - SCLIP Feature Extractor
\begin{itemize}
    \item SCLIPFeatureExtractor class
    \item Multi-layer feature aggregation (layers 6, 12, 18, 24)
    \item CSA attention mechanism implementation
    \item Sliding window inference support
\end{itemize}

\textbf{models/sam2\_segmentation.py} - SAM2 Integration
\begin{itemize}
    \item SAM2MaskGenerator class
    \item Automatic and prompted mask generation modes
    \item Point prompt extraction from dense predictions
    \item Mask quality filtering (IoU, stability scores)
\end{itemize}

\textbf{models/clip\_features.py} - Standard CLIP Features
\begin{itemize}
    \item CLIPFeatureExtractor class (for proposal-based approach)
    \item Multi-scale feature extraction
    \item Text encoding with prompt ensembling
\end{itemize}

\textbf{models/mask\_alignment.py} - Mask-Text Alignment
\begin{itemize}
    \item MaskTextAligner class
    \item Multi-scale CLIP voting (224px, 336px, 512px)
    \item Background suppression
    \item Confidence scoring
\end{itemize}

\textbf{models/adaptive\_selection.py} - Intelligent Mask Selection
\begin{itemize}
    \item AdaptiveMaskSelector class
    \item Query-aware selection strategies
    \item Single object vs. multiple instance detection
    \item Part-based segmentation handling
\end{itemize}

\textbf{models/inpainting.py} - Generative Editing
\begin{itemize}
    \item StableDiffusionInpainter class
    \item Object removal, replacement, style transfer
    \item Mask dilation and blending
    \item Side-by-side comparison generation
\end{itemize}

\textbf{models/pamr.py} - Post-processing Refinement
\begin{itemize}
    \item Pixel-Adaptive Mask Refinement (PAMR)
    \item Optional boundary improvement (disabled by default)
\end{itemize}

\subsection{Supporting Modules}

\begin{itemize}
    \item \texttt{datasets.py}: COCOStuffDataset, PASCALVOCDataset loaders
    \item \texttt{benchmarks/metrics.py}: mIoU, pixel accuracy, per-class metrics
    \item \texttt{config.py}: Pipeline configuration dataclasses
    \item \texttt{utils.py}: Image I/O, visualization, timing utilities
\end{itemize}

\section{Model Configurations}

\subsection{Dense SCLIP Prediction}

\begin{itemize}
    \item \textbf{Backbone:} ViT-B/16 (Vision Transformer with 16×16 patch size)
    \item \textbf{Feature layers:} 6, 12, 18, 24 (multi-scale extraction)
    \item \textbf{Layer weights:} Equal weighting (0.25 each)
    \item \textbf{Patch size:} 16 pixels
    \item \textbf{Input resolution:} 224×224 for CLIP, upsampled to original
    \item \textbf{Text encoding:} Prompt ensembling with template variations
    \item \textbf{Temperature scaling:} $T = 0.01$ (logit sharpening)
    \item \textbf{Sliding window:} Crop size 224, stride 112 (50\% overlap)
\end{itemize}

\subsection{SAM2 Refinement Parameters}

\textbf{Prompted Segmentation Mode (Our Contribution):}
\begin{itemize}
    \item \textbf{Model variant:} SAM2-base (optimized for 6GB VRAM)
    \item \textbf{Point extraction:} Connected component analysis on SCLIP predictions
    \item \textbf{Points per class:} Adaptive (typically 5-15 based on region size)
    \item \textbf{Minimum spatial separation:} 20 pixels between prompt points
    \item \textbf{Masks per point:} 3 (tight, medium, loose granularities)
    \item \textbf{Majority voting threshold:} 60\% coverage for mask retention
    \item \textbf{Speedup:} 2× faster than automatic mode ($\sim$60 vs 2,304 points)
\end{itemize}

\textbf{Automatic Mode (Proposal-based Approach):}
\begin{itemize}
    \item \textbf{Model variant:} SAM2-hiera-base-plus
    \item \textbf{Points per side:} 32 (32×32 grid = 1,024 points)
    \item \textbf{Predicted IoU threshold:} 0.88
    \item \textbf{Stability score threshold:} 0.95
    \item \textbf{Typical output:} 100-300 mask candidates per image
\end{itemize}

\subsection{Multi-Scale CLIP Voting (Proposal-based)}

\begin{itemize}
    \item \textbf{Resolutions:} 224px, 336px, 512px
    \item \textbf{Weights:} 0.2, 0.5, 0.3 respectively (336px prioritized)
    \item \textbf{Background suppression:} $\alpha = 0.3$
    \item \textbf{Score threshold:} 0.15-0.20 (adaptive based on mask size)
    \item \textbf{IoU threshold for NMS:} 0.70 (avoid duplicate masks)
\end{itemize}

\subsection{Stable Diffusion Inpainting}

\begin{itemize}
    \item \textbf{Model:} stabilityai/stable-diffusion-2-inpainting
    \item \textbf{Inference steps:} 50
    \item \textbf{Guidance scale:} 7.5
    \item \textbf{Mask dilation:} 5-10 pixels for smooth blending
    \item \textbf{Strength:} 0.8-1.0 depending on editing task
    \item \textbf{Negative prompts:} "blurry, low quality, distorted, artifacts"
\end{itemize}

\section{Evaluation Benchmarks}

\subsection{PASCAL VOC 2012}

\begin{itemize}
    \item \textbf{Classes:} 20 object categories + background (21 total)
    \item \textbf{Split:} Validation set (1,449 images)
    \item \textbf{Metric:} Mean Intersection-over-Union (mIoU)
    \item \textbf{Image resolution:} Variable, resized to max dimension 512px
    \item \textbf{Evaluation protocol:} Dense prediction, no post-processing
\end{itemize}

\subsection{COCO-Stuff 164K}

\begin{itemize}
    \item \textbf{Classes:} 171 categories (80 things + 91 stuff)
    \item \textbf{Split:} Validation set (5,000 images)
    \item \textbf{Metric:} Mean Intersection-over-Union (mIoU)
    \item \textbf{Image resolution:} Variable, resized to max dimension 512px
    \item \textbf{Challenge:} High class count, diverse stuff classes
\end{itemize}

\section{Computational Performance}

Inference timing on NVIDIA GeForce GTX 1060 6GB Max-Q:

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Method} & \textbf{Time per Image} & \textbf{GPU Memory} \\
\hline
Dense SCLIP only & $\sim$8-10s & 4.5 GB \\
SCLIP + Prompted SAM2 (ours) & $\sim$25-35s & 5.8 GB \\
Proposal-based (SAM2+CLIP) & $\sim$15-20s & 5.2 GB \\
\hline
\end{tabular}
\caption{Inference performance comparison for 512×512 images on GTX 1060 6GB Max-Q. The mobile GPU shows 2-3× slower performance compared to datacenter GPUs due to lower compute capability and memory bandwidth.}
\label{tab:performance}
\end{table}

\subsection{Performance Breakdown (Dense SCLIP + SAM2)}

\begin{itemize}
    \item \textbf{SCLIP CSA feature extraction:} $\sim$8-10s
    \begin{itemize}
        \item Multi-layer extraction (4 layers)
        \item Sliding window inference
        \item Similarity computation
    \end{itemize}
    \item \textbf{Prompted SAM2 refinement:} $\sim$15-25s
    \begin{itemize}
        \item Point extraction from SCLIP predictions
        \item SAM2 mask generation ($\sim$60 prompts)
        \item Majority voting and combination
    \end{itemize}
    \item \textbf{Total:} $\sim$25-35s per image
\end{itemize}

\subsection{Memory Optimization Strategies}

To enable operation within 6GB VRAM, we employed:

\begin{itemize}
    \item \textbf{FP16 precision:} Reduces memory by $\sim$50\% with minimal accuracy loss
    \item \textbf{Sequential processing:} Process SAM2 point prompts in batches
    \item \textbf{Model variant selection:} SAM2-base instead of SAM2-large
    \item \textbf{Resolution limits:} Cap maximum input to 512×512 pixels
    \item \textbf{Gradient-free inference:} torch.no\_grad() throughout pipeline
    \item \textbf{Cache clearing:} Explicit CUDA cache cleanup between stages
\end{itemize}

These optimizations enable full pipeline execution on consumer-grade hardware, making the approach accessible for researchers without access to high-end GPUs.

\section{Usage Examples}

\subsection{Basic Segmentation}

\begin{verbatim}
# Dense SCLIP + SAM2 (IMPORTANT: Use --vocabulary for context)
python main.py --image photo.jpg --prompt "car" \
    --vocabulary background sky road building tree --mode segment -v

# Without vocabulary (may over-segment - forces binary classification)
python main.py --image photo.jpg --prompt "car" --mode segment -v

# Proposal-based (alternative, doesn't need vocabulary)
python main.py --image photo.jpg --prompt "car" --use-proposals --mode segment -v
\end{verbatim}

\subsection{Generative Editing}

\begin{verbatim}
# Object removal
python main.py --image photo.jpg --prompt "car" --mode remove -v

# Object replacement
python main.py --image photo.jpg --prompt "car" --mode replace \
    --edit "red sports car" -v

# Style transfer
python main.py --image landscape.jpg --prompt "sky" --mode style \
    --edit "dramatic sunset" --vocabulary sky clouds ocean -v
\end{verbatim}

\subsection{Benchmark Evaluation}

\begin{verbatim}
# PASCAL VOC evaluation (dense SCLIP + SAM2)
python run_benchmarks.py --dataset pascal-voc --num-samples 100

# COCO-Stuff evaluation (dense SCLIP only, faster)
python run_benchmarks.py --dataset coco-stuff --num-samples 50 \
    --use-sam=False

# Full COCO-Stuff with SAM refinement
python run_benchmarks.py --dataset coco-stuff --use-sam
\end{verbatim}

\section{Reproducibility}

All experiments can be reproduced using the provided scripts and default hyperparameter settings. The code repository includes:

\begin{itemize}
    \item \textbf{Deterministic operations:} Fixed random seeds where applicable
    \item \textbf{Configuration files:} Default settings in \texttt{config.py}
    \item \textbf{Pretrained models:} Downloaded automatically on first run
    \item \textbf{Benchmark scripts:} Automated evaluation on standard datasets
    \item \textbf{Documentation:} Comprehensive README and inline comments
\end{itemize}

\subsection{Hardware Requirements}

\textbf{Minimum:}
\begin{itemize}
    \item GPU: 6GB VRAM (tested on GTX 1060 Max-Q)
    \item RAM: 16GB system memory
    \item Storage: 20GB for models and datasets
\end{itemize}

\textbf{Recommended:}
\begin{itemize}
    \item GPU: 12GB+ VRAM (RTX 3080, A5000)
    \item RAM: 32GB system memory
    \item Storage: 50GB for full datasets and checkpoints
\end{itemize}

\section{Key Implementation Insights}

\subsection{Prompted SAM2 Strategy}

Our novel prompted SAM2 approach extracts point prompts from SCLIP's dense predictions using connected component analysis:

\begin{enumerate}
    \item Apply morphological operations to SCLIP predictions
    \item Extract centroids from each connected region
    \item Enforce minimum 20-pixel separation between points
    \item Generate 3 masks per point at different granularities
    \item Filter masks via majority voting (60\% threshold)
    \item Combine retained masks with logical OR
\end{enumerate}

This achieves 2× speedup over automatic mask generation while maintaining segmentation quality.

\subsection{Text Feature Caching}

Since text embeddings are constant across all images in a dataset:
\begin{itemize}
    \item Pre-compute embeddings once per evaluation run
    \item Store in memory (171 classes × 512 dims $\approx$ 350KB)
    \item Reuse cached embeddings for all images
    \item \textbf{Speedup:} 41\% on subsequent images
\end{itemize}

\subsection{Sliding Window Inference}

For images larger than CLIP's native resolution:
\begin{itemize}
    \item Divide image into 224×224 overlapping crops
    \item Stride of 112 pixels (50\% overlap)
    \item Process each crop independently
    \item Blend overlapping regions via averaging
    \item \textbf{Benefit:} Better boundary quality on high-res images
\end{itemize}

\section{Future Optimization Opportunities}

\begin{itemize}
    \item \textbf{Model quantization:} INT8 quantization for 4× memory reduction
    \item \textbf{TensorRT optimization:} Faster inference on NVIDIA GPUs
    \item \textbf{Batched processing:} Process multiple images simultaneously
    \item \textbf{Distillation:} Smaller student models for mobile deployment
    \item \textbf{Pruning:} Remove redundant parameters from ViT backbone
\end{itemize}

These optimizations could enable real-time performance (>10 FPS) on modern hardware while maintaining accuracy.
