\chapter{Implementation Details and Code Repository (INCOMPLETE \& OUTDATED)}
\label{appendix:implementation}

This appendix provides comprehensive technical details about the implementation of our open-vocabulary semantic segmentation system, including software architecture, hyperparameter configurations, and reproducibility information.

\section{Software Environment}

Our implementation is built using the following core dependencies:

\begin{itemize}
    \item \textbf{Python:} 3.8 or higher
    \item \textbf{PyTorch:} 2.0.0+ with CUDA support
    \item \textbf{CLIP:} OpenAI CLIP (git+https://github.com/openai/CLIP.git)
    \item \textbf{SAM 2:} Meta's Segment Anything Model 2
    \item \textbf{Diffusers:} Hugging Face diffusers library (Stable Diffusion v2)
    \item \textbf{OpenCV:} Image processing utilities (cv2)
    \item \textbf{NumPy, PIL, tqdm:} Scientific computing and utilities
\end{itemize}

All experiments were conducted on an NVIDIA GeForce GTX 1060 6GB Max-Q mobile GPU. Due to the limited 6GB VRAM constraint, careful memory management and optimization were required. The dense SCLIP + SAM2 approach was adapted to work within this memory budget through FP16 precision, sequential processing, and model variant selection.

\section{Code Repository Structure}

The complete implementation is organized into modular components:

\subsection{Main Entry Points}

\begin{itemize}
    \item \texttt{clip\_guided\_segmentation.py}: CLIP-guided prompting implementation
    \begin{itemize}
        \item Intelligent prompt extraction from CLIP predictions
        \item SAM2 segmentation at semantic points
        \item Direct class assignment
        \item Supports segmentation, removal, replacement, and style transfer
    \end{itemize}

    \item \texttt{run\_benchmarks.py}: Benchmark evaluation script
    \begin{itemize}
        \item PASCAL VOC 2012 evaluation (completed)
        \item COCO-Stuff 164K evaluation (infrastructure ready, not evaluated in this thesis)
        \item Configurable SCLIP modes (dense, CLIP-guided with SAM)
        \item Metrics computation and visualization
    \end{itemize}
\end{itemize}

\subsection{Core Models}

\textbf{models/sclip\_features.py} - SCLIP Feature Extractor
\begin{itemize}
    \item SCLIPFeatureExtractor class
    \item Cross-layer Self-Attention (CSA) mechanism on final transformer layer
    \item Dense patch feature extraction
    \item Sliding window inference support
    \item Dense semantic prediction
\end{itemize}

\textbf{models/sam2\_predictor.py} - SAM2 Integration
\begin{itemize}
    \item SAM2 predictor wrapper
    \item Prompted segmentation at intelligent points
    \item Efficient batch processing
    \item High-quality mask generation
\end{itemize}

\textbf{models/inpainting.py} - Generative Editing
\begin{itemize}
    \item StableDiffusionInpainter class
    \item Object removal, replacement, style transfer
    \item Mask dilation and blending
    \item Side-by-side comparison generation
\end{itemize}

\textbf{models/pamr.py} - Post-processing Refinement
\begin{itemize}
    \item Pixel-Adaptive Mask Refinement (PAMR)
    \item Optional boundary improvement (disabled by default)
\end{itemize}

\subsection{Supporting Modules}

\begin{itemize}
    \item \texttt{datasets.py}: COCOStuffDataset, PASCALVOCDataset loaders
    \item \texttt{benchmarks/metrics.py}: mIoU, pixel accuracy, per-class metrics
    \item \texttt{config.py}: Pipeline configuration dataclasses
    \item \texttt{utils.py}: Image I/O, visualization, timing utilities
\end{itemize}

\section{Model Configurations}

\subsection{Dense SCLIP Prediction}

\begin{itemize}
    \item \textbf{Backbone:} ViT-B/16 (Vision Transformer with 16×16 patch size)
    \item \textbf{CSA mechanism:} Applied to final transformer layer
    \item \textbf{Patch size:} 16 pixels (14×14 grid for 224×224 input)
    \item \textbf{Feature dimension:} 512 (ViT-B/16 output)
    \item \textbf{Input resolution:} 224×224 for CLIP crops, processed at 2048px via sliding window
    \item \textbf{Text encoding:} Prompt ensembling with template variations
    \item \textbf{Temperature scaling:} $T = 40.0$ (logit scaling)
    \item \textbf{Sliding window:} Crop size 224, stride 112 (50\% overlap)
\end{itemize}

\subsection{CLIP-Guided Prompting Parameters}

\textbf{Intelligent Prompt Extraction:}
\begin{itemize}
    \item \textbf{Model variant:} SAM2-hiera-large
    \item \textbf{Point extraction:} Connected component analysis on SCLIP predictions
    \item \textbf{Confidence threshold:} 0.3 (filter low-confidence regions)
    \item \textbf{Minimum region size:} 100 pixels
    \item \textbf{Typical prompts per image:} 50-300 (vs 4096 for blind grid)
    \item \textbf{Efficiency gain:} 96\% reduction in prompts
    \item \textbf{Class assignment:} Direct from CLIP prediction at prompt point
\end{itemize}

\subsection{Stable Diffusion Inpainting}

\begin{itemize}
    \item \textbf{Model:} stabilityai/stable-diffusion-2-inpainting
    \item \textbf{Inference steps:} 50
    \item \textbf{Guidance scale:} 7.5
    \item \textbf{Mask dilation:} 5-10 pixels for smooth blending
    \item \textbf{Strength:} 0.8-1.0 depending on editing task
    \item \textbf{Negative prompts:} "blurry, low quality, distorted, artifacts"
\end{itemize}

\section{Evaluation Benchmarks}

\subsection{PASCAL VOC 2012}

\begin{itemize}
    \item \textbf{Classes:} 20 object categories + background (21 total)
    \item \textbf{Split:} Validation set (1,449 images)
    \item \textbf{Metric:} Mean Intersection-over-Union (mIoU)
    \item \textbf{Image resolution:} Variable, resized to max dimension 512px
    \item \textbf{Evaluation protocol:} Dense prediction, no post-processing
\end{itemize}

\subsection{COCO-Stuff 164K}

\textit{Note: Dataset infrastructure is implemented and ready, but COCO-Stuff 164K evaluation was not completed in this thesis. Future work will extend evaluation to this challenging dataset with 171 categories.}

\begin{itemize}
    \item \textbf{Classes:} 171 categories (80 things + 91 stuff)
    \item \textbf{Split:} Validation set (5,000 images)
    \item \textbf{Metric:} Mean Intersection-over-Union (mIoU)
    \item \textbf{Image resolution:} Variable, resized to max dimension 512px
    \item \textbf{Challenge:} High class count, diverse stuff classes
    \item \textbf{Status:} \textit{Not evaluated in this thesis (planned future work)}
\end{itemize}

\section{Computational Performance}

Inference timing on NVIDIA GeForce GTX 1060 6GB Max-Q:

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|}
\hline
\textbf{Method} & \textbf{Time per Image} & \textbf{GPU Memory} \\
\hline
Dense SCLIP only & $\sim$8-10s & 4.5 GB \\
CLIP-Guided Prompting (ours) & $\sim$12-33s & 5.8 GB \\
\hline
\end{tabular}
\caption{Inference performance for 512×512 images on GTX 1060 6GB Max-Q. The mobile GPU shows 2-3× slower performance compared to datacenter GPUs due to lower compute capability and memory bandwidth.}
\label{tab:performance}
\end{table}

\subsection{Performance Breakdown (CLIP-Guided Prompting)}

\begin{itemize}
    \item \textbf{SCLIP dense prediction:} $\sim$1-2s
    \begin{itemize}
        \item CSA attention computation on final layer
        \item Dense patch feature extraction
        \item Pixel-wise similarity computation
    \end{itemize}
    \item \textbf{Intelligent prompt extraction:} $<$0.1s
    \begin{itemize}
        \item Connected component analysis
        \item Confidence and size filtering
        \item Centroid computation
    \end{itemize}
    \item \textbf{SAM2 segmentation:} $\sim$10-30s
    \begin{itemize}
        \item Depends on number of prompts (50-300)
        \item High-quality mask generation per prompt
        \item Direct class assignment
    \end{itemize}
    \item \textbf{Overlap resolution:} $<$0.5s
    \item \textbf{Total:} $\sim$12-33s per image
\end{itemize}

\subsection{Memory Optimization Strategies}

To enable operation within 6GB VRAM, we employed:

\begin{itemize}
    \item \textbf{FP16 precision:} Reduces memory by $\sim$50\% with minimal accuracy loss
    \item \textbf{Sequential processing:} Process SAM2 point prompts in batches
    \item \textbf{Model variant selection:} SAM2-base instead of SAM2-large
    \item \textbf{Resolution limits:} Cap maximum input to 512×512 pixels
    \item \textbf{Gradient-free inference:} torch.no\_grad() throughout pipeline
    \item \textbf{Cache clearing:} Explicit CUDA cache cleanup between stages
\end{itemize}

These optimizations enable full pipeline execution on consumer-grade hardware, making the approach accessible for researchers without access to high-end GPUs.

\section{Usage Examples}

\subsection{Basic Segmentation}

\begin{verbatim}
# CLIP-guided prompting (provide full vocabulary for best results)
python clip_guided_segmentation.py --image photo.jpg \
    --vocabulary "car" "road" "sky" "building" "tree" "background"

# Segment specific object
python clip_guided_segmentation.py --image photo.jpg \
    --vocabulary "person" "background"
\end{verbatim}

\subsection{Generative Editing}

\begin{verbatim}
# Object removal
python main.py --image photo.jpg --prompt "car" --mode remove -v

# Object replacement
python main.py --image photo.jpg --prompt "car" --mode replace \
    --edit "red sports car" -v

# Style transfer
python main.py --image landscape.jpg --prompt "sky" --mode style \
    --edit "dramatic sunset" --vocabulary sky clouds ocean -v
\end{verbatim}

\subsection{Benchmark Evaluation}

\begin{verbatim}
# PASCAL VOC evaluation (CLIP-Guided Prompting with SAM2)
python run_benchmarks.py --dataset pascal-voc --num-samples 100

# COCO-Stuff infrastructure (prepared for future evaluation)
# Note: Not evaluated in this thesis
# python run_benchmarks.py --dataset coco-stuff --num-samples 50
\end{verbatim}

\section{Reproducibility}

All experiments can be reproduced using the provided scripts and default hyperparameter settings. The code repository includes:

\begin{itemize}
    \item \textbf{Deterministic operations:} Fixed random seeds where applicable
    \item \textbf{Configuration files:} Default settings in \texttt{config.py}
    \item \textbf{Pretrained models:} Downloaded automatically on first run
    \item \textbf{Benchmark scripts:} Automated evaluation on standard datasets
    \item \textbf{Documentation:} Comprehensive README and inline comments
\end{itemize}

\subsection{Hardware Requirements}

\textbf{Minimum:}
\begin{itemize}
    \item GPU: 6GB VRAM (tested on GTX 1060 Max-Q)
    \item RAM: 16GB system memory
    \item Storage: 20GB for models and datasets
\end{itemize}

\textbf{Recommended:}
\begin{itemize}
    \item GPU: 12GB+ VRAM (RTX 3080, A5000)
    \item RAM: 32GB system memory
    \item Storage: 50GB for full datasets and checkpoints
\end{itemize}

\section{Key Implementation Insights}

\subsection{Prompted SAM2 Strategy}

Our novel prompted SAM2 approach extracts point prompts from SCLIP's dense predictions using connected component analysis:

\begin{enumerate}
    \item Apply morphological operations to SCLIP predictions
    \item Extract centroids from each connected region
    \item Enforce minimum 20-pixel separation between points
    \item Generate 3 masks per point at different granularities
    \item Filter masks via majority voting (60\% threshold)
    \item Combine retained masks with logical OR
\end{enumerate}

This achieves 2× speedup over automatic mask generation while maintaining segmentation quality.

\subsection{Text Feature Caching}

Since text embeddings are constant across all images in a dataset:
\begin{itemize}
    \item Pre-compute embeddings once per evaluation run
    \item Store in memory (171 classes × 512 dims $\approx$ 350KB)
    \item Reuse cached embeddings for all images
    \item \textbf{Speedup:} 41\% on subsequent images
\end{itemize}

\subsection{Sliding Window Inference}

For images larger than CLIP's native resolution:
\begin{itemize}
    \item Divide image into 224×224 overlapping crops
    \item Stride of 112 pixels (50\% overlap)
    \item Process each crop independently
    \item Blend overlapping regions via averaging
    \item \textbf{Benefit:} Better boundary quality on high-res images
\end{itemize}

\section{Future Optimization Opportunities}

\begin{itemize}
    \item \textbf{Model quantization:} INT8 quantization for 4× memory reduction
    \item \textbf{TensorRT optimization:} Faster inference on NVIDIA GPUs
    \item \textbf{Batched processing:} Process multiple images simultaneously
    \item \textbf{Distillation:} Smaller student models for mobile deployment
    \item \textbf{Pruning:} Remove redundant parameters from ViT backbone
\end{itemize}

These optimizations could enable real-time performance (>10 FPS) on modern hardware while maintaining accuracy.
