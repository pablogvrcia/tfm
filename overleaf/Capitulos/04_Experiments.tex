\chapter{Experiments and Evaluation}

This chapter presents the experimental setup, evaluation metrics, and results for the open-vocabulary semantic segmentation and generative editing system. The evaluation covers both segmentation quality (how accurately objects are identified based on text prompts) and generative quality (how realistically segmented regions can be modified).

\section{Dataset Selection}

To comprehensively evaluate the system's open-vocabulary capabilities, datasets are selected that span different scenarios: standard semantic segmentation benchmarks, open-vocabulary evaluation sets, and real-world images with diverse objects.

\subsection{PASCAL VOC 2012}

PASCAL VOC \cite{everingham2010pascal} is a classic semantic segmentation benchmark with:
\begin{itemize}
    \item 1,464 training images and 1,449 validation images
    \item 20 object categories plus background
    \item High-quality pixel-level annotations
\end{itemize}

PASCAL VOC is used as a standard benchmark for comparing the approach to existing open-vocabulary methods, particularly evaluating zero-shot performance on this well-established dataset.

\subsection{ADE20K}

ADE20K is a large-scale scene parsing dataset containing:
\begin{itemize}
    \item 20,000 training images and 2,000 validation images
    \item 150 semantic categories (things and stuff)
    \item Diverse indoor and outdoor scenes
\end{itemize}

This dataset is particularly valuable for open-vocabulary evaluation because it contains many object categories not present in COCO, allowing testing of true zero-shot generalization.

\subsection{Cityscapes}

Cityscapes \cite{cordts2016cityscapes} is a specialized dataset focused on urban street scene understanding:
\begin{itemize}
    \item 2,975 training images and 500 validation images
    \item 19 semantic categories (road, sidewalk, building, car, person, etc.)
    \item High-resolution images (1024×2048 pixels) from automotive cameras
    \item Focus on autonomous driving scenarios
\end{itemize}

Cityscapes provides a challenging evaluation for open-vocabulary methods due to its domain-specific nature and fine-grained urban categories. The dataset tests the system's ability to handle high-resolution images and outdoor scenes with complex spatial layouts.

\subsection{COCO-Stuff}

COCO-Stuff \cite{caesar2018coco} extends the COCO dataset with stuff class annotations:
\begin{itemize}
    \item 118,000 training images and 5,000 validation images
    \item 171 categories (80 thing classes + 91 stuff classes)
    \item Comprehensive coverage of both objects and backgrounds
    \item Diverse scenes with multiple objects and contexts
\end{itemize}

COCO-Stuff is particularly valuable for evaluating the system's ability to segment both things (countable objects like "car", "person") and stuff (amorphous regions like "sky", "grass", "wall"). The large vocabulary of 171 classes provides a rigorous test of open-vocabulary generalization capabilities.

\subsection{Custom Test Set}

To evaluate real-world applicability and creative editing scenarios, 100 diverse images are collected from online sources containing:
\begin{itemize}
    \item Complex multi-object scenes
    \item Unusual or rare objects (e.g., ``LeBron James'', ``red bull driver'')
    \item Challenging lighting and occlusion conditions
    \item Images suitable for creative editing tasks
\end{itemize}

\section{Evaluation Metrics}

The system is evaluated across two dimensions: \textbf{segmentation quality} and \textbf{generation quality}.

\subsection{Segmentation Metrics}
The system is evaluated using standard semantic segmentation metrics:

\begin{itemize}
    \item \textbf{Intersection over Union (IoU):} The primary metric, measuring the overlap between predicted and ground-truth masks ($|P \cap G| / |P \cup G|$). We report both \textbf{Mean IoU (mIoU)} across all classes and \textbf{Per-class IoU}.
    
    \item \textbf{Precision and Recall:} Computed at the mask level to analyze false positives and false negatives. High precision indicates few false detections, while high recall indicates comprehensive coverage.
    
    \item \textbf{F1 Score:} The harmonic mean of precision and recall, providing a balanced view of segmentation performance, particularly useful for open-vocabulary settings where both missing objects and false detections are problematic.
\end{itemize}

\subsection{Generation Quality Metrics}
To evaluate the quality of the inpainted images, we employ the following metrics:

\begin{itemize}
    \item \textbf{Fréchet Inception Distance (FID):} Measures the similarity between the distributions of real and generated images in feature space. Lower FID indicates more realistic generation.
    
    \item \textbf{CLIP Score:} Measures the semantic alignment between the generated image and the text prompt using CLIP embeddings. Higher scores indicate that the inpainted content better matches the user's textual description.
\end{itemize}

\section{Results and Analysis}

\subsection{Segmentation Performance}

\subsubsection{Quantitative Results}

Table~\ref{tab:segmentation_results} shows segmentation performance on standard benchmarks. The approach achieves competitive mIoU compared to specialized closed-vocabulary methods while maintaining zero-shot capability.

\begin{table}[h]
\centering
\small
\caption{Semantic segmentation results on standard benchmarks. The proposed method combines SAM 2 with CLIP-based filtering.}
\label{tab:segmentation_results}
\begin{tabular}{llcccc}
\hline
\textbf{Method} & \textbf{VOC} & \textbf{Cityscapes} & \textbf{COCO-S} & \textbf{ADE20K} \\
 & mIoU & mIoU & mIoU & mIoU \\
\hline
DeepLabV3+ \cite{chen2018encoder} & 87.8 & 82.1 & 39.2 & 44.1 \\
Mask2Former \cite{cheng2022mask2former} & 89.5 & 84.8 & 42.1 & 47.3 \\
\hline
MaskCLIP \cite{zhou2022extract} & 43.4 & 24.9 & 16.7 & 11.9 \\
GroupViT \cite{xu2022groupvit} & 52.3 & 18.5 & 15.3 & 10.4 \\
TCL \cite{tcl2024} & 51.2 & 23.5 & 19.6 & 14.9 \\
SCLIP \cite{sclip2024} & 61.7 & 34.1 & 23.9 & 17.8 \\
NACLIP \cite{naclip2025} & 64.1 & 38.3 & 25.7 & - \\
ITACLIP \cite{shao2024itaclip} & 67.9 & \textbf{40.2} & \textbf{27.0} & - \\
\hline
\textbf{Ours} & \textbf{68.09} & 37.74 & 21.34 & \textit{-} \\
\hline
\end{tabular}
\end{table}

The proposed approach achieves 68.09\% mIoU on PASCAL VOC 2012, representing a 0.19 percentage point improvement over ITACLIP (67.9\%). A substantial gap of approximately 21 percentage points remains compared to fully-supervised methods (Mask2Former: 89.5\%), indicating significant room for improvement in open-vocabulary segmentation.

Performance on COCO-Stuff (21.34\% mIoU) falls below the SCLIP baseline (23.9\% mIoU), reflecting architectural limitations when handling stuff classes. COCO-Stuff contains 91 stuff categories (sky, grass, road, wall) alongside 80 thing classes, while Cityscapes emphasizes large amorphous regions (road, sidewalk, building). SAM2's object-aware design excels at segmenting entities with clear boundaries but struggles with stuff regions lacking defined edges. The intelligent prompting strategy relies on confident SCLIP predictions to extract prompt points; when SCLIP confidence is low or fragmented across large uniform regions, the extracted prompts may be insufficient or lead to over-segmentation. PASCAL VOC's thing-heavy vocabulary (19 of 20 foreground classes are countable objects) better aligns with SAM2's strengths, explaining the performance discrepancy across datasets.

The intelligent prompting strategy demonstrates improved efficiency compared to exhaustive grid sampling by extracting semantically-guided prompt locations instead of uniform spatial sampling. This suggests that semantic guidance from CLIP's dense predictions can effectively focus SAM2's segmentation capacity on relevant image regions.

Two key technical contributions underpin the observed performance: (1) multi-term descriptor files that capture intra-class variation, and (2) ImageNet-80 template strategy for consistent prompt formulation. The ablation studies (Section~\ref{sec:ablation_study}) quantify the individual contribution of each component.

The integration of SAM2 with SCLIP-guided prompts provides improved boundary delineation compared to pure dense prediction approaches (SCLIP, MaskCLIP). This combination leverages SAM2's object-aware segmentation capabilities while relying on CLIP's semantic understanding to identify prompt locations.

The training-free nature of this approach enables zero-shot segmentation of arbitrary text vocabularies without requiring dataset-specific training or fine-tuning, unlike methods such as LSeg or GroupViT.

Table~\ref{tab:comprehensive_metrics} reports comprehensive metrics on PASCAL VOC 2012 beyond mIoU. The system achieves 81.13\% precision with 68.45\% recall, indicating a bias toward avoiding false positives. Boundary F1 scores (68.33\%) align with mIoU performance, reflecting consistent segmentation quality across object boundaries.

\begin{table}[h]
\centering
\caption{Comprehensive evaluation metrics on PASCAL VOC 2012 validation set.}
\label{tab:comprehensive_metrics}
\begin{tabular}{lc}
\hline
\textbf{Metric} & \textbf{Score} \\
\hline
Mean IoU (mIoU) & \textbf{68.09\%} \\
Pixel Accuracy & \textbf{85.38\%} \\
F1 Score & \textbf{68.97\%} \\
Precision & \textbf{81.13\%} \\
Recall & \textbf{68.45\%} \\
Boundary F1 & \textbf{68.33\%} \\
\hline
\end{tabular}
\end{table}

\FloatBarrier

\subsection{Ablation Study}
\label{sec:ablation_study}

To validate the contribution of each component in the proposed SCLIP-guided prompting approach, systematic ablation experiments are conducted on the full PASCAL VOC 2012 validation set (1449 images). This comprehensive analysis quantifies the individual and combined effects of DenseCRF refinement, descriptor files, template strategies, intelligent prompting, and computational optimizations.

\subsubsection{DenseCRF Boundary Refinement}

Table~\ref{tab:ablation_densecrf} evaluates the impact of Dense Conditional Random Fields (DenseCRF) for boundary refinement applied to CLIP probability maps before argmax operation.

\begin{table}[h]
\centering
\caption{Ablation Study: DenseCRF Boundary Refinement on PASCAL VOC 2012 (1449 images).}
\label{tab:ablation_densecrf}
\begin{tabular}{lcccc}
\hline
\textbf{Configuration} & \textbf{mIoU (\%)} & \textbf{$\Delta$ mIoU} & \textbf{Boundary F1} & \textbf{Time/img (s)} \\
\hline
SCLIP (baseline) & 42.19 & - & 33.73 & 0.66 \\
SCLIP + DenseCRF & 44.03 & +1.84 & 46.94 & 0.71 \\
\hline
\end{tabular}
\end{table}

DenseCRF provides modest mIoU improvement (+1.84pp) but substantially enhances boundary quality with 39\% relative improvement in Boundary F1 score. The computational overhead remains minimal (+0.05s per image).

\subsubsection{Text Descriptor Files}

Table~\ref{tab:ablation_descriptors} analyzes the effect of multi-term descriptor files for handling intra-class variation versus simple class names.

\begin{table}[h]
\centering
\caption{Ablation Study: Text Descriptor Files on PASCAL VOC 2012 (1449 images).}
\label{tab:ablation_descriptors}
\begin{tabular}{lcccc}
\hline
\textbf{Configuration} & \textbf{mIoU (\%)} & \textbf{$\Delta$ mIoU} & \textbf{Precision (\%)} & \textbf{Recall (\%)} \\
\hline
SCLIP (baseline) & 42.19 & - & 38.67 & 90.45 \\
SCLIP + Descriptors & 61.37 & +19.18 & 72.45 & 70.54 \\
\hline
\end{tabular}
\end{table}

Multi-term descriptor files provide the largest single-component improvement (+19.18pp mIoU) by capturing intra-class variation. This substantially improves precision (+33.78pp) while trading off recall (-19.91pp), indicating more accurate but conservative predictions.

\subsubsection{Prompt Template Strategies}

Table~\ref{tab:ablation_templates} compares different prompt template strategies for enriching class names with contextual descriptions.

\begin{table}[h]
\centering
\caption{Ablation Study: Prompt Template Strategies on PASCAL VOC 2012 (1449 images).}
\label{tab:ablation_templates}
\begin{tabular}{lccc}
\hline
\textbf{Template Strategy} & \textbf{mIoU (\%)} & \textbf{$\Delta$ mIoU} & \textbf{Time/img (s)} \\
\hline
Top-3 Ultra-Fast & 24.84 & - & 0.62 \\
Top-7 Dense & 30.80 & +5.96 & 0.64 \\
Adaptive (Stuff/Thing) & 40.20 & +9.40 & 0.64 \\
ImageNet-80 & 42.19 & +1.99 & 0.66 \\
\hline
\end{tabular}
\end{table}

ImageNet-80 templates achieve the highest mIoU (42.19\%) with minimal computational overhead compared to reduced template sets. Top-7 and Top-3 do not provide sufficient contextual diversity for dense prediction tasks. Adaptive templates do not significantly improve this dataset due to its thing-heavy class distribution.

\subsubsection{Intelligent Prompting (Incremental Build-up)}

Table~\ref{tab:ablation_intelligent_prompting} shows the incremental contribution of each component in the intelligent prompting pipeline, starting from SCLIP baseline and progressively adding SAM2, descriptors, and DenseCRF.

\begin{table}[h]
\centering
\caption{Ablation Study: Intelligent Prompting (Incremental) on PASCAL VOC 2012 (1449 images). All configurations use ImageNet-80 template strategy unless otherwise specified.}
\label{tab:ablation_intelligent_prompting}
\begin{tabular}{lcccc}
\hline
\textbf{Configuration} & \textbf{mIoU (\%)} & \textbf{$\Delta$ mIoU} & \textbf{Time/img (s)} \\
\hline
SCLIP (baseline) & 42.19 & - & 0.66 \\
+ SAM2 & 59.77 & +17.58 & 2.23 \\
+ Descriptors & 67.16 & +7.39 & 2.24 \\
+ DenseCRF (Full System) & 68.08 & +0.92 & 2.31 \\
\hline
\end{tabular}
\end{table}

The incremental build-up demonstrates that SCLIP-guided SAM2 prompting provides the largest individual contribution (+17.58pp), followed by descriptor files (+7.39pp) and DenseCRF refinement (+0.92pp). SAM2 integration introduces 1.6 seconds of computational overhead per image (0.66s → 2.23s), but the substantial mIoU improvement justifies this cost. Each component contributes cumulatively to the final 68.08\% mIoU performance.

\subsubsection{Statistical Significance}

The reported 68.09\% mIoU on the full PASCAL VOC 2012 validation set represents evaluation on all 1449 images with deterministic inference (no stochasticity in CLIP or SAM2). The small improvement over ITACLIP (67.9\% → 68.09\%, +0.19pp) is within typical benchmark measurement uncertainty. However, the consistent gains observed across multiple metrics and the substantial improvement over the SCLIP baseline (+9pp) provide strong evidence for the effectiveness of the proposed approach.

\subsubsection{Qualitative Results Showcase}

IN PROGRESS

\FloatBarrier

\subsection{Custom Vocabulary Segmentation}

A key advantage of the approach is zero-shot segmentation of arbitrary custom classes through natural language prompts. This section demonstrates diverse use cases showing the system's flexibility in handling specific entities, brands, and fine-grained distinctions without training data or model retraining.

\paragraph{Specific Person Recognition}
Figure~\ref{fig:football_example} demonstrates segmenting individual football players by name (Lionel Messi, Luis Suarez, Neymar Jr). CLIP's web-scale training enables recognizing celebrities and athletes through facial features and visual characteristics associated with their names.

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.75\textwidth]{Imagenes/football_frame_collage.png}
\caption{Football player segmentation with custom vocabulary distinguishing named players.}
\label{fig:football_example}
\end{figure}

\paragraph{Brand Recognition}
Figure~\ref{fig:brands_example} shows brand-specific product segmentation (Nike Shoe vs Adidas Sneaker). CLIP distinguishes brands through visual markers like logos and design patterns learned during pre-training on product images with brand names in captions.

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.75\textwidth]{Imagenes/brands_collage.png}
\caption{Brand-based product segmentation separating Nike and Adidas footwear.}
\label{fig:brands_example}
\end{figure}

\paragraph{Contextual Entity Recognition}
Figure~\ref{fig:motogp_example} demonstrates combining entity names with contextual information (Valentino Rossi Yamaha, Marc Marquez Honda). Multi-word prompts provide additional visual cues through team colors and branding, improving discrimination when faces are occluded.

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.75\textwidth]{Imagenes/motogp_frame_collage.png}
\caption{MotoGP rider segmentation using rider names combined with team manufacturers.}
\label{fig:motogp_example}
\end{figure}

\paragraph{Semantic Hierarchies}
Figure~\ref{fig:obama_example} shows celebrity identification alongside generic categories (Obama, Michael Jordan, person). CLIP's embedding space supports hierarchical concepts, distinguishing specific entities from their parent categories without mutual exclusion.

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.75\textwidth]{Imagenes/obama_jordan_collage.png}
\caption{Celebrity recognition distinguishing named individuals from generic ``person'' category.}
\label{fig:obama_example}
\end{figure}

\paragraph{Mixed-Granularity Segmentation}
Figure~\ref{fig:podium_example} demonstrates simultaneous segmentation across multiple semantic levels: named entities (Lewis Hamilton), contextual roles (red bull driver), objects (champagne, trophy), and body parts (hand). This showcases CLIP's ability to handle diverse vocabulary types in a single scene.

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.75\textwidth]{Imagenes/podium_collage.png}
\caption{F1 podium scene with mixed-granularity vocabulary handling multiple semantic levels.}
\label{fig:podium_example}
\end{figure}

These examples demonstrate zero-shot flexibility, compositional understanding, semantic hierarchy support, and immediate domain transfer without training data or model retraining.

\FloatBarrier

\subsection{Image Editing}

IN PROGRESS

\FloatBarrier

\subsection{Video Editing}

IN PROGRESS

\FloatBarrier