\chapter{Methodology}
\label{ch:methodology}

This chapter details the methodology employed to develop the proposed open-vocabulary semantic segmentation system. Unlike traditional approaches that rely on closed-vocabulary classifiers, this work leverages recent advances in vision-language models and promptable segmentation to enable flexible, language-driven image understanding and manipulation.

The core contribution is a CLIP-guided prompting approach that combines SCLIP's dense predictions with SAM2's segmentation quality through intelligent prompt extraction. This fully annotation-free method extracts semantic prompt points from CLIP's high-confidence regions, using SAM2 to generate high-quality masks with direct class assignment.

Building upon insights from SCLIP \cite{sclip2024}, MaskCLIP \cite{zhou2022extract}, CLIPSeg \cite{luddecke2022clipseg}, and SAM 2 \cite{ravi2024sam2}, this chapter presents the CLIP-guided prompting methodology in detail.

\section{CLIP-Guided Prompting Approach}
\label{sec:clip_guided_approach}

The main contribution is a CLIP-guided prompting system that leverages SCLIP's Cross-layer Self-Attention mechanism to extract intelligent prompt points for SAM2 segmentation. This approach achieves fully annotation-free open-vocabulary segmentation by combining semantic understanding with high-quality boundary delineation through efficient, semantically-guided prompting.

\subsection{Motivation and Design Philosophy}

CLIP-guided prompting offers several key advantages for open-vocabulary segmentation:

\begin{itemize}
    \item \textbf{Efficient semantic guidance:} Instead of blindly prompting SAM2 at thousands of points (e.g., 4096 in a grid), CLIP identifies 50-300 semantically meaningful locations, reducing computational cost by 96\% while maintaining competitive accuracy.

    \item \textbf{Zero spatial user input:} Users provide only a text vocabulary; the system automatically determines where objects are located without requiring manual clicks or bounding boxes.

    \item \textbf{High-quality segmentation:} SAM2 provides precise object boundaries at each semantically-guided prompt point, combining CLIP's semantic understanding with SAM2's superior mask quality.

    \item \textbf{Training-free operation:} Uses frozen CLIP and SAM2 models without any fine-tuning, enabling zero-shot transfer to new domains.
\end{itemize}

The key insight is to use CLIP's dense predictions not as the final output, but as intelligent guidance for SAM2 prompting, achieving both efficiency and quality.

\subsection{System Overview}

The CLIP-guided prompting pipeline consists of four main stages:

\begin{enumerate}
    \item \textbf{Dense Semantic Prediction:} SCLIP extracts dense features using Cross-layer Self-Attention (CSA) and produces pixel-wise class predictions through similarity matching with text embeddings.

    \item \textbf{Intelligent Prompt Extraction:} Extract 50-300 representative points from high-confidence regions in SCLIP's predictions using connected component analysis and centroid computation.

    \item \textbf{SAM2 Segmentation:} Prompt SAM2 at each extracted point to generate high-quality instance masks with direct class assignment based on CLIP predictions.

    \item \textbf{Overlap Resolution:} Resolve overlapping masks through IoU-based filtering and confidence-based priority assignment.
\end{enumerate}

\begin{figure}[h]
\centering
\fbox{\parbox{0.9\textwidth}{\centering
\vspace{1cm}
\textbf{[PLACEHOLDER: CLIP-Guided Prompting Pipeline Diagram]}\\[0.5cm]
\textit{This figure should show the CLIP-guided prompting pipeline:}\\[0.3cm]
\begin{tabular}{l}
\textbf{Stage 1 - Dense CLIP Prediction:}\\
\quad Input image → SCLIP (ViT-B/16 with CSA) → Dense class probabilities (H×W×C)\\
\quad Apply Cross-layer Self-Attention → Extract dense patch features → Pixel-wise predictions\\[0.3cm]
\textbf{Stage 2 - Intelligent Prompt Extraction:}\\
\quad SCLIP predictions → Connected component analysis → Extract centroids\\
\quad Filter by confidence (>0.3) and size (>100 pixels) → 50-300 prompt points\\[0.3cm]
\textbf{Stage 3 - SAM2 Segmentation:}\\
\quad Prompt points → SAM2 predictor → High-quality instance masks\\
\quad Direct class assignment from CLIP predictions → Instance-level segmentation\\[0.3cm]
\textbf{Stage 4 - Overlap Resolution:}\\
\quad IoU-based filtering → Confidence-based priority → Final segmentation\\[0.3cm]
\end{tabular}
\textit{Include visualization: dense heatmap, extracted points, SAM2 masks, final result.}
\vspace{1cm}
}}
\caption{Overview of the CLIP-guided prompting pipeline. The system uses CLIP's dense predictions to extract intelligent prompt points, achieving 96\% reduction in prompts compared to blind grid sampling while maintaining competitive accuracy.}
\label{fig:clip_guided_pipeline}
\end{figure}

The key insight motivating this design is that CLIP's dense predictions identify where objects are located, enabling intelligent SAM2 prompting with far fewer points than blind grid sampling. By extracting points from SCLIP's high-confidence regions, SAM2 is guided to focus on semantically relevant areas, achieving both efficiency and accuracy.

\subsection{Technical Background: CLIP and Vision Transformers}
\label{sec:technical_background}

The approach builds on two foundation models: \textbf{CLIP} \cite{radford2021learning} for vision-language alignment and \textbf{Vision Transformers (ViT)} \cite{dosovitskiy2020image} for dense feature extraction. A brief overview is provided here; detailed technical explanations are available in Appendix \ref{appendix:technical_foundations}.

\subsubsection{CLIP: Vision-Language Foundation}

CLIP learns a shared embedding space between images and text through contrastive learning on 400M image-text pairs. For open-vocabulary segmentation, CLIP provides:
\begin{itemize}
    \item \textbf{Zero-shot capability}: Recognizes unseen objects by computing similarity between image regions and text descriptions
    \item \textbf{Semantic understanding}: Trained on natural language, not fixed class labels
    \item \textbf{Dense features}: Patch-level representations from ViT-B/16 backbone (14×14 grid for 224×224 images)
\end{itemize}

\subsubsection{Vision Transformer Architecture}

ViT processes images as sequences of 16×16 patches through 12 Transformer layers. Key properties for dense prediction:
\begin{itemize}
    \item \textbf{Global context}: Self-attention captures long-range dependencies across patches
    \item \textbf{Patch tokens}: Unlike CNNs, provides per-patch features suitable for dense tasks
    \item \textbf{Standard attention}: Computes $\text{Attention}(Q, K, V) = \text{softmax}(QK^T/\sqrt{d})V$
\end{itemize}

However, standard ViT attention optimized for classification struggles with dense prediction, motivating SCLIP's modification (next section).

\subsection{SCLIP Dense Prediction with Cross-Layer Self-Attention}
\label{sec:sclip_dense_extraction}

\textit{Note: SCLIP \cite{sclip2024} is prior work adopted for dense feature extraction. \textbf{This thesis's contribution} is using these features for intelligent SAM2 prompting (Section \ref{sec:intelligent_prompting}) and enhancing them with descriptor files and template strategies (Sections \ref{sec:descriptor_files}, \ref{sec:template_strategies}).}

\subsubsection{Cross-Layer Self-Attention: The Key Modification}

SCLIP's main innovation is modifying CLIP's final attention layer (layer 12 of ViT-B/16) to use \textbf{Cross-layer Self-Attention (CSA)} instead of standard attention:

\textbf{Standard attention} (layers 1-11):
\begin{equation}
A_h^{\text{standard}} = \text{softmax}\left(\frac{Q_h K_h^T}{\sqrt{d_h}}\right)
\end{equation}

\textbf{Cross-layer Self-Attention} (layer 12):
\begin{equation}
\boxed{A_h^{\text{CSA}} = \text{softmax}\left(\frac{Q_h Q_h^T + K_h K_h^T}{\sqrt{d_h}}\right)}
\end{equation}

\textbf{Why this improves dense prediction:} The $Q_h Q_h^T$ and $K_h K_h^T$ terms measure patch-to-patch self-similarity, encouraging spatial grouping. Patches with similar queries or keys (e.g., all "sky" patches) mutually reinforce each other, producing spatially coherent features. Standard $Q_h K_h^T$ attention works well for classification but creates fragmented dense predictions.

\subsubsection{Dense Prediction Pipeline}

The complete SCLIP pipeline (see Figure \ref{fig:clip_guided_pipeline} for overview):

\textbf{1. Image encoding:} ViT-B/16 processes 224×224 images into 14×14 patch features ($F \in \mathbb{R}^{14 \times 14 \times 512}$)

\textbf{2. Text encoding:} For each class, encode text descriptors with templates (detailed in Sections \ref{sec:descriptor_files}, \ref{sec:template_strategies}) to get embeddings $e_c \in \mathbb{R}^{512}$

\textbf{3. Similarity computation:} Calculate dense cosine similarity between image patches and text embeddings:
\begin{equation}
S_{i,j,c} = F[i,j,:] \cdot e_c \in [-1, 1], \quad S \in \mathbb{R}^{14 \times 14 \times C}
\end{equation}

\textbf{4. Dense predictions:} Upsample to original resolution and apply softmax with temperature scaling ($T=40$):
\begin{equation}
P_{x,y,c} = \frac{\exp(T \cdot S_{x,y,c})}{\sum_{c'} \exp(T \cdot S_{x,y,c'})}, \quad \hat{c}(x,y) = \arg\max_c P_{x,y,c}
\end{equation}

This produces pixel-wise class predictions $\hat{c} \in \mathbb{R}^{H \times W}$ and confidence maps $P \in \mathbb{R}^{H \times W \times C}$ that feed into the intelligent prompting strategy.

\subsubsection{Descriptor Files: Multi-Term Class Representations}
\label{sec:descriptor_files}

\fbox{\textbf{Thesis Contribution}} This section describes the enhancement to SCLIP through descriptor files.

In practice, using a single class name (e.g., "person") limits CLIP's ability to recognize intra-class variations. \textbf{Descriptor files} address this limitation by providing multiple descriptive terms for each class, enabling CLIP to match different visual manifestations of the same semantic category.

\paragraph{Motivation and Design}

Standard approaches encode each class with a single term: ["person", "car", "dog"]. However, objects exhibit significant visual diversity:
\begin{itemize}
    \item \textbf{People} appear in various clothing (shirts, jeans, dresses)
    \item \textbf{Backgrounds} comprise multiple stuff classes (sky, wall, grass, road)
    \item \textbf{Compound objects} have material-specific variants (brick wall, wooden floor)
\end{itemize}

Descriptor files (e.g., \texttt{cls\_voc21.txt}) map each class index to a comma-separated list of descriptive terms, allowing CLIP to recognize these variations. This simple yet effective technique significantly improves segmentation quality without requiring retraining.

\paragraph{Implementation for PASCAL VOC 2012}

Our PASCAL VOC descriptor file (\texttt{configs/cls\_voc21.txt}) provides rich descriptions for all 21 classes:

\textbf{Background class} (line 1):
\begin{verbatim}
sky, wall, tree, wood, grass, road, sea, river, mountain,
sands, desk, bed, building, cloud, lamp, door, window,
wardrobe, ceiling, shelf, curtain, stair, floor, hill,
rail, fence
\end{verbatim}

This comprehensive list allows CLIP to recognize diverse background elements as a single semantic category, dramatically improving background IoU (86.90\% vs. much lower with single "background" term).

\textbf{Person class} (line 16):
\begin{verbatim}
person, person in shirt, person in jeans, person in dress,
person in sweater, person in skirt, person in jacket
\end{verbatim}

Clothing-aware descriptors help CLIP match people across different outfits, improving person segmentation (56.70\% IoU).

\textbf{Compound classes} (lines 12, 21):
\begin{verbatim}
table                    (line 12: diningtable)
television monitor, tv monitor, monitor, television, screen
                         (line 21: tvmonitor)
\end{verbatim}

Synonym expansion (diningtable → "table") and multi-word variants (tvmonitor → "television monitor", "tv monitor") help CLIP understand Pascal VOC's specific naming conventions.

\paragraph{Text Encoding with Descriptors}

When using descriptor files, Step 5 (Text Encoding) is modified:

\textbf{For each class $c$:}
\begin{enumerate}
    \item Load descriptor terms: $D_c = \{\text{term}_1, \text{term}_2, \ldots, \text{term}_{K_c}\}$
    \item For each term $d \in D_c$ and template $t_i$:
    \begin{equation}
    e_{c,d,i} = \text{CLIP}_{\text{text}}(t_i(d)) \in \mathbb{R}^{D}
    \end{equation}
    \item Average across all terms and templates:
    \begin{equation}
    e_c = \frac{1}{K_c \cdot M} \sum_{d \in D_c} \sum_{i=1}^{M} e_{c,d,i}
    \end{equation}
    \item L2 normalize: $e_c \leftarrow e_c / \|e_c\|_2$
\end{enumerate}

This produces a richer class embedding that captures multiple visual manifestations. For "person" with 7 descriptors and 80 templates, 560 text embeddings are averaged into a single robust class representation.

\paragraph{Impact on Performance}

Descriptor files contribute significantly to the 68.09\% mIoU on PASCAL VOC:
\begin{itemize}
    \item \textbf{Background}: 86.90\% IoU - comprehensive coverage of stuff classes
    \item \textbf{Person}: 56.70\% IoU - improved through clothing-aware variants
    \item \textbf{TVmonitor}: 52.64\% IoU - benefits from synonym expansion
    \item \textbf{Overall}: Substantial improvement in segmentation quality
\end{itemize}

\subsubsection{Template Optimization Strategies}
\label{sec:template_strategies}

\fbox{\textbf{Thesis Contribution}} This section describes the evaluation and selection of template strategies.

While descriptor files address \textit{what} terms to encode, template strategies determine \textit{how} to frame those terms for CLIP. Standard approaches use 80 ImageNet templates \cite{zhou2022learning}, but recent research \cite{huang2024pixelclip, wysoczanska2024clipdiy} shows that carefully curated templates tailored for dense prediction can achieve better accuracy with 10× fewer templates.

\paragraph{Template Strategy Taxonomy}

We implement several template strategies, selectable via the \texttt{--template-strategy} argument:

\textbf{1. ImageNet-80 (Baseline)}
\begin{itemize}
    \item 80 diverse templates designed for ImageNet classification
    \item Examples: "a photo of a \{class\}", "a rendering of a \{class\}", "art of a \{class\}"
    \item Pros: Well-tested, comprehensive coverage
    \item Cons: Slow (80× text encoding cost), many templates irrelevant for segmentation
\end{itemize}

\textbf{2. Top-7 Dense Prediction} (PixelCLIP \cite{huang2024pixelclip}, recommended)
\begin{itemize}
    \item 7 templates selected via forward selection on segmentation tasks
    \item Examples: "a photo of a \{class\}", "a \{class\} in the scene", "the \{class\}"
    \item Pros: 11× faster than ImageNet-80, comparable accuracy
    \item Rationale: Spatial context ("in the scene") critical for dense prediction
\end{itemize}

\textbf{3. Spatial Context Templates} (MaskCLIP/DenseCLIP inspiration)
\begin{itemize}
    \item 7 templates emphasizing spatial/scene context
    \item Examples: "\{class\} in the scene", "segment the \{class\}", "there is a \{class\} in the scene"
    \item Pros: Explicitly guides CLIP toward localization, improved spatial understanding
    \item Use case: When spatial understanding is critical (indoor scenes, crowded images)
\end{itemize}

\textbf{4. Top-3 Ultra-Fast}
\begin{itemize}
    \item 3 most effective templates for minimal latency
    \item Examples: "a photo of a \{class\}", "a \{class\} in the scene", "the \{class\}"
    \item Pros: 27× faster than ImageNet-80, minimal accuracy loss
    \item Use case: Real-time applications, resource-constrained environments
\end{itemize}

\textbf{5. Adaptive (Stuff vs. Thing)}
\begin{itemize}
    \item Automatically selects template set based on class type
    \item Stuff classes (sky, grass, road): Emphasize continuity, use mass nouns
    \item Thing classes (car, person, cat): Emphasize countability, use articles
    \item Pros: Class-aware optimization as demonstrated by CLIP-DIY \cite{wysoczanska2024clipdiy}
    \item Example: "the sky" (stuff) vs. "a car" (thing)
\end{itemize}

\paragraph{Implementation Details}

Template selection is configured via Python code (\texttt{prompts/dense\_prediction\_templates.py}):

\begin{verbatim}
top7_dense_templates = [
    lambda c: f'a photo of a {c}.',
    lambda c: f'a {c} in the scene.',
    lambda c: f'the {c}.',
    lambda c: f'a close-up photo of a {c}.',
    lambda c: f'a photo of the large {c}.',
    lambda c: f'a photo of the small {c}.',
    lambda c: f'one {c}.',
]
\end{verbatim}

Our best results (68.09\% mIoU) use the \textbf{ImageNet-80} strategy, trading inference speed for maximum accuracy. For faster inference with minimal accuracy loss, Top-7 is recommended.

\paragraph{Performance Comparison}

Ablation study on PASCAL VOC 2012 (100 samples):
\begin{table}[h]
\centering
\begin{tabular}{lccc}
\hline
\textbf{Strategy} & \textbf{Templates} & \textbf{Speedup} & \textbf{mIoU (\%)} \\
\hline
ImageNet-80 & 80 & 1× (baseline) & \textbf{68.09} \\
Top-7 Dense & 7 & 11× & 67.2 \\
Spatial Context & 7 & 11× & 67.5 \\
Top-3 Fast & 3 & 27× & 66.8 \\
Adaptive & 5-7 & 11-16× & 67.8 \\
\hline
\end{tabular}
\caption{Template strategy comparison. ImageNet-80 achieves best accuracy; Top-7 offers best speed/accuracy tradeoff.}
\label{tab:template_strategies}
\end{table}

\subsubsection{Computational Optimizations}
\label{sec:computational_optimizations}

\fbox{\textbf{Thesis Contribution}} This section describes the implementation of computational optimizations.

Beyond algorithmic improvements (descriptor files, template strategies, intelligent prompting), several low-level computational optimizations are implemented that significantly improve inference speed and memory efficiency without affecting accuracy. These optimizations are critical for practical deployment, especially on resource-constrained hardware.

\paragraph{Mixed Precision Inference (FP16)}

Modern GPUs achieve substantially higher throughput when operating on 16-bit floating-point (FP16) data compared to standard 32-bit (FP32). The implementation leverages PyTorch's automatic mixed precision (AMP) to accelerate inference while preserving numerical stability.

\textbf{Implementation:}
\begin{verbatim}
with torch.cuda.amp.autocast(enabled=use_fp16):
    # CLIP vision encoder forward pass
    image_features = clip_model.encode_image(images)
    # Text encoder forward pass
    text_features = clip_model.encode_text(text_tokens)
    # Similarity computation
    similarity = image_features @ text_features.T
\end{verbatim}

The \texttt{autocast} context automatically identifies operations that benefit from FP16 (e.g., matrix multiplications, convolutions) while keeping precision-sensitive operations (e.g., softmax, normalization) in FP32. This hybrid approach provides:

\begin{itemize}
    \item \textbf{Speed improvement}: Approximately 1.5-2× faster inference on modern GPUs (Ampere, Ada Lovelace architectures)
    \item \textbf{Memory reduction}: 50\% reduction in activation memory, enabling larger batch sizes or higher-resolution inputs
    \item \textbf{Numerical stability}: Critical operations remain in FP32, preventing precision-related artifacts
    \item \textbf{Accuracy preservation}: No measurable impact on mIoU (68.09\% maintained)
\end{itemize}

\textbf{Note:} FP16 optimizations are only enabled on CUDA devices. CPU inference defaults to FP32 due to limited FP16 hardware support on most CPUs.

\paragraph{Just-In-Time Compilation (torch.compile)}

PyTorch 2.0+ introduces \texttt{torch.compile()}, a JIT compiler that fuses operations, eliminates Python overhead, and generates optimized CUDA kernels. The implementation applies compilation to both the CLIP vision encoder and text encoder.

\textbf{Implementation:}
\begin{verbatim}
if use_compile:
    clip_model.visual = torch.compile(
        clip_model.visual,
        mode="reduce-overhead"
    )
    clip_model.transformer = torch.compile(
        clip_model.transformer,
        mode="reduce-overhead"
    )
\end{verbatim}

The \texttt{"reduce-overhead"} mode prioritizes minimizing kernel launch overhead and maximizing GPU occupancy. First-inference compilation incurs a one-time overhead (10-20 seconds), but subsequent inferences benefit from:

\begin{itemize}
    \item \textbf{Kernel fusion}: Combines consecutive operations (e.g., LayerNorm + Linear + GELU) into single kernels
    \item \textbf{Memory optimization}: Reduces intermediate tensor allocations through in-place operations
    \item \textbf{Graph-level optimization}: Eliminates redundant computations and optimizes data movement
    \item \textbf{Speedup}: Additional 1.2-1.5× improvement on top of FP16 optimizations
\end{itemize}

\textbf{Trade-offs:} Compilation adds initial latency and slightly increases memory usage during compilation. For single-image inference, the overhead may not be justified. For benchmark evaluation (hundreds of images), compilation provides substantial cumulative savings.

\paragraph{Batched SAM2 Prompting}

Standard SAM2 usage prompts one point at a time, requiring separate forward passes for each prompt. With 50-300 prompts per image, this creates significant overhead from repeated model loading, tensor transfers, and kernel launches. The implementation uses batched prompting to process multiple points simultaneously.

\textbf{Implementation:}
\begin{verbatim}
# Standard approach (sequential, slow)
masks = []
for point in prompt_points:
    mask = sam2_predictor.predict(point)
    masks.append(mask)

# Batched approach (parallel, fast)
batch_size = 32
for i in range(0, len(prompt_points), batch_size):
    batch = prompt_points[i:i+batch_size]
    batch_masks = sam2_predictor.predict_batch(batch)
    masks.extend(batch_masks)
\end{verbatim}

Batching exploits GPU parallelism by:
\begin{itemize}
    \item \textbf{Amortizing overhead}: Single image encoding shared across all prompts in batch
    \item \textbf{Parallel mask decoding}: Decoder processes multiple prompts simultaneously
    \item \textbf{Efficient memory access}: Coalesced GPU memory reads/writes
    \item \textbf{Speedup}: 2-4× faster than sequential prompting, depending on batch size
\end{itemize}

\textbf{Batch size selection:} Larger batches improve GPU utilization but increase memory consumption. The default batch size of 32 provides a balance, fitting comfortably within 6GB VRAM while maintaining high throughput.

\paragraph{Combined Impact}

When all optimizations are enabled together, they provide multiplicative speedup:

\begin{table}[h]
\centering
\begin{tabular}{lcc}
\hline
\textbf{Configuration} & \textbf{Time/Image (s)} & \textbf{Relative Speedup} \\
\hline
Baseline (FP32, no compile, sequential) & 45-60 & 1.0× \\
+ FP16 & 25-35 & 1.5-1.8× \\
+ FP16 + torch.compile & 18-28 & 2.1-2.5× \\
+ FP16 + torch.compile + batching & 12-20 & 3.0-3.8× \\
\hline
\end{tabular}
\caption{Cumulative impact of computational optimizations on inference time (NVIDIA RTX series GPU, PASCAL VOC images).}
\label{tab:computational_optimizations}
\end{table}

These optimizations enable practical deployment on consumer hardware (e.g., NVIDIA GeForce GTX 1060 6GB) where the best configuration achieves 12-33 seconds per image for the complete CLIP-guided SAM2 pipeline.


\subsection{Intelligent Prompt Extraction}
\label{sec:intelligent_prompting}

\fbox{\textbf{Main Contribution}} This is the core algorithmic contribution: using SCLIP's dense predictions to extract intelligent prompt points for SAM2, achieving 96\% reduction in prompts while outperforming recent training-free methods on PASCAL VOC 2012.

The intelligent prompt extraction uses SCLIP's semantic understanding to guide SAM2 prompting, achieving massive efficiency gains without sacrificing accuracy. This section details the complete pipeline from SCLIP's dense predictions to semantically meaningful prompt points.

\subsubsection{Motivation: From Dense Predictions to Sparse Prompts}

Naive approaches to combining CLIP with SAM often use exhaustive grid sampling: prompting SAM at every point in a dense grid (e.g., 64×64 = 4096 points) and using CLIP to classify the resulting masks. This has two major drawbacks:

\begin{itemize}
    \item \textbf{Computational inefficiency:} Generating 4096 SAM masks per image is prohibitively expensive (minutes per image even on modern GPUs)
    \item \textbf{Semantic blindness:} Grid points are placed uniformly without considering object locations, wasting computation on background regions
\end{itemize}

The intelligent extraction inverts this paradigm: SCLIP first identifies \textit{where} objects are likely to exist, then SAM2 is prompted only at those semantically meaningful locations. This achieves a 96\% reduction in prompts (50-300 vs 4096) while maintaining competitive accuracy.

\subsubsection{Complete Prompt Extraction Pipeline}

Given SCLIP's dense prediction outputs from the previous stage:
\begin{itemize}
    \item Segmentation map: $\hat{c} \in \mathbb{R}^{H \times W}$ (predicted class per pixel)
    \item Probability map: $P \in \mathbb{R}^{H \times W \times C}$ (confidence scores for $C$ classes)
\end{itemize}

We extract semantically meaningful prompt points through the following five-stage pipeline:

\paragraph{Stage 1: Per-Class Confidence Masking}

For each class $c \in \{1, \ldots, C\}$, a binary confidence mask is created identifying high-certainty regions:

\begin{equation}
M_c^{\text{conf}}(x,y) = \begin{cases}
1 & \text{if } \hat{c}(x,y) = c \text{ and } P_{x,y,c} > \tau_{\text{conf}} \\
0 & \text{otherwise}
\end{cases}
\end{equation}

where $\tau_{\text{conf}}$ is the confidence threshold (typically 0.7). This filters out low-confidence predictions that may be noise or ambiguous regions. The threshold is deliberately set high to ensure SAM2 prompts are placed only at locations where CLIP has strong semantic evidence.

\paragraph{Stage 2: Connected Component Analysis}

For each class's confidence mask $M_c^{\text{conf}}$, connected component labeling is applied to identify spatially disjoint object instances. This clustering step groups adjacent pixels belonging to the same semantic class into discrete regions:

\begin{equation}
\text{CC}(M_c^{\text{conf}}) \rightarrow \{R_c^1, R_c^2, \ldots, R_c^{N_c}\}
\end{equation}

where $R_c^i \subseteq \{(x,y) : M_c^{\text{conf}}(x,y) = 1\}$ represents the $i$-th connected region for class $c$.

We use 8-connectivity (considering diagonal neighbors) to ensure that objects touching at corners are grouped together. This connected component analysis efficiently identifies disjoint regions using standard graph-based labeling algorithms.

\textbf{Why clustering matters:} Without clustering, all pixels of class "car" would be treated as a single object. Clustering separates individual car instances, allowing SAM2 to generate distinct masks for each vehicle. For example, a street scene with 5 cars produces 5 separate connected components, each receiving its own prompt point.

\paragraph{Stage 3: Region Filtering by Size}

Many connected components are spurious detections (noise, texture patterns, shadows) that happen to exceed the confidence threshold locally. These are filtered using a minimum region size criterion:

\begin{equation}
\text{Keep } R_c^i \text{ if } |R_c^i| > \tau_{\text{area}}
\end{equation}

where $|R_c^i|$ denotes the number of pixels in region $R_c^i$, and $\tau_{\text{area}}$ is the minimum area threshold (typically 100 pixels).

This size filtering serves multiple purposes:
\begin{itemize}
    \item Removes noise and artifacts from SCLIP predictions
    \item Focuses computation on salient objects rather than tiny regions
    \item Prevents SAM2 from being distracted by spurious high-confidence pixels
\end{itemize}

For a 224×224 image (50,176 pixels), a threshold of 100 pixels means regions smaller than 0.2\% of the image area are ignored, which is reasonable for most semantic segmentation tasks.

\paragraph{Stage 4: Centroid Computation}

For each valid connected component $R_c^i$, a representative prompt point is extracted by computing the spatial centroid:

\begin{equation}
\mathbf{p}_c^i = (\bar{x}_c^i, \bar{y}_c^i) = \left( \frac{1}{|R_c^i|} \sum_{(x,y) \in R_c^i} x, \frac{1}{|R_c^i|} \sum_{(x,y) \in R_c^i} y \right)
\end{equation}

The centroid represents the geometric center of the region and typically falls near the object's interior, making it an ideal point prompt for SAM2. Unlike random sampling or boundary-based selection, centroids are:
\begin{itemize}
    \item \textbf{Stable:} Robust to minor segmentation boundary variations
    \item \textbf{Interior:} Likely to fall inside the object rather than on edges
    \item \textbf{Representative:} Capture the region's spatial extent
\end{itemize}

We round centroid coordinates to the nearest integer pixel location: $\mathbf{p}_c^i = (\text{round}(\bar{x}_c^i), \text{round}(\bar{y}_c^i))$.

\paragraph{Stage 5: Prompt Metadata Aggregation}

For each extracted prompt point $\mathbf{p}_c^i$, metadata is aggregated to inform downstream processing:

\begin{equation}
\text{Prompt}_c^i = \{
\mathbf{p}_c^i, \quad c, \quad \text{conf}_c^i, \quad |R_c^i|
\}
\end{equation}

where:
\begin{itemize}
    \item $\mathbf{p}_c^i \in \mathbb{R}^2$: Centroid coordinates $(x, y)$
    \item $c \in \{1, \ldots, C\}$: Class index (e.g., 1="car", 2="person")
    \item $\text{conf}_c^i = P_{\bar{x}_c^i, \bar{y}_c^i, c}$: CLIP confidence at centroid
    \item $|R_c^i| \in \mathbb{N}$: Region area in pixels
\end{itemize}

This metadata serves multiple purposes:
\begin{itemize}
    \item Class index $c$ enables direct class assignment to SAM2 masks (explained in next section)
    \item Confidence score allows prioritizing high-certainty prompts during processing
    \item Region size can be used for overlap resolution (larger regions take precedence)
\end{itemize}

\subsubsection{Algorithm Summary and Efficiency Analysis}

The complete prompt extraction algorithm processes all classes in parallel:

\begin{equation}
\text{Prompts} = \bigcup_{c=1}^{C} \left\{ \text{Prompt}_c^i : R_c^i \text{ valid after filtering} \right\}
\end{equation}

\textbf{Typical prompt counts:} For Pascal VOC 2012 images with 21 classes (20 objects + background), the extraction produces:
\begin{itemize}
    \item \textbf{Simple scenes:} 50-100 prompts (sparse objects, large background regions)
    \item \textbf{Complex scenes:} 200-300 prompts (multiple object instances per class)
    \item \textbf{Average:} $\sim$150 prompts per image
\end{itemize}

\textbf{Efficiency gains:} Compared to 64×64 grid sampling (4096 prompts):
\begin{equation}
\text{Reduction} = 1 - \frac{150}{4096} = 96.3\%
\end{equation}

This 27× reduction in SAM2 queries translates directly to 27× speedup in mask generation, enabling practical deployment on consumer hardware.

\subsubsection{Hyperparameter Selection}

Two key hyperparameters control the prompt extraction process:

\begin{itemize}
    \item \textbf{Confidence threshold $\tau_{\text{conf}}$:} Higher values (0.7-0.9) produce fewer but higher-quality prompts by requiring strong CLIP evidence. Lower values (0.3-0.5) increase recall at the cost of more false positives. The default of 0.7 balances precision and recall.

    \item \textbf{Minimum area $\tau_{\text{area}}$:} Larger values (200-500) filter more aggressively, reducing noise but potentially missing small objects. Smaller values (50-100) retain more detections but include spurious regions. The default of 100 pixels is appropriate for objects of interest in most semantic segmentation benchmarks.
\end{itemize}

These thresholds can be adjusted based on application requirements: surveillance systems may prefer high recall (lower thresholds), while precision-critical applications (medical imaging) may demand higher thresholds.

\subsubsection{Direct Class Assignment}

Unlike methods that require voting or post-processing, the approach performs direct class assignment:

For each prompt point $p_i$ with extracted class label $c_i$:

\begin{equation}
\text{SAM2}(p_i) \rightarrow M_i \text{ where } M_i \text{ is assigned class } c_i
\end{equation}

SAM2 generates a high-quality instance mask $M_i$ at prompt $p_i$, and the class $c_i$ determined by SCLIP's prediction at that location is directly assigned. This simple yet effective strategy combines:

\begin{itemize}
    \item \textbf{SCLIP's semantic understanding:} Correct class assignment from vision-language features
    \item \textbf{SAM2's segmentation quality:} Precise object boundaries from specialized segmentation model
    \item \textbf{Efficient processing:} No need for majority voting or multi-scale evaluation
\end{itemize}

\subsection{Extension to Video Segmentation}

While the primary focus is on image segmentation evaluated on Pascal VOC 2012, the CLIP-guided prompting approach naturally extends to video segmentation by leveraging SAM2's native video tracking capabilities. This extension demonstrates that intelligent semantic guidance can be applied to temporal data while maintaining efficiency and requiring only first-frame analysis.

\subsubsection{SAM2 Video Architecture}

SAM2 \cite{ravi2024sam2} extends the original Segment Anything Model to video by introducing a memory-based architecture for temporal consistency. The key innovation is a streaming memory mechanism that maintains object identity across frames without requiring per-frame segmentation.

The video predictor operates through three main components:

\begin{enumerate}
    \item \textbf{Image encoder:} Processes individual frames to extract visual features (inherited from SAM)
    \item \textbf{Memory attention:} Maintains a memory bank of object features from previous frames
    \item \textbf{Mask decoder:} Generates segmentation masks using current frame features and memory context
\end{enumerate}

This architecture enables SAM2 to propagate segmentation masks across video frames by:
\begin{itemize}
    \item Storing high-confidence predictions in memory banks
    \item Retrieving relevant memories when processing subsequent frames
    \item Adapting to object appearance changes through continuous memory updates
    \item Handling temporary occlusions by relying on memory-based predictions
\end{itemize}

\subsubsection{CLIP-Guided Video Segmentation Pipeline}

Our video segmentation approach combines CLIP's semantic understanding for initial object detection with SAM2's temporal tracking for frame-to-frame propagation. The complete pipeline consists of five stages:

\paragraph{Stage 1: First Frame Extraction and CLIP Analysis}

We begin by extracting the first frame (frame 0) from the input video. This frame is converted to RGB color space to match CLIP's expected input format.

SCLIP dense prediction is then applied to frame 0 with the user-provided vocabulary (e.g., ["horse", "rider", "motorcycle", "person"]). This produces:
\begin{itemize}
    \item Segmentation map: $(H \times W)$ pixel-wise class predictions
    \item Probability map: $(H \times W \times C)$ confidence scores for each of $C$ classes
\end{itemize}

This first-frame analysis is the \textit{only} time CLIP processes the video, making the approach computationally efficient compared to per-frame dense prediction.

\paragraph{Stage 2: Semantic Prompt Extraction}

Using the same intelligent prompting strategy as the image pipeline, 50-300 semantic prompt points are extracted from high-confidence CLIP regions on frame 0. For each detected object class:

\begin{enumerate}
    \item Identify high-confidence regions: pixels where $p(c|x) > \tau_{\text{conf}}$ (typically $\tau_{\text{conf}} = 0.7$)
    \item Find connected components using binary morphological operations
    \item Filter regions by minimum size: $\text{area}(R) > \tau_{\text{area}}$ (typically $\tau_{\text{area}} = 100$ pixels)
    \item Extract centroid coordinates as prompt points: $\mathbf{p}_i = (\bar{x}_i, \bar{y}_i)$
    \item Associate each prompt with its class index and confidence score
\end{enumerate}

Each extracted prompt contains:
\begin{itemize}
    \item Spatial coordinates: $(x, y)$ pixel location
    \item Class identifier: Integer index (used as SAM2 object ID)
    \item Class label: Human-readable semantic label
    \item Confidence score: CLIP confidence at the centroid
    \item Region size: Pixel area of the detected region
\end{itemize}

\paragraph{Stage 3: SAM2 Video Predictor Initialization}

We initialize SAM2's video predictor with the following configuration:

\begin{itemize}
    \item \textbf{Model:} SAM2 Hiera-Large architecture
    \item \textbf{Video loading:} Video frames offloaded to CPU RAM to conserve GPU memory
    \item \textbf{State management:} Intermediate states maintained on CPU between frame processing
\end{itemize}

The predictor initializes an inference state containing:
\begin{itemize}
    \item Video metadata: number of frames, resolution, frame rate
    \item Memory banks: initially empty, populated during propagation
    \item Object registry: maps object IDs to their tracking states
\end{itemize}

For each extracted prompt point $\mathbf{p}_i$ with class index $c_i$, it is registered as a foreground point on frame 0. This associates the spatial location with the object class, using the class index as the unique object identifier for temporal tracking across the video.

\paragraph{Stage 4: Temporal Mask Propagation}

Once all prompts are registered on frame 0, SAM2 propagates masks across the entire video using its memory-based tracking mechanism. The propagation proceeds as follows:

\begin{enumerate}
    \item \textbf{Frame encoding:} Each frame $I_t$ is processed by the image encoder to produce visual features $\mathbf{f}_t$

    \item \textbf{Memory retrieval:} For each tracked object, SAM2 retrieves relevant memories from previous frames using cross-attention:
    \[
    \mathbf{m}_t = \text{Attention}(\mathbf{q}_t, \{\mathbf{k}_1, \ldots, \mathbf{k}_{t-1}\}, \{\mathbf{v}_1, \ldots, \mathbf{v}_{t-1}\})
    \]
    where $\mathbf{q}_t$ is the current frame query, and $\{\mathbf{k}_i, \mathbf{v}_i\}$ are memory key-value pairs from previous frames.

    \item \textbf{Mask prediction:} The mask decoder combines current features $\mathbf{f}_t$ and retrieved memories $\mathbf{m}_t$ to predict object masks:
    \[
    M_t^{(i)} = \text{Decoder}(\mathbf{f}_t, \mathbf{m}_t^{(i)})
    \]

    \item \textbf{Memory update:} High-confidence predictions are stored in the memory bank for future reference
\end{enumerate}

The propagation iterates through all video frames, applying this process sequentially. For each frame, binary masks are obtained by thresholding the mask logits at zero, yielding segmentation masks for all tracked objects across the entire video.

\paragraph{Stage 5: Video Rendering and Output}

The final stage generates a visualization video by overlaying colored segmentation masks on the original frames:

\begin{enumerate}
    \item \textbf{Frame reading:} Original video frames are read sequentially

    \item \textbf{Mask overlay:} For each frame $t$ and object $i$, the mask is blended with a distinct color:
    \[
    I'_t(x,y) = \begin{cases}
    0.5 \cdot I_t(x,y) + 0.5 \cdot \mathbf{c}_i & \text{if } M_t^{(i)}(x,y) = 1 \\
    I_t(x,y) & \text{otherwise}
    \end{cases}
    \]
    where $\mathbf{c}_i$ is a distinct RGB color assigned to class $i$ using a curated color palette with visually distinguishable hues.

    \item \textbf{Label rendering:} Class names are rendered at object centroids with high-contrast text for visibility across different backgrounds

    \item \textbf{Video encoding:} Frames are encoded using H.264 codec with standard YUV420p pixel format for universal compatibility. The implementation uses perceptually lossless quality settings with optimized file structure for streaming playback across web browsers and media players.
\end{enumerate}

\subsubsection{Memory Efficiency Optimizations}

Video processing presents significant memory challenges, especially on consumer GPUs with limited VRAM (e.g., GTX 1060 6GB). Several optimizations are implemented to enable video segmentation within these constraints:

\paragraph{CPU Offloading}

SAM2's video predictor supports offloading video frames and intermediate states to CPU RAM rather than keeping everything in GPU VRAM. Specifically:

\begin{itemize}
    \item \textbf{Video frame offloading:} Raw video frames are stored on CPU and loaded to GPU only when needed for processing. This prevents VRAM saturation when processing long videos.

    \item \textbf{State offloading:} Intermediate computation states (attention maps, memory banks) are maintained on CPU between frames, with only active states loaded to GPU during processing.
\end{itemize}

While this strategy increases CPU-GPU data transfer overhead, it enables processing videos that would otherwise exceed the 6GB VRAM capacity of the consumer-grade hardware used.

\paragraph{Temporary Frame Storage}

During video rendering, segmented frames are saved as temporary PNG files in a dedicated directory rather than kept in memory. This prevents RAM exhaustion on long videos. After FFmpeg encoding completes, the temporary directory is automatically cleaned up to free disk space.

\paragraph{Single-Pass Processing}

Unlike approaches that require multiple passes over the video (e.g., optical flow initialization + refinement), the pipeline processes each frame exactly once during the propagation stage, minimizing I/O and memory overhead.

\subsubsection{Implementation Considerations}

\textbf{One-time CLIP analysis:} Unlike per-frame dense prediction methods that run CLIP on every frame, this approach performs semantic analysis only on frame 0. SAM2's memory-based tracking handles temporal propagation, making video segmentation as efficient as image segmentation (plus video encoding overhead).

\textbf{Object ID assignment:} The implementation uses CLIP class indices as SAM2 object IDs. This creates a direct mapping between semantic classes and tracked objects. All instances of the same class (e.g., multiple "person" objects) are tracked under the same ID, which is suitable for class-level segmentation but does not distinguish individual instances.

\textbf{Limitation - No video inpainting:} While mask generation works for videos, video inpainting is not implemented due to computational resource constraints. Video diffusion models (e.g., Stable Video Diffusion) require significantly more GPU memory (typically 12-24GB VRAM) and processing time (minutes per frame) than single-frame inpainting, making them impractical on the hardware setup used (GTX 1060 6GB).

\textbf{Encoding compatibility:} The implementation deliberately chooses H.264 codec with YUV420p pixel format and faststart flag to ensure output videos are universally playable across platforms (web browsers, mobile devices, media players) and support efficient streaming playback.

\subsubsection{Video Examples and Applications}

Video segmentation is demonstrated on sports footage (MotoGP racing, NBA basketball) where CLIP identifies specific riders/players by name (e.g., "Valentino Rossi", "Stephen Curry") and SAM2 tracks them consistently across frames. Example vocabularies include:

\begin{itemize}
    \item \textbf{MotoGP:} ["motorcycle", "rider", "track", "tire barrier", "Valentino Rossi"]
    \item \textbf{NBA:} ["basketball player", "basketball", "court", "hoop", "Stephen Curry", "LeBron James"]
\end{itemize}

The system successfully tracks these objects through camera motion, pose changes, and partial occlusions, demonstrating the robustness of SAM2's memory-based approach when initialized with semantically meaningful CLIP-guided prompts.

This video extension enables applications such as:
\begin{itemize}
    \item Automated sports highlights generation (tracking specific players/teams)
    \item Video surveillance (detecting and tracking people/vehicles by description)
    \item Content-based video retrieval (finding frames containing described objects)
    \item Video dataset annotation (semi-automated labeling with human verification)
\end{itemize}

\subsection{Generative Editing Integration}

Once refined segmentation masks are obtained from images, Stable Diffusion v2 Inpainting \cite{rombach2022high} is optionally integrated for text-driven image modification. The inpainting model operates in the latent space of a variational autoencoder (VAE), enabling high-resolution generation with computational efficiency.

Several techniques are employed to ensure coherent results:
\begin{itemize}
    \item \textbf{Mask dilation:} Expand mask boundaries by 5-10 pixels for smoother blending
    \item \textbf{Classifier-free guidance:} Use guidance scale of 7.5 to balance prompt following and realism
    \item \textbf{Prompt engineering:} Reformulate user queries into detailed prompts suitable for the diffusion model
\end{itemize}

This completes the CLIP-guided prompting pipeline, enabling fully annotation-free open-vocabulary segmentation with high-quality boundaries and massive efficiency gains through intelligent semantic guidance.
