\chapter{Methodology}
\label{ch:methodology}

This chapter presents our systematic approach to enhancing SCLIP-based open-vocabulary semantic segmentation through four improvement phases. We integrate recent state-of-the-art techniques into a unified training-free framework, progressively addressing CLIP's limitations in spatial localization, human parsing, and computational efficiency.

\section{Baseline: SCLIP Dense Prediction Framework}
\label{sec:baseline_sclip}

Our work builds upon SCLIP \cite{sclip2024}, which modifies CLIP's final transformer layer to improve dense prediction quality. This section briefly describes the baseline architecture (detailed technical explanation in Annex A).

\subsection{Cross-layer Self-Attention (CSA)}

SCLIP replaces the standard attention mechanism in CLIP's final layer:

\begin{equation}
\text{Standard:} \quad A = \text{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)
\end{equation}

\begin{equation}
\text{CSA:} \quad A_{\text{CSA}} = \text{softmax}\left(\frac{QQ^T + KK^T}{\sqrt{d}}\right)
\end{equation}

This modification encourages spatial coherence: patches with similar queries or keys (likely belonging to the same object) mutually reinforce through self-similarity, producing more consistent segmentation masks.

\subsection{Dense Prediction Pipeline}

SCLIP processes images through four stages:

\begin{enumerate}
\item \textbf{Patch embedding:} 224×224 image → 14×14 grid of 16×16 patches → 196 tokens
\item \textbf{Transformer encoding:} 12 ViT layers, with CSA applied in layer 12
\item \textbf{Text encoding:} Vocabulary prompts → normalized text embeddings
\item \textbf{Similarity matching:} Dense dot-product between patch features and text embeddings → pixel-wise class predictions
\end{enumerate}

\textbf{Baseline performance:} 22.77\% mIoU on COCO-Stuff-164k (reported in SCLIP paper).

\begin{figure}[h]
\centering
\fbox{\parbox{0.9\textwidth}{\centering
\vspace{1cm}
\textbf{[FIGURE 3.1: SCLIP Baseline Architecture]}\\[0.5cm]
\textit{Show 4-stage pipeline:}\\[0.3cm]
\textbf{Input Image (224×224)} → \textbf{Patch Embedding (14×14 grid)}\\
→ \textbf{Transformer Layers 1-11 (standard attention)}\\
→ \textbf{Layer 12 (CSA: $QQ^T + KK^T$)}\\
→ \textbf{Dense Similarity Matching (14×14×C logits)}\\
→ \textbf{Upsample \& Softmax (H×W segmentation)}\\[0.3cm]
\textit{Highlight the CSA layer in a different color. Show example output: input image, predicted mask, ground truth.}
\vspace{1cm}
}}
\caption{SCLIP baseline architecture. Cross-layer Self-Attention in the final layer improves spatial coherence for dense prediction.}
\label{fig:sclip_baseline}
\end{figure}

\section{Phase 1: Spatial Enhancement and Boundary Refinement}

Phase 1 addresses CLIP's weak spatial localization through three complementary techniques targeting resolution, feature coherence, and boundary quality.

\subsection{LoftUp: Feature Upsampling}

\textbf{Motivation:} CLIP's 14×14 feature grid (for 224×224 input) provides only coarse spatial resolution. Naive bilinear upsampling introduces blur and semantic drift.

\textbf{Solution:} LoftUp \cite{loftup2025} learns to upsample CLIP features from 14×14 to 28×28 while preserving semantic content through:

\begin{itemize}
\item Learned interpolation kernels trained on vision-language alignment objectives
\item Semantic consistency loss ensuring upsampled features maintain similarity to text embeddings
\item Pre-trained weights available via torch.hub, enabling training-free integration
\end{itemize}

\textbf{Implementation:}
\begin{verbatim}
upsampler = torch.hub.load('loftup_model')
features_14x14 = sclip_encoder(image)  # [14, 14, 512]
features_28x28 = upsampler(features_14x14)  # [28, 28, 512]
\end{verbatim}

\textbf{Expected improvement:} +2-4\% mIoU by doubling effective spatial resolution.

\begin{figure}[h]
\centering
\fbox{\parbox{0.9\textwidth}{\centering
\vspace{1cm}
\textbf{[FIGURE 3.2: LoftUp Feature Upsampling]}\\[0.5cm]
\textit{Show comparison:}\\[0.3cm]
\textbf{Top row:} Input image | SCLIP 14×14 features (visualized) | Bilinear upsample 28×28\\
\textbf{Bottom row:} Input image | SCLIP 14×14 features | LoftUp upsample 28×28\\[0.3cm]
\textit{Highlight how LoftUp preserves sharp semantic boundaries compared to blurry bilinear interpolation. Use heatmap visualization.}
\vspace{1cm}
}}
\caption{LoftUp learns semantic-aware upsampling, preserving feature quality while doubling spatial resolution.}
\label{fig:loftup}
\end{figure}

\subsection{ResCLIP: Residual Attention Enhancement}

\textbf{Motivation:} Even with higher resolution, CLIP features lack spatial coherence within object regions. Adjacent patches of the same object may have inconsistent predictions.

\textbf{Solution:} ResCLIP \cite{resclip2025} introduces two mechanisms:

\subsubsection{Residual Cross-correlation Self-Attention (RCS)}

Enhances patch-to-patch similarity within semantic regions:

\begin{equation}
C = F F^T \quad \text{(cross-correlation matrix, } N \times N)
\end{equation}

\begin{equation}
F_{\text{enhanced}} = F + \alpha \cdot \text{softmax}(C) F
\end{equation}

where $\alpha$ controls the strength of residual enhancement (default 0.3).

\textbf{Intuition:} Patches with similar features (high $F_i \cdot F_j$) reinforce each other's representations, creating smoother, more coherent object regions.

\subsubsection{Semantic Feedback Refinement (SFR)}

Multi-scale coarse-to-fine refinement:

\begin{enumerate}
\item Generate coarse predictions at 14×14 resolution
\item Identify high-confidence regions as "semantic anchors"
\item Upsample to 28×28, using anchors to guide boundary placement
\item Iteratively refine predictions with feedback from previous scale
\end{enumerate}

\textbf{Implementation complexity:} Moderate. Requires 2-3 forward passes at different scales, increasing inference time by ~30\%.

\textbf{Expected improvement:} +8-13\% mIoU through enhanced spatial coherence.

\begin{figure}[h]
\centering
\fbox{\parbox{0.9\textwidth}{\centering
\vspace{1cm}
\textbf{[FIGURE 3.3: ResCLIP RCS and SFR]}\\[0.5cm]
\textit{Left panel: RCS Mechanism}\\
Show cross-correlation matrix C (heatmap) and how similar patches reinforce each other.\\[0.3cm]
\textit{Right panel: SFR Multi-Scale Refinement}\\
Show 3 stages: Coarse (14×14) → Medium (28×28) → Fine (H×W)\\
Visualize how high-confidence anchors guide boundary refinement.\\[0.3cm]
\textit{Include before/after segmentation: baseline SCLIP vs ResCLIP enhanced.}
\vspace{1cm}
}}
\caption{ResCLIP's RCS and SFR mechanisms enhance spatial coherence through patch similarity and multi-scale refinement.}
\label{fig:resclip}
\end{figure}

\subsection{DenseCRF: Boundary Refinement}

\textbf{Motivation:} Even with improved features, boundaries may be jagged due to independent pixel-wise predictions. Post-processing can enforce appearance consistency.

\textbf{Solution:} Dense Conditional Random Fields \cite{densecrf2011} define an energy function:

\begin{equation}
E(x) = \sum_i \psi_u(x_i) + \sum_{i<j} \psi_p(x_i, x_j)
\end{equation}

where:
\begin{itemize}
\item $\psi_u(x_i)$: Unary potential from SCLIP predictions (negative log-probability)
\item $\psi_p(x_i, x_j)$: Pairwise potential encouraging label agreement between similar pixels:
\end{itemize}

\begin{equation}
\psi_p(x_i, x_j) = \mu(x_i, x_j) \left[ w_1 \exp\left(-\frac{\|p_i - p_j\|^2}{2\sigma_{\alpha}^2}\right) + w_2 \exp\left(-\frac{\|p_i - p_j\|^2}{2\sigma_{\beta}^2} - \frac{\|I_i - I_j\|^2}{2\sigma_{\gamma}^2}\right) \right]
\end{equation}

The first term enforces spatial smoothness; the second enforces appearance consistency (similar-colored pixels prefer the same label).

\textbf{Implementation:} Uses pydensecrf library, 10 mean-field iterations.

\textbf{Expected improvement:} +1-2\% mIoU, +3-5\% boundary F1-score.

\begin{figure}[h]
\centering
\fbox{\parbox{0.9\textwidth}{\centering
\vspace{1cm}
\textbf{[FIGURE 3.4: DenseCRF Boundary Refinement]}\\[0.5cm]
\textit{Show 4-column comparison for 2 example images:}\\[0.3cm]
\textbf{Column 1:} Input image\\
\textbf{Column 2:} SCLIP baseline prediction (jagged boundaries)\\
\textbf{Column 3:} After DenseCRF (smooth boundaries)\\
\textbf{Column 4:} Ground truth\\[0.3cm]
\textit{Zoom insets highlighting boundary improvements. Show boundary F1 scores.}
\vspace{1cm}
}}
\caption{DenseCRF post-processing enforces appearance consistency, producing smooth object boundaries.}
\label{fig:densecrf}
\end{figure}

\subsection{Phase 1 Summary}

\textbf{Combined pipeline:}
\begin{equation}
\text{SCLIP} \xrightarrow{\text{LoftUp}} 28 \times 28 \xrightarrow{\text{ResCLIP}} \text{Enhanced features} \xrightarrow{\text{Similarity}} \text{Logits} \xrightarrow{\text{DenseCRF}} \text{Final mask}
\end{equation}

\textbf{Expected cumulative improvement:} +11-19\% mIoU over baseline.

\textbf{Ablation strategy:} Each component can be enabled/disabled independently for systematic evaluation.

\section{Phase 2A: Training-Free Human Parsing Enhancement}

Phase 2A specifically targets the "person" class, which exhibits poor baseline performance due to pose variation, clothing diversity, and CLIP's global feature aggregation bias.

\subsection{CLIPtrase: Self-Correlation Recalibration}

\textbf{Observation:} CLIP's self-attention tends to overly smooth local details, aggregating body parts into holistic "person" representations that lose fine-grained structure.

\textbf{Solution:} CLIPtrase \cite{cliptrase2024} recalibrates the correlation matrix to enhance local awareness:

\begin{equation}
C_{\text{orig}} = \text{softmax}(QK^T) \in \mathbb{R}^{N \times N}
\end{equation}

\begin{equation}
C_{\text{recal}} = C_{\text{orig}} \odot M_{\text{local}}
\end{equation}

where $M_{\text{local}}$ is a learned mask emphasizing short-range correlations (neighboring patches) over long-range aggregation.

\textbf{Implementation:} Training-free; uses pre-computed recalibration matrices from CLIPtrase paper.

\textbf{Expected improvement:} +5-10\% mIoU for person class by preserving body part boundaries.

\begin{figure}[h]
\centering
\fbox{\parbox{0.9\textwidth}{\centering
\vspace{1cm}
\textbf{[FIGURE 3.5: CLIPtrase Self-Correlation Recalibration]}\\[0.5cm]
\textit{Show attention visualization:}\\[0.3cm]
\textbf{Top:} Original CLIP attention (overly global, merges body parts)\\
\textbf{Bottom:} CLIPtrase recalibrated attention (local, preserves structure)\\[0.3cm]
\textit{Include example: person wearing striped shirt. Show how CLIPtrase correctly segments arms, torso separately instead of merging.}
\vspace{1cm}
}}
\caption{CLIPtrase recalibrates self-attention to emphasize local correlations, crucial for parsing articulated human poses.}
\label{fig:cliptrase}
\end{figure}

\subsection{CLIP-RC: Regional Clue Extraction}

\textbf{Observation:} CLIP features are dominated by global scene context. For humans, this means body part details are overshadowed by background and holistic pose information.

\textbf{Solution:} CLIP-RC \cite{cliprc2024} extracts regional features through spatial pyramid pooling:

\begin{enumerate}
\item Divide feature map into $k \times k$ grids (e.g., $k=3$ for 9 regions)
\item For each region, compute regional descriptor: $r_i = \text{MaxPool}(F_{\text{region}_i})$
\item Concatenate regional descriptors with global feature: $F_{\text{RC}} = [F_{\text{global}}; r_1; r_2; \ldots; r_9]$
\item Use $F_{\text{RC}}$ for similarity matching instead of $F_{\text{global}}$
\end{enumerate}

\textbf{Intuition:} Regional descriptors capture local variations (e.g., "head" region, "torso" region) that global pooling would average out.

\textbf{Expected improvement:} +8-12\% mIoU for person class.

\begin{figure}[h]
\centering
\fbox{\parbox{0.9\textwidth}{\centering
\vspace{1cm}
\textbf{[FIGURE 3.6: CLIP-RC Regional Clue Extraction]}\\[0.5cm]
\textit{Show spatial pyramid:}\\[0.3cm]
\textbf{Left:} Input image of person\\
\textbf{Middle:} 3×3 grid overlay showing regional divisions\\
\textbf{Right:} Feature extraction diagram showing regional descriptors concatenated\\[0.3cm]
\textit{Example: Basketball player. Show how different regions capture ball, jersey, legs separately.}
\vspace{1cm}
}}
\caption{CLIP-RC extracts regional features to combat global feature dominance, preserving body part details.}
\label{fig:cliprc}
\end{figure}

\subsection{Phase 2A Summary}

\textbf{Combined human parsing pipeline:}
\begin{equation}
\text{SCLIP} \xrightarrow{\text{CLIPtrase}} \text{Local attention} \xrightarrow{\text{CLIP-RC}} \text{Regional features} \rightarrow \text{Enhanced person masks}
\end{equation}

\textbf{Expected improvement:} +7-12\% overall mIoU, +13-22\% for person class specifically.

\textbf{Compatibility:} Can be combined with Phase 1 enhancements for cumulative gains.

\section{Phase 2B: Prompt Engineering and Template Optimization}

Phase 2B addresses computational inefficiency by optimizing text prompt templates used for CLIP encoding.

\subsection{Baseline: 80-Template Ensembling}

Standard practice uses 80 generic ImageNet classification templates:

\begin{verbatim}
templates = [
    "a photo of a {}",
    "a rendering of a {}",
    "{} in the scene",
    ...  # 80 total
]
\end{verbatim}

For each class, compute: $e_c = \frac{1}{80} \sum_{i=1}^{80} \text{CLIP}_{\text{text}}(\text{template}_i(c))$

\textbf{Problem:} Computationally expensive (80× text encoding) and not optimized for dense prediction.

\subsection{Top-7 Template Strategy}

PixelCLIP research identified 7 templates most effective for segmentation:

\begin{verbatim}
top7_templates = [
    "a photo of a {}",
    "{} in the image",
    "this is a {}",
    "there is a {}",
    "{} in the scene",
    "a picture of a {}",
    "an image of a {}"
]
\end{verbatim}

\textbf{Speedup:} $80/7 \approx 11.4\times$ faster text encoding.

\textbf{Expected improvement:} +2-3\% mIoU (slightly better than 80-template due to segmentation-specific curation).

\subsection{Adaptive Template Selection}

Different object types benefit from different prompts:

\begin{itemize}
\item \textbf{Stuff classes} (sky, road, grass): "this is {}", "the {}"
\item \textbf{Things classes} (car, person, dog): "a photo of a {}", "{} in the image"
\item \textbf{Material classes} (wall-brick, floor-marble): "a {} surface", "{} texture"
\end{itemize}

\textbf{Implementation:} Pre-classify COCO-Stuff categories into stuff/things/material, apply appropriate template subset.

\textbf{Expected improvement:} +3-5\% mIoU over generic templates.

\begin{figure}[h]
\centering
\fbox{\parbox{0.9\textwidth}{\centering
\vspace{1cm}
\textbf{[FIGURE 3.7: Template Engineering Results]}\\[0.5cm]
\textit{Show bar chart:}\\[0.3cm]
\textbf{X-axis:} Template strategy (80-generic, Top-7, Adaptive)\\
\textbf{Y-axis Left:} mIoU (\%)\\
\textbf{Y-axis Right:} Inference time (seconds)\\[0.3cm]
\textit{Include dual y-axis showing accuracy vs speed trade-off.}\\
\textit{Highlight: Top-7 achieves 11.4× speedup with minimal accuracy loss; Adaptive achieves best accuracy.}
\vspace{1cm}
}}
\caption{Prompt engineering achieves substantial speedups and accuracy gains through task-specific template curation.}
\label{fig:templates}
\end{figure}

\subsection{Phase 2B Summary}

\textbf{Expected improvement:} +3-5\% mIoU with 3-11× computational speedup.

\textbf{Implementation note:} Template selection is a simple configuration change, no model modification required.

\section{Phase 2C: Confidence Sharpening (Work in Progress)}

Phase 2C addresses a common failure mode: flat prediction distributions where multiple classes have similar confidence scores, causing noisy segmentation.

\subsection{Hierarchical Class Grouping}

\textbf{Observation:} Predicting among 171 COCO-Stuff classes simultaneously is challenging. Many errors occur within semantically similar groups (e.g., confusing "car" with "bus").

\textbf{Solution:} Two-stage hierarchical prediction:

\textbf{Stage 1:} Classify into coarse groups (stuff vs things, or semantic categories like "vehicle", "furniture", "nature")

\textbf{Stage 2:} Within predicted group, classify into specific classes

\textbf{Implementation:} Define class hierarchy manually or via hierarchical clustering of CLIP embeddings.

\textbf{Expected improvement:} +3-5\% mIoU by reducing classification complexity.

\subsection{Confidence Calibration}

\textbf{Observation:} Softmax often produces overconfident predictions even for ambiguous pixels.

\textbf{Solution:} Temperature scaling and entropy-based filtering:

\begin{equation}
P_{\text{calibrated}} = \text{softmax}(L / T_{\text{calib}})
\end{equation}

where $T_{\text{calib}} < T_{\text{original}}$ sharpens the distribution for high-entropy pixels (uncertain regions).

\textbf{Expected improvement:} +2-3\% mIoU by reducing label noise.

\subsection{Phase 2C Status}

\textbf{Current status:} Implemented but not fully evaluated in this thesis. Preliminary experiments show promising results.

\textbf{Expected total improvement:} +5-8\% mIoU.

\section{Integrated System Architecture}

Figure~\ref{fig:full_pipeline} shows the complete system with all phases enabled.

\begin{figure}[h]
\centering
\fbox{\parbox{0.9\textwidth}{\centering
\vspace{1cm}
\textbf{[FIGURE 3.8: Full Integrated Pipeline]}\\[0.5cm]
\textit{Show complete dataflow diagram:}\\[0.3cm]
\textbf{Input Image}\\
→ \textbf{SCLIP Encoder (CSA)}\\
→ \textbf{Phase 2A (CLIPtrase + CLIP-RC)} [optional]\\
→ \textbf{Phase 1 (LoftUp → ResCLIP RCS + SFR)} [optional]\\
→ \textbf{Dense Similarity} (with Phase 2B optimized templates)\\
→ \textbf{Phase 1 (DenseCRF)} [optional]\\
→ \textbf{Phase 2C (Hierarchical + Calibration)} [optional]\\
→ \textbf{Final Segmentation}\\[0.3cm]
\textit{Use different colors for each phase. Include toggle switches showing modular design.}
\vspace{1cm}
}}
\caption{Integrated pipeline with all enhancement phases. Each phase can be independently enabled for ablation studies.}
\label{fig:full_pipeline}
\end{figure}

\section{Implementation and Computational Considerations}

\subsection{Modular Design}

Our implementation allows independent control of each phase:

\begin{verbatim}
config = {
    'use_loftup': True,
    'use_resclip_rcs': True,
    'use_resclip_sfr': True,
    'use_densecrf': True,
    'use_cliptrase': False,  # Enable for human parsing
    'use_cliprc': False,
    'template_strategy': 'top7',  # or 'adaptive'
    'use_hierarchical': False,  # Phase 2C
}
\end{verbatim}

\subsection{Computational Cost}

\textbf{Baseline SCLIP:} ~8-10s per image on GTX 1060 6GB

\textbf{Phase 1 (all):} ~12-15s per image (+30-50\% overhead)
\begin{itemize}
\item LoftUp: +1s (learned upsampling)
\item ResCLIP: +2-4s (multi-scale refinement)
\item DenseCRF: +1s (mean-field inference)
\end{itemize}

\textbf{Phase 2A:} +2-3s per image (correlation matrix computation)

\textbf{Phase 2B:} \textit{Reduces} time by 5-7s (fewer text encodings)

\textbf{Total (all phases):} ~15-20s per image (still practical for offline evaluation)

\subsection{Memory Requirements}

\textbf{Peak GPU memory (GTX 1060 6GB):}
\begin{itemize}
\item Baseline SCLIP: ~4.5GB
\item +LoftUp: ~5.0GB (larger feature maps)
\item +ResCLIP: ~5.8GB (multi-scale features)
\item +DenseCRF: CPU-only, no GPU overhead
\end{itemize}

All phases fit within 6GB VRAM constraint through careful memory management.

\section{Summary}

This chapter presented our systematic approach to enhancing SCLIP-based open-vocabulary segmentation:

\begin{itemize}
\item \textbf{Phase 1:} Spatial enhancement (LoftUp, ResCLIP, DenseCRF) → +11-19\% mIoU
\item \textbf{Phase 2A:} Human parsing (CLIPtrase, CLIP-RC) → +7-12\% mIoU overall, +13-22\% for person
\item \textbf{Phase 2B:} Prompt engineering → +3-5\% mIoU with 3-11× speedup
\item \textbf{Phase 2C:} Confidence sharpening (in progress) → +5-8\% mIoU
\end{itemize}

\textbf{Expected cumulative improvement:} +17-32\% mIoU over baseline SCLIP (22.77\%), targeting 40-48\% on COCO-Stuff-164k.

Chapter 4 presents experimental validation of these improvements through comprehensive ablation studies.
