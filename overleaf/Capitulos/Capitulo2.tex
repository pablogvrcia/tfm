\chapter{Methodology}
\label{ch:methodology}

This chapter details the methodology employed to develop open-vocabulary semantic segmentation systems. Unlike traditional approaches that rely on closed-vocabulary classifiers, our work leverages recent advances in vision-language models and promptable segmentation to enable flexible, language-driven image understanding and manipulation.

We explore two complementary methodological approaches that represent different paradigms in CLIP-based segmentation:

\begin{itemize}
    \item \textbf{Approach 1: Proposal-based (SAM2+CLIP)} - Uses SAM 2 to generate high-quality mask proposals, then scores them using CLIP's vision-language features. Achieves 69.3\% mIoU on PASCAL VOC and integrates with generative AI for image editing.

    \item \textbf{Approach 2: Extended SCLIP with SAM2 refinement} - Builds upon SCLIP's Cross-layer Self-Attention (CSA) for dense prediction, extending it with a novel SAM2-based mask refinement layer. Achieves 49.52\% mIoU on COCO-Stuff and 48.09\% mIoU on PASCAL VOC in annotation-free settings.
\end{itemize}

Building upon insights from MaskCLIP \cite{zhou2022extract}, CLIPSeg \cite{luddecke2022clipseg}, SCLIP \cite{sclip2024}, and SAM 2 \cite{ravi2024sam2}, this chapter presents both methodologies, discusses their relative strengths, and demonstrates their complementary nature for different types of segmentation tasks.

\section{Approach 1: Proposal-Based Segmentation (SAM2+CLIP)}

Our first approach combines multiple complementary techniques to achieve robust open-vocabulary semantic segmentation and editing. The pipeline consists of four main stages: (1) dense vision-language feature extraction, (2) mask proposal generation, (3) mask-text alignment and selection, and (4) generative inpainting. Figure~\ref{fig:system_pipeline} provides a high-level overview of this pipeline.

\subsection{System Overview}

\begin{figure}[h]
\centering
\fbox{\parbox{0.9\textwidth}{\centering
\vspace{1cm}
\textbf{[PLACEHOLDER: System Pipeline Diagram]}\\[0.5cm]
\textit{This figure should show the complete pipeline with 4 stages flowing left-to-right:}\\[0.3cm]
\begin{tabular}{l}
\textbf{Stage 1:} Input image → CLIP ViT-L/14 → Dense feature maps (layers 6, 12, 18, 24)\\
\textbf{Stage 2:} Input image → SAM 2 automatic mask generation → 100-300 mask proposals\\
\textbf{Stage 3:} Feature maps + Masks + Text prompt → Similarity scoring → Top-K masks\\
\textbf{Stage 4:} Selected masks + Text prompt → Stable Diffusion Inpainting → Output image\\[0.3cm]
\end{tabular}
\textit{Include visual examples at each stage (e.g., feature heatmaps, mask overlays, final result).}\\
\textit{Use arrows to show data flow and highlight the integration points between components.}
\vspace{1cm}
}}
\caption{Overview of the proposed open-vocabulary semantic segmentation and editing pipeline. The system combines CLIP's vision-language features, SAM 2's mask generation, and Stable Diffusion's inpainting capabilities.}
\label{fig:system_pipeline}
\end{figure}

The key insight motivating our design is that different components excel at different aspects of the task: CLIP-based models \cite{radford2021learning} provide rich semantic understanding aligned with language, SAM 2 \cite{ravi2024sam2} generates high-quality segmentation masks with precise boundaries, and diffusion models \cite{rombach2022high} enable realistic content generation. By combining these strengths, we achieve both semantic flexibility and visual quality.

\subsection{Dense Vision-Language Feature Extraction}
\label{sec:dense_features}

Following the approach of MaskCLIP \cite{zhou2022extract} and CLIPSeg \cite{luddecke2022clipseg}, we extract dense vision-language features that capture pixel-level semantic information. Unlike standard CLIP, which produces image-level embeddings, we need spatially-resolved features to determine which image regions correspond to a given text prompt.

We explore two complementary strategies:

\begin{itemize}
    \item \textbf{Modified CLIP Architecture (CLIPSeg approach):} We augment the standard CLIP image encoder with additional decoder layers that produce dense per-pixel predictions. This follows CLIPSeg's transformer-based decoder design, which takes CLIP's intermediate features and progressively upsamples them to produce dense segmentation maps. The decoder is lightweight, adding minimal computational overhead while enabling fine-grained spatial reasoning.

    \item \textbf{Direct Feature Extraction (MaskCLIP approach):} Alternatively, we extract multi-scale features directly from CLIP's vision transformer, using features from multiple layers to capture both high-level semantic information and low-level spatial details. Following MaskCLIP, we compute similarity maps by comparing these dense features with text embeddings, creating pixel-wise affinity scores without requiring additional training.
\end{itemize}

\begin{figure}[h]
\centering
\fbox{\parbox{0.9\textwidth}{\centering
\vspace{1cm}
\textbf{[PLACEHOLDER: Multi-Scale CLIP Feature Visualization]}\\[0.5cm]
\textit{This figure should illustrate the multi-scale feature extraction process:}\\[0.3cm]
\begin{tabular}{l}
\textbf{Left:} Input image (e.g., a living room scene)\\
\textbf{Middle:} Four heatmaps showing CLIP features from layers 6, 12, 18, and 24\\
\quad - Layer 6: Low-level edges and textures\\
\quad - Layer 12: Mid-level patterns and shapes\\
\quad - Layer 18: Object parts and regions\\
\quad - Layer 24: High-level semantic concepts\\
\textbf{Right:} Combined multi-scale feature map with similarity scores for prompt "sofa"\\[0.3cm]
\end{tabular}
\textit{Use color-coded heatmaps (e.g., blue=low similarity, red=high similarity).}
\vspace{1cm}
}}
\caption{Multi-scale CLIP feature extraction from transformer layers. Different layers capture information at varying levels of abstraction, which are combined to produce spatially-resolved semantic features.}
\label{fig:multiscale_features}
\end{figure}

For text encoding, we use CLIP's text encoder to generate embeddings for user-provided prompts. To improve robustness, we employ prompt ensembling \cite{zhou2022learning}, generating multiple variations of the input text (e.g., ``a photo of a \{object\}'', ``\{object\} in the scene'') and averaging their embeddings.

\subsection{Mask Proposal Generation with SAM 2}

While dense CLIP features provide semantic information, they may lack precise object boundaries. To address this, we leverage SAM 2 \cite{ravi2024sam2} to generate high-quality mask proposals with accurate delineation.

SAM 2 operates in automatic mask generation mode, producing a comprehensive set of object masks across multiple scales without requiring explicit prompts. The model's hierarchical mask generation ensures coverage of objects at different granularities—from small details to large scene elements. This is crucial for open-vocabulary settings where we cannot anticipate which objects users might query.

Key advantages of SAM 2 include:
\begin{itemize}
    \item \textbf{Training-free deployment:} SAM 2 requires no fine-tuning, avoiding potential overfitting to specific object categories.
    \item \textbf{High-quality boundaries:} The model produces masks with precise edges, superior to those obtainable from CLIP features alone.
    \item \textbf{Comprehensive coverage:} Automatic mask generation yields hundreds of candidate masks per image, ensuring thorough coverage of all potential objects.
\end{itemize}

\begin{figure}[h]
\centering
\fbox{\parbox{0.9\textwidth}{\centering
\vspace{1cm}
\textbf{[PLACEHOLDER: SAM 2 Mask Generation Visualization]}\\[0.5cm]
\textit{This figure should demonstrate SAM 2's hierarchical mask generation:}\\[0.3cm]
\begin{tabular}{l}
\textbf{Left:} Input image (e.g., street scene with cars, pedestrians, buildings)\\
\textbf{Middle:} SAM 2 automatic mask generation with color-coded masks\\
\quad - Show \textasciitilde150-200 overlapping masks at different scales\\
\quad - Use semi-transparent colors to visualize overlaps\\
\textbf{Right:} Hierarchical grouping showing masks at 3 scales:\\
\quad - Fine scale: Small objects (traffic signs, windows)\\
\quad - Medium scale: People, cars\\
\quad - Coarse scale: Buildings, road segments\\[0.3cm]
\end{tabular}
\textit{Annotate a few masks with their IoU confidence scores (e.g., 0.92, 0.88, 0.95).}
\vspace{1cm}
}}
\caption{SAM 2 automatic mask generation produces comprehensive coverage at multiple scales. The hierarchical structure ensures both fine-grained details and large scene elements are captured.}
\label{fig:sam2_masks}
\end{figure}

\subsection{Mask-Text Alignment and Selection}

Given the dense vision-language features from Section~\ref{sec:dense_features} and mask proposals from SAM 2, we now align masks with the user's text query. This step determines which of SAM 2's masks correspond to the concept specified by the user.

For each mask $M_i$, we compute a relevance score $S_i$ that quantifies its alignment with the text prompt $t$:

\begin{equation}
S_i = \frac{1}{|M_i|} \sum_{p \in M_i} \text{sim}(f_p, e_t)
\end{equation}

where $f_p$ is the CLIP feature at pixel $p$, $e_t$ is the text embedding, $|M_i|$ is the number of pixels in mask $M_i$, and $\text{sim}(\cdot, \cdot)$ denotes cosine similarity.

To improve robustness, we incorporate several refinements inspired by recent work:

\begin{itemize}
    \item \textbf{Background suppression:} We compute a background score using negative prompts (e.g., ``background'', ``nothing'') and subtract it from object scores, reducing false positives from uniform regions.

    \item \textbf{Multi-scale aggregation:} Following MaskCLIP, we extract features from multiple CLIP layers and combine their similarity scores, capturing both semantic and spatial information.

    \item \textbf{Adaptive thresholding:} Rather than using a fixed threshold, we select the top-K masks or apply percentile-based thresholding, adapting to the distribution of similarity scores in each image.
\end{itemize}

\subsection{Generative Inpainting with Stable Diffusion}

Once semantically relevant masks are identified, we employ Stable Diffusion v2 Inpainting \cite{rombach2022high} to modify the selected regions according to user instructions. The inpainting model operates in the latent space of a variational autoencoder (VAE), enabling high-resolution generation with computational efficiency.

The inpainting process proceeds as follows:
\begin{enumerate}
    \item The original image and binary mask are encoded into the VAE's latent space.
    \item The masked latent region is iteratively denoised using the diffusion model, conditioned on both the text prompt and the surrounding unmasked context.
    \item The final latent representation is decoded back to pixel space, producing the modified image.
\end{enumerate}

We employ several techniques to ensure coherent and high-quality results:
\begin{itemize}
    \item \textbf{Prompt engineering:} User queries are reformulated into detailed prompts suitable for the diffusion model, incorporating context from the original image.
    \item \textbf{Classifier-free guidance:} We use high guidance scales (typically 7-10) to ensure generated content closely follows the text prompt while maintaining realism.
    \item \textbf{Mask refinement:} Mask boundaries are slightly blurred to ensure smooth transitions between inpainted and original regions, preventing visible seams.
\end{itemize}

The integration of these components—dense CLIP features, SAM 2 masks, and diffusion-based inpainting—yields a flexible system capable of understanding and modifying images based solely on natural language instructions, without requiring category-specific training or annotations.

\section{Implementation Details}
This section provides specific implementation details for each component of our system, including model selection, hyperparameters, and design choices that impact performance.

\subsection{CLIP Feature Extraction}

We use the CLIP ViT-L/14 variant, which provides a good balance between computational efficiency and feature quality. The vision transformer processes images at $336 \times 336$ resolution, producing a $24 \times 24$ grid of patch embeddings. To obtain dense features:

\begin{itemize}
    \item We extract features from multiple transformer layers (layers 6, 12, 18, and 24), capturing information at different levels of abstraction.
    \item Each layer's features are bilinearly upsampled to a common resolution and concatenated, following MaskCLIP's multi-scale approach.
    \item The resulting feature map has dimensions $H \times W \times D$, where $D$ is the combined feature dimension.
\end{itemize}

For text encoding, we generate prompt ensembles using templates: ``a photo of a \{class\}'', ``\{class\} in a scene'', ``a rendering of a \{class\}''. The resulting embeddings are averaged to improve robustness to prompt variations.

\subsection{SAM 2 Configuration}

We employ SAM 2 in automatic mask generation mode with the following parameters:

\begin{itemize}
    \item \textbf{Points per side:} 32 (generating a $32 \times 32$ grid of point prompts)
    \item \textbf{Predicted IoU threshold:} 0.88 (filtering low-confidence masks)
    \item \textbf{Stability score threshold:} 0.95 (ensuring stable mask predictions)
    \item \textbf{Crop overlap ratio:} 512/1500 (for handling large images through cropping)
\end{itemize}

These settings typically generate 100-300 mask candidates per image, providing comprehensive coverage across different object scales. We use the ``sam2\_hiera\_large'' checkpoint, which offers strong performance while remaining computationally tractable.

\subsection{Mask Scoring and Selection}

For each mask $M_i$, we employ a multi-scale CLIP voting strategy to compute robust alignment scores. Instead of evaluating masks at a single resolution, we extract CLIP features at three scales to capture different levels of detail:

\subsubsection{Multi-Scale CLIP Voting}

To address CLIP's sensitivity to object scale, we resize each masked region to three target resolutions:
\begin{itemize}
    \item \textbf{224px:} Captures fine-grained details, beneficial for small objects
    \item \textbf{336px:} Standard CLIP resolution, provides balanced semantic features
    \item \textbf{512px:} Captures broader context, helps with large objects
\end{itemize}

For each scale $s \in \{224, 336, 512\}$, we compute the cosine similarity between the resized mask region's CLIP embedding and the text embedding:

\begin{equation}
S_i^{(s)} = \text{sim}(f_{M_i}^{(s)}, e_t)
\end{equation}

where $f_{M_i}^{(s)}$ is the CLIP image embedding of mask region $M_i$ at scale $s$. The final similarity score is computed as a weighted average:

\begin{equation}
S_i = 0.2 \cdot S_i^{(224)} + 0.5 \cdot S_i^{(336)} + 0.3 \cdot S_i^{(512)}
\end{equation}

The weights reflect that the standard 336px resolution typically provides the most reliable features, while 224px and 512px contribute complementary information for scale robustness.

\subsubsection{Background Suppression and Confuser Penalization}

We also compute a background score $S_{\text{bg}}$ using negative prompts (``background'', ``nothing'', ``empty space'') and a confuser score $S_{\text{conf}}$ that penalizes common mismatches (e.g., distinguishing ``tire'' from ``grille''). The final score incorporates all three components:

\begin{equation}
S_i^{\text{final}} = S_i - \alpha \cdot S_{\text{bg}} - \beta \cdot S_{\text{conf}}
\end{equation}

with $\alpha = 0.3$ and $\beta = 0.3$ to balance object similarity against background and confuser suppression. Masks are filtered using adaptive thresholds based on mask size (0.15-0.20), and multiple non-overlapping instances are retained to handle queries that may refer to multiple objects (e.g., ``person'' in a group photo).

\subsubsection{Multi-Instance Selection Strategy}

A key challenge in open-vocabulary segmentation is determining how many masks to select for a given query. A query like ``car'' typically refers to a single complete object, while ``tire'' might refer to four parts, and ``person'' in a group photo should capture all individuals. We employ a multi-instance selection strategy that addresses this challenge:

\begin{enumerate}
    \item \textbf{Size-based filtering:} Masks smaller than 0.1\% of the image area are discarded as noise.

    \item \textbf{Score-based selection:} Masks must exceed an adaptive threshold (0.15 for large masks, 0.20 for small masks) based on their multi-scale similarity score.

    \item \textbf{Non-maximum suppression:} For each class, masks are sorted by size (largest first) and selected iteratively. A new mask is added only if it has less than 70\% overlap with already-selected masks of the same class, ensuring diverse coverage of distinct object instances.

    \item \textbf{Confidence-based assignment:} When multiple classes' masks overlap, pixels are assigned to the class with the highest confidence score, enabling proper handling of class boundaries and occlusions.
\end{enumerate}

This strategy ensures that the system adaptively selects the appropriate number of masks based on the query semantics and image content, capturing single objects, multiple instances, or object parts as needed.

\subsection{Stable Diffusion Inpainting}

We use the \texttt{stabilityai/stable-diffusion-2-inpainting} model with the following configuration:

\begin{itemize}
    \item \textbf{Inference steps:} 50 (balancing quality and speed)
    \item \textbf{Guidance scale:} 7.5 (standard value for classifier-free guidance)
    \item \textbf{Negative prompts:} ``blurry, low quality, distorted, artifacts'' (improving output quality)
    \item \textbf{Mask blur radius:} 8 pixels (ensuring smooth transitions)
\end{itemize}

Before inpainting, we dilate the selected mask by 5 pixels to ensure complete coverage of the target region and prevent edge artifacts. The diffusion model operates on $512 \times 512$ images; larger images are processed in tiles with overlapping regions to maintain coherence.

\subsection{Computational Considerations}

The pipeline's computational requirements are as follows:
\begin{itemize}
    \item \textbf{CLIP feature extraction:} $\sim$100ms per image (ViT-L/14 on GPU)
    \item \textbf{SAM 2 mask generation:} $\sim$2-4 seconds per image (depending on image size)
    \item \textbf{Mask scoring:} $\sim$50ms for 200 masks
    \item \textbf{Stable Diffusion inpainting:} $\sim$5-10 seconds per mask (50 steps)
\end{itemize}

Total processing time for end-to-end segmentation and editing is typically 10-20 seconds per image on an NVIDIA RTX 3090 GPU. The most expensive component is Stable Diffusion; for applications requiring real-time performance, fewer diffusion steps (20-30) can be used with minimal quality degradation.

\section{Benefits and Applications}
This methodology offers several advantages:
\begin{itemize}
    \item \textbf{Open-Vocabulary Flexibility:}  
    The system does not rely on a fixed set of class labels. Users can specify arbitrary concepts in natural language, and the pipeline adapts by filtering segmentation masks accordingly.

    \item \textbf{Interactive and Semantic Editing:}  
    Instead of manually selecting regions to edit, users can rely on semantic descriptions. For example, describing “the red car” will allow the system to segment and modify that object without further manual intervention.

    \item \textbf{Generative Capabilities:}  
    The integration of stable diffusion inpainting models allows for creative image modifications. Objects can be removed, replaced, or stylized based on textual instructions. This leads to endless possibilities in content creation, data augmentation, and user-driven image editing.
\end{itemize}

In summary, this proposal-based approach leverages promptable segmentation from SAM2, open-vocabulary alignment from CLIP, and generative inpainting from Stable Diffusion v2 to create a versatile tool. It seamlessly combines segmentation, natural language understanding, and generative transformations into a unified pipeline, achieving 69.3\% mIoU on PASCAL VOC and enabling interactive semantic editing.

\section{Approach 2: Extended SCLIP with Novel SAM2 Refinement}

As a complementary exploration, we investigate dense prediction methods that directly extract pixel-wise semantic labels from CLIP without relying on mask proposals. We build upon SCLIP's \cite{sclip2024} Cross-layer Self-Attention (CSA) mechanism for improved dense feature quality, and extend it with a novel SAM2-based mask refinement layer that significantly improves segmentation accuracy through majority voting.

\subsection{Motivation and Design Philosophy}

While the proposal-based approach (Section 2.1) excels at segmenting discrete objects with well-defined boundaries, dense prediction methods offer advantages for:

\begin{itemize}
    \item \textbf{Stuff classes:} Amorphous regions like sky, grass, water that lack clear object boundaries
    \item \textbf{Fine-grained semantic understanding:} Pixel-level classification captures subtle semantic variations
    \item \textbf{Datasets with many classes:} COCO-Stuff (171 classes) benefits from dense semantic reasoning rather than proposal filtering
    \item \textbf{Training-free deployment:} No reliance on external mask generators, purely CLIP-based
\end{itemize}

SCLIP addresses a fundamental limitation of standard CLIP: its self-attention mechanism is optimized for image-level classification, not dense prediction. By modifying the attention computation, SCLIP produces spatially-coherent features suitable for pixel-wise segmentation.

\subsection{SCLIP's Cross-layer Self-Attention (CSA) Foundation}

Our dense prediction approach leverages SCLIP's Cross-layer Self-Attention (CSA), which modifies the standard self-attention mechanism in CLIP's Vision Transformer to produce better features for dense prediction. Standard self-attention computes:

\begin{equation}
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V
\end{equation}

where $Q$, $K$, $V$ are query, key, and value matrices. This formulation captures query-key relationships but may miss important spatial correlations. SCLIP's CSA instead computes:

\begin{equation}
\text{CSA}(Q, K, V) = \text{softmax}\left(\frac{QQ^T + KK^T}{\sqrt{d}}\right)V
\end{equation}

This modification introduces two key changes that we utilize in our approach:

\begin{itemize}
    \item \textbf{Query-query similarity ($QQ^T$):} Captures relationships between different spatial positions based on their query representations, encouraging spatial consistency
    \item \textbf{Key-key similarity ($KK^T$):} Captures relationships based on key representations, providing complementary structural information
\end{itemize}

By leveraging CSA, we obtain attention maps that are more spatially coherent and better suited for dense prediction tasks. The attention mechanism now considers not just ``which keys match this query'' but also ``which queries are similar to each other'' and ``which keys are similar to each other,'' leading to smoother, more consistent segmentation maps that serve as the foundation for our SAM2 refinement layer.

\subsection{Multi-Layer Feature Aggregation}

Following MaskCLIP \cite{zhou2022extract} and ITACLIP \cite{shao2024itaclip}, we extract features from multiple ViT layers to capture information at different semantic levels:

\begin{itemize}
    \item \textbf{Layer 6 (early):} Low-level features (edges, textures, colors)
    \item \textbf{Layer 12 (middle):} Mid-level features (object parts, patterns)
    \item \textbf{Layer 18 (late-middle):} High-level semantic features
    \item \textbf{Layer 24 (final):} Abstract semantic concepts
\end{itemize}

For each layer $\ell$, we extract patch features $F_\ell \in \mathbb{R}^{H_p \times W_p \times D}$ where $H_p, W_p$ are the patch grid dimensions and $D$ is the feature dimension. These features are upsampled to the original image resolution using bilinear interpolation.

\subsection{Dense Prediction Pipeline}

The SCLIP dense prediction pipeline consists of the following steps:

\begin{enumerate}
    \item \textbf{Feature extraction:} Extract CSA-enhanced features from layers 6, 12, 18, 24 of CLIP ViT-B/16

    \item \textbf{Text encoding:} Encode class labels using CLIP's text encoder with prompt templates (``a photo of a \{class\}'')

    \item \textbf{Similarity computation:} Compute pixel-wise cosine similarity between image features and text embeddings:
    \begin{equation}
    S_{\ell}(x, y, c) = \text{sim}(F_\ell(x, y), T_c)
    \end{equation}
    where $F_\ell(x, y)$ is the feature at pixel $(x, y)$ from layer $\ell$ and $T_c$ is the text embedding for class $c$

    \item \textbf{Multi-layer fusion:} Aggregate similarity maps across layers using weighted averaging:
    \begin{equation}
    S(x, y, c) = \sum_{\ell \in \{6,12,18,24\}} w_\ell \cdot S_\ell(x, y, c)
    \end{equation}
    with equal weights $w_\ell = 0.25$

    \item \textbf{Temperature scaling:} Apply temperature scaling to sharpen the distribution:
    \begin{equation}
    P(x, y, c) = \frac{\exp(S(x, y, c) / T)}{\sum_{c'} \exp(S(x, y, c') / T)}
    \end{equation}
    with temperature $T = 0.01$ following SCLIP's configuration

    \item \textbf{Prediction:} Assign each pixel to the class with maximum probability:
    \begin{equation}
    \hat{y}(x, y) = \arg\max_c P(x, y, c)
    \end{equation}
\end{enumerate}

This pure SCLIP approach (without SAM2 refinement) achieves 38.50\% mIoU on PASCAL VOC and 35.41\% mIoU on COCO-Stuff, representing a 10.3$\times$ improvement over naive CLIP baseline (4.68\% VOC).

\subsection{Novel SAM2 Mask Refinement Layer}

While SCLIP's dense predictions capture rich semantic information, boundaries may be imprecise. This is our key extension: we propose a novel SAM2-based refinement layer that combines SCLIP's semantic understanding with SAM2's boundary quality through majority voting:

\begin{enumerate}
    \item \textbf{SAM2 mask generation:} Generate comprehensive mask proposals using SAM2's automatic mode (same configuration as Approach 1)

    \item \textbf{Majority voting:} For each SAM2 mask $M_i$, determine its predicted class by majority voting over SCLIP predictions within the mask:
    \begin{equation}
    \hat{c}_i = \arg\max_c \sum_{(x,y) \in M_i} \mathbb{1}[\hat{y}(x,y) = c]
    \end{equation}

    \item \textbf{Mask filtering:} Retain only masks where the majority class covers at least 60\% of the mask area, ensuring semantic consistency

    \item \textbf{Final prediction:} For pixels covered by multiple masks, assign the class with highest average SCLIP confidence within the mask
\end{enumerate}

This novel refinement layer represents our key contribution to extending SCLIP. By leveraging SAM2's superior boundary quality while maintaining SCLIP's semantic understanding, our refinement strategy significantly improves results to 48.09\% mIoU on PASCAL VOC (+24.9\% relative improvement over SCLIP alone) and 49.52\% mIoU on COCO-Stuff (+39.9\% relative improvement), achieving 83\% improvement over state-of-the-art ITACLIP (27.0\%) on COCO-Stuff.

\subsection{Text Feature Caching}

A key optimization for deployment efficiency is text feature caching. Since text embeddings for class labels are constant across all images, we pre-compute and cache them:

\begin{itemize}
    \item \textbf{Preprocessing:} Encode all class labels once using CLIP's text encoder
    \item \textbf{Storage:} Cache embeddings in memory (171 classes $\times$ 512 dimensions $\approx$ 350KB)
    \item \textbf{Inference:} Reuse cached embeddings for all images in the dataset
\end{itemize}

This optimization provides a 41\% speedup (37.55s → 26.57s per image on first vs. subsequent images) with zero accuracy loss, making the approach practical for large-scale evaluation.

\subsection{Computational Considerations}

The SCLIP+SAM2 pipeline has the following computational profile:

\begin{itemize}
    \item \textbf{CSA feature extraction:} $\sim$500ms per image (4 layers from ViT-B/16)
    \item \textbf{Multi-layer upsampling:} $\sim$200ms (bilinear interpolation to full resolution)
    \item \textbf{Similarity computation:} $\sim$100ms (171 classes for COCO-Stuff)
    \item \textbf{SAM2 mask generation:} $\sim$2-4 seconds per image
    \item \textbf{Majority voting refinement:} $\sim$300ms (200 masks $\times$ 171 classes)
\end{itemize}

Total inference time is approximately 27-30 seconds per image on an NVIDIA RTX 3090 GPU, which is 6.75$\times$ slower than the proposal-based approach but still practical for offline evaluation. The text caching optimization amortizes the text encoding cost across the dataset.

\section{Comparative Analysis and Method Selection}

The two approaches represent complementary paradigms in open-vocabulary segmentation:

\subsection{Proposal-Based (SAM2+CLIP) Strengths}
\begin{itemize}
    \item \textbf{Speed:} 2-4s per image (6.75$\times$ faster than dense prediction)
    \item \textbf{Discrete objects:} Excels on PASCAL VOC (69.3\% mIoU) with well-defined objects
    \item \textbf{Generative integration:} Seamless connection to Stable Diffusion for image editing
    \item \textbf{Multi-instance handling:} Adaptive selection strategy for variable object counts
    \item \textbf{Precise boundaries:} SAM2's high-quality masks provide accurate object delineation
\end{itemize}

\subsection{Dense Prediction (SCLIP+SAM2) Strengths}
\begin{itemize}
    \item \textbf{Stuff classes:} Excels on COCO-Stuff (49.52\% mIoU), 83\% better than ITACLIP (27.0\%)
    \item \textbf{Semantic consistency:} CSA attention produces spatially coherent predictions
    \item \textbf{Fine-grained understanding:} Pixel-level classification captures subtle semantic variations
    \item \textbf{Training-free:} Purely CLIP-based, no dependence on external mask generators for core prediction
    \item \textbf{Dense semantic scenes:} Better for datasets with many overlapping semantic regions
\end{itemize}

\subsection{Method Selection Guidelines}

Based on our empirical evaluation, we recommend:

\begin{itemize}
    \item \textbf{Use Proposal-Based for:}
    \begin{itemize}
        \item Datasets dominated by discrete objects (VOC, Objects365)
        \item Applications requiring speed (<5s per image)
        \item Interactive image editing scenarios
        \item Multi-instance object detection and manipulation
    \end{itemize}

    \item \textbf{Use Dense Prediction for:}
    \begin{itemize}
        \item Datasets with many stuff classes (COCO-Stuff, ADE20K)
        \item Semantic scene understanding tasks
        \item Applications prioritizing fine-grained semantic consistency
        \item Scenarios where boundary precision is less critical than semantic coverage
    \end{itemize}
\end{itemize}

\subsection{Hybrid Potential}

Future work could explore hybrid approaches that combine both methodologies:
\begin{itemize}
    \item Use proposal-based for thing classes (discrete objects)
    \item Use dense prediction for stuff classes (amorphous regions)
    \item Ensemble predictions using confidence-weighted averaging
    \item Adaptive method selection based on query type (object vs. stuff)
\end{itemize}

In summary, this chapter has presented two complementary open-vocabulary segmentation methodologies, each with distinct strengths. The proposal-based approach achieves state-of-the-art results on discrete object datasets and enables interactive editing, while the dense prediction approach excels on stuff-heavy datasets with superior semantic consistency. Together, they demonstrate the versatility of CLIP-based segmentation across diverse application scenarios.
