\chapter{Methodology}
\label{ch:methodology}

This chapter details the methodology employed to develop our open-vocabulary semantic segmentation system. Unlike traditional approaches that rely on closed-vocabulary classifiers, our work leverages recent advances in vision-language models and promptable segmentation to enable flexible, language-driven image understanding and manipulation.

Our contribution is a CLIP-guided prompting approach that combines SCLIP's dense predictions with SAM2's segmentation quality through intelligent prompt extraction. This fully annotation-free method extracts semantic prompt points from CLIP's high-confidence regions, using SAM2 to generate high-quality masks with direct class assignment.

Building upon insights from SCLIP \cite{sclip2024}, MaskCLIP \cite{zhou2022extract}, CLIPSeg \cite{luddecke2022clipseg}, and SAM 2 \cite{ravi2024sam2}, this chapter presents our CLIP-guided prompting methodology in detail.

\section{CLIP-Guided Prompting Approach}
\label{sec:clip_guided_approach}

Our main contribution is a CLIP-guided prompting system that leverages SCLIP's Cross-layer Self-Attention mechanism to extract intelligent prompt points for SAM2 segmentation. This approach achieves fully annotation-free open-vocabulary segmentation by combining semantic understanding with high-quality boundary delineation through efficient, semantically-guided prompting.

\subsection{Motivation and Design Philosophy}

CLIP-guided prompting offers several key advantages for open-vocabulary segmentation:

\begin{itemize}
    \item \textbf{Efficient semantic guidance:} Instead of blindly prompting SAM2 at thousands of points (e.g., 4096 in a grid), CLIP identifies 50-300 semantically meaningful locations, reducing computational cost by 96\% while maintaining competitive accuracy.

    \item \textbf{Zero spatial user input:} Users provide only a text vocabulary; the system automatically determines where objects are located without requiring manual clicks or bounding boxes.

    \item \textbf{High-quality segmentation:} SAM2 provides precise object boundaries at each semantically-guided prompt point, combining CLIP's semantic understanding with SAM2's superior mask quality.

    \item \textbf{Training-free operation:} Uses frozen CLIP and SAM2 models without any fine-tuning, enabling zero-shot transfer to new domains.
\end{itemize}

Our key insight is to use CLIP's dense predictions not as the final output, but as intelligent guidance for SAM2 prompting, achieving both efficiency and quality.

\subsection{System Overview}

Our CLIP-guided prompting pipeline consists of four main stages:

\begin{enumerate}
    \item \textbf{Dense Semantic Prediction:} SCLIP extracts dense features using Cross-layer Self-Attention (CSA) and produces pixel-wise class predictions through similarity matching with text embeddings.

    \item \textbf{Intelligent Prompt Extraction:} Extract 50-300 representative points from high-confidence regions in SCLIP's predictions using connected component analysis and centroid computation.

    \item \textbf{SAM2 Segmentation:} Prompt SAM2 at each extracted point to generate high-quality instance masks with direct class assignment based on CLIP predictions.

    \item \textbf{Overlap Resolution:} Resolve overlapping masks through IoU-based filtering and confidence-based priority assignment.
\end{enumerate}

\begin{figure}[h]
\centering
\fbox{\parbox{0.9\textwidth}{\centering
\vspace{1cm}
\textbf{[PLACEHOLDER: CLIP-Guided Prompting Pipeline Diagram]}\\[0.5cm]
\textit{This figure should show the CLIP-guided prompting pipeline:}\\[0.3cm]
\begin{tabular}{l}
\textbf{Stage 1 - Dense CLIP Prediction:}\\
\quad Input image → SCLIP (ViT-B/16 with CSA) → Dense class probabilities (H×W×C)\\
\quad Apply Cross-layer Self-Attention → Extract dense patch features → Pixel-wise predictions\\[0.3cm]
\textbf{Stage 2 - Intelligent Prompt Extraction:}\\
\quad SCLIP predictions → Connected component analysis → Extract centroids\\
\quad Filter by confidence (>0.3) and size (>100 pixels) → 50-300 prompt points\\[0.3cm]
\textbf{Stage 3 - SAM2 Segmentation:}\\
\quad Prompt points → SAM2 predictor → High-quality instance masks\\
\quad Direct class assignment from CLIP predictions → Instance-level segmentation\\[0.3cm]
\textbf{Stage 4 - Overlap Resolution:}\\
\quad IoU-based filtering → Confidence-based priority → Final segmentation\\[0.3cm]
\end{tabular}
\textit{Include visualization: dense heatmap, extracted points, SAM2 masks, final result.}
\vspace{1cm}
}}
\caption{Overview of our CLIP-guided prompting pipeline. The system uses CLIP's dense predictions to extract intelligent prompt points, achieving 96\% reduction in prompts compared to blind grid sampling while maintaining competitive accuracy.}
\label{fig:clip_guided_pipeline}
\end{figure}

The key insight motivating our design is that CLIP's dense predictions identify where objects are located, enabling intelligent SAM2 prompting with far fewer points than blind grid sampling. By extracting points from SCLIP's high-confidence regions, we guide SAM2 to focus on semantically relevant areas, achieving both efficiency and accuracy.

\subsection{Foundation: Vision Transformer and CLIP Architecture}
\label{sec:vit_clip_foundation}

Before explaining SCLIP's modifications, we first establish how the Vision Transformer (ViT) and CLIP architectures function. Understanding these foundations is essential for comprehending SCLIP's Cross-layer Self-Attention innovation.

\subsubsection{Vision Transformer: From Images to Sequences}

The Vision Transformer \cite{dosovitskiy2020image} adapts the Transformer architecture \cite{vaswani2017attention} from NLP to computer vision by treating an image as a sequence of patches.

\textbf{Core Idea:} Instead of processing images through convolutional layers, ViT:
\begin{enumerate}
    \item Divides the image into fixed-size patches
    \item Linearly embeds each patch
    \item Processes the patch sequence through standard Transformer layers
    \item Uses the output for classification or (in our case) dense prediction
\end{enumerate}

\textbf{Why this works:} Transformers excel at modeling long-range dependencies through self-attention. By treating spatial patches as sequence elements, ViT can capture relationships between distant image regions (e.g., "this patch of sky relates to that patch of sky") more effectively than CNNs with limited receptive fields.

\subsubsection{Self-Attention Mechanism: The Heart of Transformers}

Self-attention allows each element in a sequence to attend to all other elements, computing context-aware representations.

\textbf{Input:} Sequence of $N$ token embeddings $X \in \mathbb{R}^{N \times D}$

\textbf{Learnable Parameters:} Three weight matrices per attention head:
\begin{itemize}
    \item $W_Q \in \mathbb{R}^{D \times d_h}$ (Query projection)
    \item $W_K \in \mathbb{R}^{D \times d_h}$ (Key projection)
    \item $W_V \in \mathbb{R}^{D \times d_h}$ (Value projection)
\end{itemize}
where $d_h = D / H$ is the dimension per head ($H$ = number of heads, typically 8-16).

\textbf{Step-by-step computation:}

\textbf{(1) Project to Queries, Keys, Values:}
\begin{align}
Q &= X W_Q \in \mathbb{R}^{N \times d_h} \quad \text{(What am I looking for?)} \\
K &= X W_K \in \mathbb{R}^{N \times d_h} \quad \text{(What do I contain?)} \\
V &= X W_V \in \mathbb{R}^{N \times d_h} \quad \text{(What information do I provide?)}
\end{align}

\textbf{Intuition:}
\begin{itemize}
    \item Each token $i$ produces a \textit{query} $Q_i$: "What information am I seeking?"
    \item Each token $j$ produces a \textit{key} $K_j$: "What information do I offer?"
    \item Each token $j$ produces a \textit{value} $V_j$: "Here's my actual information"
\end{itemize}

\textbf{(2) Compute Attention Weights:}
\begin{equation}
A = \text{softmax}\left(\frac{Q K^T}{\sqrt{d_h}}\right) \in \mathbb{R}^{N \times N}
\end{equation}

This computes how much token $i$ should attend to token $j$:
\begin{equation}
A_{ij} = \frac{\exp(Q_i \cdot K_j / \sqrt{d_h})}{\sum_{k=1}^{N} \exp(Q_i \cdot K_k / \sqrt{d_h})}
\end{equation}

\textbf{Why $Q K^T$?} It measures compatibility: if token $i$'s query is similar to token $j$'s key (high dot product), then $j$ likely has relevant information for $i$.

\textbf{Why divide by $\sqrt{d_h}$?} Scaling factor prevents dot products from becoming too large in high dimensions, which would cause softmax to produce very peaked distributions (gradient saturation).

\textbf{(3) Weighted Aggregation:}
\begin{equation}
\text{Output} = A V \in \mathbb{R}^{N \times d_h}
\end{equation}

Each output token is a weighted sum of all value vectors:
\begin{equation}
\text{Output}_i = \sum_{j=1}^{N} A_{ij} V_j
\end{equation}

\textbf{Intuition:} Token $i$ aggregates information from all other tokens, weighted by attention scores. If $A_{i,j} = 0.7$ and $A_{i,k} = 0.3$, then output $i$ is 70\% influenced by token $j$ and 30\% by token $k$.

\subsubsection{Multi-Head Attention: Parallel Attention Patterns}

Instead of one attention mechanism, Transformers use $H$ parallel heads (e.g., $H=8$) to capture different types of relationships.

\textbf{Why multiple heads?} Different heads can specialize:
\begin{itemize}
    \item Head 1 might learn positional relationships ("nearby patches")
    \item Head 2 might learn color similarity
    \item Head 3 might learn semantic relationships ("all sky patches")
\end{itemize}

\textbf{Computation:}
\begin{align}
\text{head}_h &= \text{Attention}(X W_Q^h, X W_K^h, X W_V^h) \in \mathbb{R}^{N \times d_h} \\
\text{MultiHead}(X) &= \text{Concat}(\text{head}_1, \ldots, \text{head}_H) W_O \in \mathbb{R}^{N \times D}
\end{align}

where $W_O \in \mathbb{R}^{D \times D}$ is an output projection matrix.

\subsubsection{Complete Transformer Layer}

A full Transformer layer combines multi-head self-attention with position-wise feed-forward networks:

\textbf{(1) Multi-Head Self-Attention with Residual:}
\begin{align}
\hat{X} &= \text{LayerNorm}(X) \\
X' &= X + \text{MultiHead}(\hat{X})
\end{align}

\textbf{(2) Feed-Forward Network (MLP) with Residual:}
\begin{align}
\hat{X}' &= \text{LayerNorm}(X') \\
X_{\text{out}} &= X' + \text{MLP}(\hat{X}')
\end{align}

where MLP typically consists of two linear layers with GELU activation:
\begin{equation}
\text{MLP}(x) = W_2 \cdot \text{GELU}(W_1 x + b_1) + b_2
\end{equation}

\textbf{Why residual connections?} They allow gradients to flow directly through the network, enabling training of very deep models (12-24 layers for ViT).

\textbf{Why LayerNorm before (not after)?} Pre-normalization stabilizes training and is the modern standard (vs. original post-normalization).

\subsubsection{Vision Transformer (ViT) Complete Pipeline}

\textbf{Architecture: ViT-B/16 (used in CLIP and SCLIP)}
\begin{itemize}
    \item Patch size: $P = 16$ pixels
    \item Embedding dimension: $D = 512$
    \item Number of layers: $L = 12$
    \item Number of heads: $H = 8$
    \item MLP hidden dimension: $2048$ (4× expansion)
\end{itemize}

\textbf{Complete forward pass:}

\textbf{(a) Patch Embedding:}
\begin{equation}
X_{\text{patches}} = \text{LinearProjection}(\text{Flatten}(\text{Patches}(I))) \in \mathbb{R}^{N \times D}
\end{equation}
where $N = H_{\text{img}} W_{\text{img}} / P^2$ (e.g., $N = 224 \times 224 / 16^2 = 196$).

\textbf{(b) Prepend CLS Token:}
\begin{equation}
X_0 = [\text{CLS}; X_{\text{patches}}] \in \mathbb{R}^{(N+1) \times D}
\end{equation}

The CLS (classification) token is a learnable embedding that aggregates global image information. For classification, only the CLS token output is used. For dense prediction (our case), we use all patch tokens.

\textbf{(c) Add Position Embeddings:}
\begin{equation}
X_0 \leftarrow X_0 + E_{\text{pos}}
\end{equation}

Position embeddings are learned vectors that encode spatial location, allowing the model to distinguish between patches at different positions.

\textbf{(d) Process through $L$ Transformer Layers:}
\begin{equation}
X_\ell = \text{TransformerLayer}_\ell(X_{\ell-1}) \quad \text{for } \ell = 1, \ldots, 12
\end{equation}

\textbf{(e) Extract Features:}
\begin{itemize}
    \item \textbf{For classification:} Use CLS token: $X_{12}[0, :]$
    \item \textbf{For dense prediction:} Use all patch tokens: $X_{12}[1:, :]$
\end{itemize}

\subsubsection{CLIP: Connecting Vision and Language}

CLIP \cite{radford2021learning} trains a Vision Transformer and Text Transformer jointly to align image and text representations in a shared embedding space.

\textbf{Training Objective (Contrastive Learning):}

Given a batch of $B$ image-text pairs $(I_i, T_i)$:

\textbf{(1) Encode images and texts:}
\begin{align}
f_i &= \text{ViT}(I_i) \in \mathbb{R}^{D} \quad \text{(use CLS token)} \\
t_i &= \text{TextTransformer}(T_i) \in \mathbb{R}^{D}
\end{align}

\textbf{(2) Normalize:}
\begin{align}
f_i &\leftarrow f_i / \|f_i\|_2 \\
t_i &\leftarrow t_i / \|t_i\|_2
\end{align}

\textbf{(3) Compute similarity matrix:}
\begin{equation}
S_{ij} = f_i \cdot t_j \quad \in [-1, 1]
\end{equation}

\textbf{(4) Contrastive loss:} Maximize $S_{ii}$ (matching pairs) and minimize $S_{ij}$ for $i \neq j$ (non-matching pairs):
\begin{equation}
\mathcal{L} = -\frac{1}{B} \sum_{i=1}^{B} \left[ \log \frac{\exp(S_{ii}/\tau)}{\sum_{j=1}^{B} \exp(S_{ij}/\tau)} + \log \frac{\exp(S_{ii}/\tau)}{\sum_{j=1}^{B} \exp(S_{ji}/\tau)} \right]
\end{equation}

where $\tau$ is a learnable temperature parameter.

\textbf{Result:} After training on 400M image-text pairs, CLIP learns:
\begin{itemize}
    \item Images of "dogs" are close to text "a photo of a dog"
    \item Images of "cars" are close to text "a car on the road"
    \item Zero-shot transfer: Can classify/segment unseen categories by comparing to text embeddings
\end{itemize}

\textbf{Why CLIP enables open-vocabulary segmentation:}
\begin{enumerate}
    \item \textbf{Shared embedding space:} Both images and text map to the same 512-D space
    \item \textbf{Semantic understanding:} Learned on natural language descriptions, not just class labels
    \item \textbf{Zero-shot:} Given text "airplane", CLIP can recognize airplanes without airplane-specific training
\end{enumerate}

\subsection{SCLIP's Dense Feature Extraction: Complete Pipeline}
\label{sec:sclip_dense_extraction}

Building on the Vision Transformer and CLIP foundations, this section explains how SCLIP modifies CLIP's final attention layer to extract dense semantic features using Cross-layer Self-Attention (CSA).

\subsubsection{Step 1: Image Preprocessing and Patchification}

\textbf{Input:} RGB image $I \in \mathbb{R}^{H \times W \times 3}$ (e.g., 224×224×3)

\textbf{Normalization:}
\begin{equation}
I_{\text{norm}} = \frac{I / 255 - \mu}{\sigma}
\end{equation}
where $\mu$ and $\sigma$ are CLIP's standard ImageNet normalization parameters (mean and standard deviation per RGB channel).

\textbf{Patch Embedding:} The normalized image is divided into non-overlapping patches of size $P \times P$ (ViT-B/16 uses $P=16$):
\begin{equation}
\text{Number of patches: } N = \frac{H}{P} \times \frac{W}{P} = 14 \times 14 = 196 \text{ (for 224×224 input)}
\end{equation}

Each patch is linearly projected to a $D$-dimensional embedding ($D=512$ for ViT-B/16):
\begin{equation}
X_{\text{patch}} \in \mathbb{R}^{N \times D}
\end{equation}

A learnable [CLS] token is prepended:
\begin{equation}
X_0 = [\text{CLS}; X_{\text{patch}}] \in \mathbb{R}^{(N+1) \times D}
\end{equation}

\textbf{Positional Encoding:} Learnable positional embeddings $E_{\text{pos}} \in \mathbb{R}^{(N+1) \times D}$ are added:
\begin{equation}
X_0 \leftarrow X_0 + E_{\text{pos}}
\end{equation}

\subsubsection{Step 2: Transformer Layers with Standard Attention}

ViT-B/16 has 12 transformer layers. Layers 1-11 use standard multi-head self-attention (MHSA):

\textbf{For each layer $\ell = 1, \ldots, 11$:}

\textbf{(a) Layer Normalization:}
\begin{equation}
\hat{X}_{\ell-1} = \text{LayerNorm}(X_{\ell-1})
\end{equation}

\textbf{(b) Standard Multi-Head Self-Attention:}

For each attention head $h$:
\begin{align}
Q_h &= \hat{X}_{\ell-1} W_Q^h \in \mathbb{R}^{(N+1) \times d_h} \\
K_h &= \hat{X}_{\ell-1} W_K^h \in \mathbb{R}^{(N+1) \times d_h} \\
V_h &= \hat{X}_{\ell-1} W_V^h \in \mathbb{R}^{(N+1) \times d_h}
\end{align}

where $d_h = D / H$ (head dimension, $d_h = 512/8 = 64$ for ViT-B/16 with 8 heads).

\textbf{Standard attention computation:}
\begin{equation}
\text{Attention}_h(Q_h, K_h, V_h) = \text{softmax}\left(\frac{Q_h K_h^T}{\sqrt{d_h}}\right) V_h
\end{equation}

The attention matrix $A_h = \text{softmax}\left(\frac{Q_h K_h^T}{\sqrt{d_h}}\right) \in \mathbb{R}^{(N+1) \times (N+1)}$ represents how much each token attends to every other token.

\textbf{Concatenate all heads and project:}
\begin{equation}
\text{MHSA}(\hat{X}_{\ell-1}) = \text{Concat}(\text{Attention}_1, \ldots, \text{Attention}_H) W_O
\end{equation}

\textbf{(c) Residual Connection + MLP:}
\begin{align}
X'_{\ell} &= X_{\ell-1} + \text{MHSA}(\hat{X}_{\ell-1}) \\
X_{\ell} &= X'_{\ell} + \text{MLP}(\text{LayerNorm}(X'_{\ell}))
\end{align}

\subsubsection{Step 3: Final Layer with Cross-layer Self-Attention (CSA)}

\textbf{This is SCLIP's key innovation.} The final layer ($\ell = 12$) uses CSA instead of standard attention.

\textbf{(a) Layer Normalization:}
\begin{equation}
\hat{X}_{11} = \text{LayerNorm}(X_{11})
\end{equation}

\textbf{(b) Compute Q, K, V as before:}
\begin{align}
Q_h &= \hat{X}_{11} W_Q^h \in \mathbb{R}^{(N+1) \times d_h} \\
K_h &= \hat{X}_{11} W_K^h \in \mathbb{R}^{(N+1) \times d_h} \\
V_h &= \hat{X}_{11} W_V^h \in \mathbb{R}^{(N+1) \times d_h}
\end{align}

\textbf{(c) Cross-layer Self-Attention (CSA) - The Key Difference:}

Instead of computing $Q_h K_h^T$, CSA computes:
\begin{equation}
\boxed{A_h^{\text{CSA}} = \text{softmax}\left(\frac{Q_h Q_h^T + K_h K_h^T}{\sqrt{d_h}}\right)}
\end{equation}

\textbf{Why this works better for dense prediction:}

\begin{itemize}
    \item \textbf{$Q_h Q_h^T$:} Measures similarity between queries. If patch $i$ and patch $j$ have similar queries, they likely belong to the same semantic region (e.g., both are "sky"). This encourages \textit{spatial grouping}.

    \item \textbf{$K_h K_h^T$:} Measures similarity between keys. Provides complementary structural information about which patches should be grouped together.

    \item \textbf{Combined effect:} Patches in the same semantic region mutually reinforce each other through both query and key similarities, producing more spatially coherent features.

    \item \textbf{Contrast with standard $Q_h K_h^T$:} Standard attention measures query-key compatibility, which works well for global understanding (classification) but can create noisy, fragmented dense predictions.
\end{itemize}

\textbf{Apply attention to values:}
\begin{equation}
\text{CSA}_h(Q_h, K_h, V_h) = A_h^{\text{CSA}} V_h \in \mathbb{R}^{(N+1) \times d_h}
\end{equation}

\textbf{(d) Concatenate and project:}
\begin{equation}
X_{12} = X_{11} + \text{Concat}(\text{CSA}_1, \ldots, \text{CSA}_H) W_O + \text{MLP}(\cdots)
\end{equation}

\subsubsection{Step 4: Extract Dense Patch Features}

From the final layer output $X_{12} \in \mathbb{R}^{(N+1) \times D}$:

\textbf{(a) Remove CLS token:}
\begin{equation}
F_{\text{patch}} = X_{12}[1:, :] \in \mathbb{R}^{N \times D} = \mathbb{R}^{196 \times 512}
\end{equation}

\textbf{(b) Reshape to spatial grid:}
\begin{equation}
F_{\text{spatial}} = \text{reshape}(F_{\text{patch}}, (H_p, W_p, D)) \in \mathbb{R}^{14 \times 14 \times 512}
\end{equation}

where $H_p = W_p = \sqrt{N} = 14$ (for 224×224 input).

\textbf{(c) L2 Normalization:}
\begin{equation}
F_{\text{norm}}[i,j,:] = \frac{F_{\text{spatial}}[i,j,:]}{\|F_{\text{spatial}}[i,j,:]\|_2}
\end{equation}

Now $F_{\text{norm}} \in \mathbb{R}^{14 \times 14 \times 512}$ contains normalized dense features.

\subsubsection{Step 5: Text Encoding with Prompt Ensembling}

For each class name $c$ (e.g., "car"), we generate multiple text prompts using templates \cite{zhou2022learning}:

\textbf{Prompt templates:}
\begin{itemize}
    \item "a photo of a \{class\}"
    \item "\{class\} in the scene"
    \item "a rendering of a \{class\}"
    \item \ldots (80 templates total in practice)
\end{itemize}

For class "car", this produces: ["a photo of a car", "car in the scene", \ldots]

\textbf{Encode each prompt:} Use CLIP's text transformer to get embeddings:
\begin{equation}
t_{c,i} = \text{CLIP}_{\text{text}}(\text{template}_i(c)) \in \mathbb{R}^{D}
\end{equation}

\textbf{Average embeddings (prompt ensembling):}
\begin{equation}
e_c = \frac{1}{M} \sum_{i=1}^{M} t_{c,i} \in \mathbb{R}^{D}
\end{equation}
where $M$ is the number of templates.

\textbf{L2 Normalization:}
\begin{equation}
e_c \leftarrow \frac{e_c}{\|e_c\|_2}
\end{equation}

This gives us normalized text embeddings $e_c \in \mathbb{R}^{512}$ for each class.

\subsubsection{Step 6: Compute Dense Similarity Scores}

For each spatial location $(i, j)$ in the 14×14 grid and each class $c$:

\begin{equation}
S_{i,j,c} = F_{\text{norm}}[i,j,:] \cdot e_c \in [-1, 1]
\end{equation}

This is the cosine similarity (dot product of normalized vectors). We get a similarity tensor:
\begin{equation}
S \in \mathbb{R}^{14 \times 14 \times C}
\end{equation}
where $C$ is the number of classes (e.g., $C=3$ for ["car", "road", "background"]).

\subsubsection{Step 7: Upsample to Original Resolution}

The 14×14 similarity maps are upsampled to the original image resolution using bilinear interpolation:

\begin{equation}
S_{\text{full}} = \text{Upsample}(S, \text{size}=(H, W)) \in \mathbb{R}^{H \times W \times C}
\end{equation}

For example, if the original image is 512×512, we upsample from 14×14 to 512×512.

\subsubsection{Step 8: Generate Dense Predictions}

\textbf{(a) Apply temperature scaling:}
\begin{equation}
L = S_{\text{full}} \times T
\end{equation}
where $T = 40.0$ is the temperature (sharpens the distribution).

\textbf{(b) Convert to probabilities via softmax:}
\begin{equation}
P_{x,y,c} = \frac{\exp(L_{x,y,c})}{\sum_{c'=1}^{C} \exp(L_{x,y,c'})}
\end{equation}

\textbf{(c) Assign class labels:}
\begin{equation}
\boxed{\hat{c}(x, y) = \arg\max_{c} P_{x,y,c}}
\end{equation}

This produces the final dense segmentation mask $\hat{c} \in \mathbb{R}^{H \times W}$ where each pixel is assigned a class index.

\subsubsection{Summary and Implementation Considerations}

\begin{enumerate}
    \item \textbf{Minimal architectural modification:} CSA requires changing only the attention computation in the final layer from $Q K^T$ to $Q Q^T + K K^T$, making it straightforward to implement on top of existing CLIP models.

    \item \textbf{Spatial coherence through self-similarity:} The $Q Q^T$ and $K K^T$ terms measure patch-to-patch similarity within the same representation space, encouraging patches with similar semantic content to have strong mutual attention weights, thereby improving spatial consistency in dense predictions.

    \item \textbf{Training-free approach:} SCLIP leverages pre-trained CLIP weights without requiring fine-tuning or additional training data, applying CSA as a modification to the inference-time forward pass.

    \item \textbf{Resolution characteristics:} The Vision Transformer with 16×16 patches produces a 14×14 feature grid for 224×224 input images. These features must be upsampled to the target resolution via bilinear interpolation for pixel-level predictions.

    \item \textbf{Sliding window inference for high-resolution inputs:} Images larger than 224×224 (e.g., 2048×2048) are processed using overlapping 224×224 crops with 50\% stride overlap, and predictions are aggregated through averaging.

    \item \textbf{Temperature scaling for prediction sharpening:} The logit scale parameter $T = 40.0$ amplifies the differences between class scores before softmax, producing more confident and less ambiguous predictions.
\end{enumerate}


\subsection{Intelligent Prompt Extraction}

Our key contribution is intelligent prompt extraction that uses SCLIP's semantic understanding to guide SAM2 prompting, achieving massive efficiency gains without sacrificing accuracy. This section details the complete pipeline from SCLIP's dense predictions to semantically meaningful prompt points.

\subsubsection{Motivation: From Dense Predictions to Sparse Prompts}

Naive approaches to combining CLIP with SAM often use exhaustive grid sampling: prompting SAM at every point in a dense grid (e.g., 64×64 = 4096 points) and using CLIP to classify the resulting masks. This has two major drawbacks:

\begin{itemize}
    \item \textbf{Computational inefficiency:} Generating 4096 SAM masks per image is prohibitively expensive (minutes per image even on modern GPUs)
    \item \textbf{Semantic blindness:} Grid points are placed uniformly without considering object locations, wasting computation on background regions
\end{itemize}

Our intelligent extraction inverts this paradigm: we first run SCLIP to identify \textit{where} objects are likely to exist, then prompt SAM2 only at those semantically meaningful locations. This achieves a 96\% reduction in prompts (50-300 vs 4096) while maintaining competitive accuracy.

\subsubsection{Complete Prompt Extraction Pipeline}

Given SCLIP's dense prediction outputs from the previous stage:
\begin{itemize}
    \item Segmentation map: $\hat{c} \in \mathbb{R}^{H \times W}$ (predicted class per pixel)
    \item Probability map: $P \in \mathbb{R}^{H \times W \times C}$ (confidence scores for $C$ classes)
\end{itemize}

We extract semantically meaningful prompt points through the following five-stage pipeline:

\paragraph{Stage 1: Per-Class Confidence Masking}

For each class $c \in \{1, \ldots, C\}$, we create a binary confidence mask identifying high-certainty regions:

\begin{equation}
M_c^{\text{conf}}(x,y) = \begin{cases}
1 & \text{if } \hat{c}(x,y) = c \text{ and } P_{x,y,c} > \tau_{\text{conf}} \\
0 & \text{otherwise}
\end{cases}
\end{equation}

where $\tau_{\text{conf}}$ is the confidence threshold (typically 0.7). This filters out low-confidence predictions that may be noise or ambiguous regions. The threshold is deliberately set high to ensure SAM2 prompts are placed only at locations where CLIP has strong semantic evidence.

\paragraph{Stage 2: Connected Component Analysis}

For each class's confidence mask $M_c^{\text{conf}}$, we apply connected component labeling to identify spatially disjoint object instances. This clustering step groups adjacent pixels belonging to the same semantic class into discrete regions:

\begin{equation}
\text{CC}(M_c^{\text{conf}}) \rightarrow \{R_c^1, R_c^2, \ldots, R_c^{N_c}\}
\end{equation}

where $R_c^i \subseteq \{(x,y) : M_c^{\text{conf}}(x,y) = 1\}$ represents the $i$-th connected region for class $c$.

We use 8-connectivity (considering diagonal neighbors) to ensure that objects touching at corners are grouped together. This connected component analysis efficiently identifies disjoint regions using standard graph-based labeling algorithms.

\textbf{Why clustering matters:} Without clustering, all pixels of class "car" would be treated as a single object. Clustering separates individual car instances, allowing SAM2 to generate distinct masks for each vehicle. For example, a street scene with 5 cars produces 5 separate connected components, each receiving its own prompt point.

\paragraph{Stage 3: Region Filtering by Size}

Many connected components are spurious detections (noise, texture patterns, shadows) that happen to exceed the confidence threshold locally. We filter these using a minimum region size criterion:

\begin{equation}
\text{Keep } R_c^i \text{ if } |R_c^i| > \tau_{\text{area}}
\end{equation}

where $|R_c^i|$ denotes the number of pixels in region $R_c^i$, and $\tau_{\text{area}}$ is the minimum area threshold (typically 100 pixels).

This size filtering serves multiple purposes:
\begin{itemize}
    \item Removes noise and artifacts from SCLIP predictions
    \item Focuses computation on salient objects rather than tiny regions
    \item Prevents SAM2 from being distracted by spurious high-confidence pixels
\end{itemize}

For a 224×224 image (50,176 pixels), a threshold of 100 pixels means we ignore regions smaller than 0.2\% of the image area, which is reasonable for most semantic segmentation tasks.

\paragraph{Stage 4: Centroid Computation}

For each valid connected component $R_c^i$, we extract a representative prompt point by computing the spatial centroid:

\begin{equation}
\mathbf{p}_c^i = (\bar{x}_c^i, \bar{y}_c^i) = \left( \frac{1}{|R_c^i|} \sum_{(x,y) \in R_c^i} x, \frac{1}{|R_c^i|} \sum_{(x,y) \in R_c^i} y \right)
\end{equation}

The centroid represents the geometric center of the region and typically falls near the object's interior, making it an ideal point prompt for SAM2. Unlike random sampling or boundary-based selection, centroids are:
\begin{itemize}
    \item \textbf{Stable:} Robust to minor segmentation boundary variations
    \item \textbf{Interior:} Likely to fall inside the object rather than on edges
    \item \textbf{Representative:} Capture the region's spatial extent
\end{itemize}

We round centroid coordinates to the nearest integer pixel location: $\mathbf{p}_c^i = (\text{round}(\bar{x}_c^i), \text{round}(\bar{y}_c^i))$.

\paragraph{Stage 5: Prompt Metadata Aggregation}

For each extracted prompt point $\mathbf{p}_c^i$, we aggregate metadata to inform downstream processing:

\begin{equation}
\text{Prompt}_c^i = \{
\mathbf{p}_c^i, \quad c, \quad \text{conf}_c^i, \quad |R_c^i|
\}
\end{equation}

where:
\begin{itemize}
    \item $\mathbf{p}_c^i \in \mathbb{R}^2$: Centroid coordinates $(x, y)$
    \item $c \in \{1, \ldots, C\}$: Class index (e.g., 1="car", 2="person")
    \item $\text{conf}_c^i = P_{\bar{x}_c^i, \bar{y}_c^i, c}$: CLIP confidence at centroid
    \item $|R_c^i| \in \mathbb{N}$: Region area in pixels
\end{itemize}

This metadata serves multiple purposes:
\begin{itemize}
    \item Class index $c$ enables direct class assignment to SAM2 masks (explained in next section)
    \item Confidence score allows prioritizing high-certainty prompts during processing
    \item Region size can be used for overlap resolution (larger regions take precedence)
\end{itemize}

\subsubsection{Algorithm Summary and Efficiency Analysis}

The complete prompt extraction algorithm processes all classes in parallel:

\begin{equation}
\text{Prompts} = \bigcup_{c=1}^{C} \left\{ \text{Prompt}_c^i : R_c^i \text{ valid after filtering} \right\}
\end{equation}

\textbf{Typical prompt counts:} For Pascal VOC 2012 images with 21 classes (20 objects + background), we extract:
\begin{itemize}
    \item \textbf{Simple scenes:} 50-100 prompts (sparse objects, large background regions)
    \item \textbf{Complex scenes:} 200-300 prompts (multiple object instances per class)
    \item \textbf{Average:} $\sim$150 prompts per image
\end{itemize}

\textbf{Efficiency gains:} Compared to 64×64 grid sampling (4096 prompts):
\begin{equation}
\text{Reduction} = 1 - \frac{150}{4096} = 96.3\%
\end{equation}

This 27× reduction in SAM2 queries translates directly to 27× speedup in mask generation, enabling practical deployment on consumer hardware.

\subsubsection{Hyperparameter Selection}

Two key hyperparameters control the prompt extraction process:

\begin{itemize}
    \item \textbf{Confidence threshold $\tau_{\text{conf}}$:} Higher values (0.7-0.9) produce fewer but higher-quality prompts by requiring strong CLIP evidence. Lower values (0.3-0.5) increase recall at the cost of more false positives. We use 0.7 as default, balancing precision and recall.

    \item \textbf{Minimum area $\tau_{\text{area}}$:} Larger values (200-500) filter more aggressively, reducing noise but potentially missing small objects. Smaller values (50-100) retain more detections but include spurious regions. We use 100 pixels as default, appropriate for objects of interest in most semantic segmentation benchmarks.
\end{itemize}

These thresholds can be adjusted based on application requirements: surveillance systems may prefer high recall (lower thresholds), while precision-critical applications (medical imaging) may demand higher thresholds.

\subsubsection{Direct Class Assignment}

Unlike methods that require voting or post-processing, our approach performs direct class assignment:

For each prompt point $p_i$ with extracted class label $c_i$:

\begin{equation}
\text{SAM2}(p_i) \rightarrow M_i \text{ where } M_i \text{ is assigned class } c_i
\end{equation}

SAM2 generates a high-quality instance mask $M_i$ at prompt $p_i$, and we directly assign it the class $c_i$ determined by SCLIP's prediction at that location. This simple yet effective strategy combines:

\begin{itemize}
    \item \textbf{SCLIP's semantic understanding:} Correct class assignment from vision-language features
    \item \textbf{SAM2's segmentation quality:} Precise object boundaries from specialized segmentation model
    \item \textbf{Efficient processing:} No need for majority voting or multi-scale evaluation
\end{itemize}

\subsection{Extension to Video Segmentation}

While our primary focus is on image segmentation evaluated on Pascal VOC 2012, our CLIP-guided prompting approach naturally extends to video segmentation by leveraging SAM2's native video tracking capabilities. This extension demonstrates that intelligent semantic guidance can be applied to temporal data while maintaining efficiency and requiring only first-frame analysis.

\subsubsection{SAM2 Video Architecture}

SAM2 \cite{ravi2024sam2} extends the original Segment Anything Model to video by introducing a memory-based architecture for temporal consistency. The key innovation is a streaming memory mechanism that maintains object identity across frames without requiring per-frame segmentation.

The video predictor operates through three main components:

\begin{enumerate}
    \item \textbf{Image encoder:} Processes individual frames to extract visual features (inherited from SAM)
    \item \textbf{Memory attention:} Maintains a memory bank of object features from previous frames
    \item \textbf{Mask decoder:} Generates segmentation masks using current frame features and memory context
\end{enumerate}

This architecture enables SAM2 to propagate segmentation masks across video frames by:
\begin{itemize}
    \item Storing high-confidence predictions in memory banks
    \item Retrieving relevant memories when processing subsequent frames
    \item Adapting to object appearance changes through continuous memory updates
    \item Handling temporary occlusions by relying on memory-based predictions
\end{itemize}

\subsubsection{CLIP-Guided Video Segmentation Pipeline}

Our video segmentation approach combines CLIP's semantic understanding for initial object detection with SAM2's temporal tracking for frame-to-frame propagation. The complete pipeline consists of five stages:

\paragraph{Stage 1: First Frame Extraction and CLIP Analysis}

We begin by extracting the first frame (frame 0) from the input video. This frame is converted to RGB color space to match CLIP's expected input format.

SCLIP dense prediction is then applied to frame 0 with the user-provided vocabulary (e.g., ["horse", "rider", "motorcycle", "person"]). This produces:
\begin{itemize}
    \item Segmentation map: $(H \times W)$ pixel-wise class predictions
    \item Probability map: $(H \times W \times C)$ confidence scores for each of $C$ classes
\end{itemize}

This first-frame analysis is the \textit{only} time CLIP processes the video, making the approach computationally efficient compared to per-frame dense prediction.

\paragraph{Stage 2: Semantic Prompt Extraction}

Using the same intelligent prompting strategy as our image pipeline, we extract 50-300 semantic prompt points from high-confidence CLIP regions on frame 0. For each detected object class:

\begin{enumerate}
    \item Identify high-confidence regions: pixels where $p(c|x) > \tau_{\text{conf}}$ (typically $\tau_{\text{conf}} = 0.7$)
    \item Find connected components using binary morphological operations
    \item Filter regions by minimum size: $\text{area}(R) > \tau_{\text{area}}$ (typically $\tau_{\text{area}} = 100$ pixels)
    \item Extract centroid coordinates as prompt points: $\mathbf{p}_i = (\bar{x}_i, \bar{y}_i)$
    \item Associate each prompt with its class index and confidence score
\end{enumerate}

Each extracted prompt contains:
\begin{itemize}
    \item Spatial coordinates: $(x, y)$ pixel location
    \item Class identifier: Integer index (used as SAM2 object ID)
    \item Class label: Human-readable semantic label
    \item Confidence score: CLIP confidence at the centroid
    \item Region size: Pixel area of the detected region
\end{itemize}

\paragraph{Stage 3: SAM2 Video Predictor Initialization}

We initialize SAM2's video predictor with the following configuration:

\begin{itemize}
    \item \textbf{Model:} SAM2 Hiera-Large architecture
    \item \textbf{Video loading:} Video frames offloaded to CPU RAM to conserve GPU memory
    \item \textbf{State management:} Intermediate states maintained on CPU between frame processing
\end{itemize}

The predictor initializes an inference state containing:
\begin{itemize}
    \item Video metadata: number of frames, resolution, frame rate
    \item Memory banks: initially empty, populated during propagation
    \item Object registry: maps object IDs to their tracking states
\end{itemize}

For each extracted prompt point $\mathbf{p}_i$ with class index $c_i$, we register it as a foreground point on frame 0. This associates the spatial location with the object class, using the class index as the unique object identifier for temporal tracking across the video.

\paragraph{Stage 4: Temporal Mask Propagation}

Once all prompts are registered on frame 0, SAM2 propagates masks across the entire video using its memory-based tracking mechanism. The propagation proceeds as follows:

\begin{enumerate}
    \item \textbf{Frame encoding:} Each frame $I_t$ is processed by the image encoder to produce visual features $\mathbf{f}_t$

    \item \textbf{Memory retrieval:} For each tracked object, SAM2 retrieves relevant memories from previous frames using cross-attention:
    \[
    \mathbf{m}_t = \text{Attention}(\mathbf{q}_t, \{\mathbf{k}_1, \ldots, \mathbf{k}_{t-1}\}, \{\mathbf{v}_1, \ldots, \mathbf{v}_{t-1}\})
    \]
    where $\mathbf{q}_t$ is the current frame query, and $\{\mathbf{k}_i, \mathbf{v}_i\}$ are memory key-value pairs from previous frames.

    \item \textbf{Mask prediction:} The mask decoder combines current features $\mathbf{f}_t$ and retrieved memories $\mathbf{m}_t$ to predict object masks:
    \[
    M_t^{(i)} = \text{Decoder}(\mathbf{f}_t, \mathbf{m}_t^{(i)})
    \]

    \item \textbf{Memory update:} High-confidence predictions are stored in the memory bank for future reference
\end{enumerate}

The propagation iterates through all video frames, applying this process sequentially. For each frame, binary masks are obtained by thresholding the mask logits at zero, yielding segmentation masks for all tracked objects across the entire video.

\paragraph{Stage 5: Video Rendering and Output}

The final stage generates a visualization video by overlaying colored segmentation masks on the original frames:

\begin{enumerate}
    \item \textbf{Frame reading:} Original video frames are read sequentially

    \item \textbf{Mask overlay:} For each frame $t$ and object $i$, we blend the mask with a distinct color:
    \[
    I'_t(x,y) = \begin{cases}
    0.5 \cdot I_t(x,y) + 0.5 \cdot \mathbf{c}_i & \text{if } M_t^{(i)}(x,y) = 1 \\
    I_t(x,y) & \text{otherwise}
    \end{cases}
    \]
    where $\mathbf{c}_i$ is a distinct RGB color assigned to class $i$ using a curated color palette with visually distinguishable hues.

    \item \textbf{Label rendering:} Class names are rendered at object centroids with high-contrast text for visibility across different backgrounds

    \item \textbf{Video encoding:} Frames are encoded using H.264 codec with standard YUV420p pixel format for universal compatibility. We use perceptually lossless quality settings with optimized file structure for streaming playback across web browsers and media players.
\end{enumerate}

\subsubsection{Memory Efficiency Optimizations}

Video processing presents significant memory challenges, especially on consumer GPUs with limited VRAM (e.g., GTX 1060 6GB). We implement several optimizations to enable video segmentation within these constraints:

\paragraph{CPU Offloading}

SAM2's video predictor supports offloading video frames and intermediate states to CPU RAM rather than keeping everything in GPU VRAM. Specifically:

\begin{itemize}
    \item \textbf{Video frame offloading:} Raw video frames are stored on CPU and loaded to GPU only when needed for processing. This prevents VRAM saturation when processing long videos.

    \item \textbf{State offloading:} Intermediate computation states (attention maps, memory banks) are maintained on CPU between frames, with only active states loaded to GPU during processing.
\end{itemize}

While this strategy increases CPU-GPU data transfer overhead, it enables processing videos that would otherwise exceed the 6GB VRAM capacity of our consumer-grade hardware.

\paragraph{Temporary Frame Storage}

During video rendering, segmented frames are saved as temporary PNG files in a dedicated directory rather than kept in memory. This prevents RAM exhaustion on long videos. After FFmpeg encoding completes, the temporary directory is automatically cleaned up to free disk space.

\paragraph{Single-Pass Processing}

Unlike approaches that require multiple passes over the video (e.g., optical flow initialization + refinement), our pipeline processes each frame exactly once during the propagation stage, minimizing I/O and memory overhead.

\subsubsection{Implementation Considerations}

\textbf{One-time CLIP analysis:} Unlike per-frame dense prediction methods that run CLIP on every frame, our approach performs semantic analysis only on frame 0. SAM2's memory-based tracking handles temporal propagation, making video segmentation as efficient as image segmentation (plus video encoding overhead).

\textbf{Object ID assignment:} We use CLIP class indices as SAM2 object IDs. This creates a direct mapping between semantic classes and tracked objects. All instances of the same class (e.g., multiple "person" objects) are tracked under the same ID, which is suitable for class-level segmentation but does not distinguish individual instances.

\textbf{Limitation - No video inpainting:} While mask generation works for videos, we do not implement video inpainting due to computational resource constraints. Video diffusion models (e.g., Stable Video Diffusion) require significantly more GPU memory (typically 12-24GB VRAM) and processing time (minutes per frame) than single-frame inpainting, making them impractical on our hardware setup (GTX 1060 6GB with only 6GB VRAM).

\textbf{Encoding compatibility:} We deliberately choose H.264 codec with YUV420p pixel format and faststart flag to ensure output videos are universally playable across platforms (web browsers, mobile devices, media players) and support efficient streaming playback.

\subsubsection{Video Examples and Applications}

We demonstrate video segmentation on sports footage (MotoGP racing, NBA basketball) where CLIP identifies specific riders/players by name (e.g., "Valentino Rossi", "Stephen Curry") and SAM2 tracks them consistently across frames. Example vocabularies include:

\begin{itemize}
    \item \textbf{MotoGP:} ["motorcycle", "rider", "track", "tire barrier", "Valentino Rossi"]
    \item \textbf{NBA:} ["basketball player", "basketball", "court", "hoop", "Stephen Curry", "LeBron James"]
\end{itemize}

The system successfully tracks these objects through camera motion, pose changes, and partial occlusions, demonstrating the robustness of SAM2's memory-based approach when initialized with semantically meaningful CLIP-guided prompts.

This video extension enables applications such as:
\begin{itemize}
    \item Automated sports highlights generation (tracking specific players/teams)
    \item Video surveillance (detecting and tracking people/vehicles by description)
    \item Content-based video retrieval (finding frames containing described objects)
    \item Video dataset annotation (semi-automated labeling with human verification)
\end{itemize}

\subsection{Generative Editing Integration}

Once refined segmentation masks are obtained from images, we optionally integrate Stable Diffusion v2 Inpainting \cite{rombach2022high} for text-driven image modification. The inpainting model operates in the latent space of a variational autoencoder (VAE), enabling high-resolution generation with computational efficiency.

We employ several techniques to ensure coherent results:
\begin{itemize}
    \item \textbf{Mask dilation:} Expand mask boundaries by 5-10 pixels for smoother blending
    \item \textbf{Classifier-free guidance:} Use guidance scale of 7.5 to balance prompt following and realism
    \item \textbf{Prompt engineering:} Reformulate user queries into detailed prompts suitable for the diffusion model
\end{itemize}

This completes our CLIP-guided prompting pipeline, enabling fully annotation-free open-vocabulary segmentation with high-quality boundaries and massive efficiency gains through intelligent semantic guidance.

% ============================================================
% Section 2.7: Multi-scale Hierarchical Query-based Refinement
% ============================================================
\input{Capitulos/Capitulo2_MHQR_Section.tex}
