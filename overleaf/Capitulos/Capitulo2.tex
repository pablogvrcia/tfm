\chapter{Methodology}
\label{ch:methodology}

This chapter details the methodology employed to develop our open-vocabulary semantic segmentation system. Unlike traditional approaches that rely on closed-vocabulary classifiers, our work leverages recent advances in vision-language models and promptable segmentation to enable flexible, language-driven image understanding and manipulation.

Our primary contribution is a dense prediction approach that extends SCLIP's Cross-layer Self-Attention (CSA) mechanism with a novel SAM2-based mask refinement layer. This fully annotation-free method combines the semantic understanding of dense CLIP-based features with SAM2's superior boundary quality through intelligent prompted segmentation.

Additionally, we explore a proposal-based approach (SAM2+CLIP) as an alternative paradigm to understand the complementary strengths of different methodological strategies. This exploration provides valuable insights into when dense versus proposal-based methods are most effective.

Building upon insights from SCLIP \cite{sclip2024}, MaskCLIP \cite{zhou2022extract}, CLIPSeg \cite{luddecke2022clipseg}, and SAM 2 \cite{ravi2024sam2}, this chapter presents our primary dense methodology in detail, followed by a brief description of the proposal-based alternative and comparative analysis of both approaches.

\section{Primary Approach: Dense SCLIP with Novel SAM2 Refinement}
\label{sec:dense_sclip_approach}

Our main contribution is a dense prediction system that builds upon SCLIP's Cross-layer Self-Attention mechanism and extends it with a novel SAM2-based mask refinement layer. This approach achieves fully annotation-free open-vocabulary segmentation by combining dense semantic understanding with high-quality boundary delineation.

\subsection{Motivation and Design Philosophy}

Dense prediction methods offer several key advantages for open-vocabulary segmentation:

\begin{itemize}
    \item \textbf{Holistic scene understanding:} Dense approaches naturally segment both discrete objects (``things'') and amorphous regions (``stuff'' classes like sky, road, grass), providing comprehensive scene interpretation.

    \item \textbf{Annotation-free operation:} Unlike proposal-based methods that require generating and scoring hundreds of candidate masks, dense prediction produces a complete segmentation in a single forward pass, making it computationally efficient and conceptually simpler.

    \item \textbf{Semantic coherence:} Dense predictions maintain spatial consistency across the entire image, avoiding fragmentation artifacts that can occur when combining discrete mask proposals.
\end{itemize}

However, dense CLIP-based features alone often produce imprecise boundaries. Our key insight is to leverage SAM2's superior segmentation quality to refine SCLIP's dense predictions, combining the strengths of both models.

\subsection{System Overview}

Our dense SCLIP + SAM2 refinement pipeline consists of three main stages:

\begin{enumerate}
    \item \textbf{Dense Semantic Prediction:} SCLIP extracts multi-layer CSA features and produces pixel-wise class predictions through similarity matching with text embeddings.

    \item \textbf{Prompted SAM2 Refinement:} Extract representative points from SCLIP's predictions to guide SAM2's mask generation, then apply majority voting to combine semantic understanding with precise boundaries.

    \item \textbf{Generative Editing (Optional):} Integrate with Stable Diffusion v2 for text-driven image modification based on the refined segmentation masks.
\end{enumerate}

\begin{figure}[h]
\centering
\fbox{\parbox{0.9\textwidth}{\centering
\vspace{1cm}
\textbf{[PLACEHOLDER: Dense SCLIP + SAM2 Pipeline Diagram]}\\[0.5cm]
\textit{This figure should show the dense prediction pipeline:}\\[0.3cm]
\begin{tabular}{l}
\textbf{Stage 1 - Dense Prediction:}\\
\quad Input image → SCLIP (ViT-B/16 with CSA) → Dense class probabilities (H×W×C)\\
\quad Extract features from layers 6, 12, 18, 24 → Aggregate → Pixel-wise predictions\\[0.3cm]
\textbf{Stage 2 - SAM2 Refinement:}\\
\quad SCLIP predictions → Extract prompt points (connected components)\\
\quad Points → SAM2 predictor → High-quality masks\\
\quad Masks + SCLIP predictions → Majority voting → Refined segmentation\\[0.3cm]
\textbf{Stage 3 - Optional Editing:}\\
\quad Refined masks + Text prompt → Stable Diffusion → Edited image\\[0.3cm]
\end{tabular}
\textit{Include visualization: dense heatmap, extracted points, SAM2 masks, final result.}
\vspace{1cm}
}}
\caption{Overview of our dense SCLIP + SAM2 refinement pipeline. The system combines SCLIP's semantic understanding with SAM2's boundary quality through prompted segmentation.}
\label{fig:dense_pipeline}
\end{figure}

The key insight motivating our design is that dense prediction provides holistic scene understanding, while SAM2's prompted segmentation refines boundaries. By extracting points from SCLIP's high-confidence regions, we guide SAM2 to focus on semantically relevant areas, achieving both accuracy and efficiency.

\subsection{SCLIP's Cross-layer Self-Attention (CSA) Foundation}
\label{sec:csa_foundation}

Our dense prediction approach leverages SCLIP's Cross-layer Self-Attention (CSA), which modifies the standard self-attention mechanism in CLIP's Vision Transformer to produce better features for dense prediction. Standard self-attention computes:

\begin{equation}
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V
\end{equation}

where $Q$, $K$, $V$ are query, key, and value matrices. This formulation captures query-key relationships but may miss important spatial correlations. SCLIP's CSA instead computes:

\begin{equation}
\text{CSA}(Q, K, V) = \text{softmax}\left(\frac{QQ^T + KK^T}{\sqrt{d}}\right)V
\end{equation}

This modification introduces two key changes:

\begin{itemize}
    \item \textbf{Query-query similarity ($QQ^T$):} Captures relationships between different spatial positions based on their query representations, encouraging spatial consistency
    \item \textbf{Key-key similarity ($KK^T$):} Captures relationships based on key representations, providing complementary structural information
\end{itemize}

By leveraging CSA, we obtain attention maps that are more spatially coherent and better suited for dense prediction tasks.

\subsection{Multi-Layer Feature Aggregation}

Following SCLIP \cite{sclip2024}, MaskCLIP \cite{zhou2022extract}, and ITACLIP \cite{shao2024itaclip}, we extract features from multiple ViT layers to capture information at different semantic levels:

\begin{itemize}
    \item \textbf{Layer 6 (early):} Low-level features (edges, textures, colors)
    \item \textbf{Layer 12 (middle):} Mid-level features (object parts, patterns)
    \item \textbf{Layer 18 (late-middle):} High-level semantic features
    \item \textbf{Layer 24 (final):} Abstract semantic concepts
\end{itemize}

For each layer $\ell$, we extract patch features $F_\ell \in \mathbb{R}^{H_p \times W_p \times D}$ where $H_p, W_p$ are the patch grid dimensions and $D$ is the feature dimension. These features are upsampled to the original image resolution using bilinear interpolation and aggregated through weighted averaging.

\begin{figure}[h]
\centering
\fbox{\parbox{0.9\textwidth}{\centering
\vspace{1cm}
\textbf{[PLACEHOLDER: Multi-Scale CLIP Feature Visualization]}\\[0.5cm]
\textit{This figure should illustrate the multi-scale feature extraction process:}\\[0.3cm]
\begin{tabular}{l}
\textbf{Left:} Input image (e.g., a living room scene)\\
\textbf{Middle:} Four heatmaps showing CLIP features from layers 6, 12, 18, and 24\\
\quad - Layer 6: Low-level edges and textures\\
\quad - Layer 12: Mid-level patterns and shapes\\
\quad - Layer 18: Object parts and regions\\
\quad - Layer 24: High-level semantic concepts\\
\textbf{Right:} Combined multi-scale feature map with similarity scores for prompt "sofa"\\[0.3cm]
\end{tabular}
\textit{Use color-coded heatmaps (e.g., blue=low similarity, red=high similarity).}
\vspace{1cm}
}}
\caption{Multi-scale CLIP feature extraction from transformer layers. Different layers capture information at varying levels of abstraction, which are combined to produce spatially-resolved semantic features.}
\label{fig:multiscale_features}
\end{figure}

For text encoding, we use CLIP's text encoder to generate embeddings for user-provided prompts. To improve robustness, we employ prompt ensembling \cite{zhou2022learning}, generating multiple variations of the input text (e.g., ``a photo of a \{object\}'', ``\{object\} in the scene'') and averaging their embeddings.

\subsection{Dense Semantic Prediction}

Following SCLIP's approach, we compute pixel-wise semantic scores by matching image features with text embeddings. For each pixel location $(x, y)$ and class $c$, we compute the cosine similarity between the aggregated multi-layer feature $f_{x,y}$ and the text embedding $e_c$:

\begin{equation}
S(x, y, c) = \text{sim}(f_{x,y}, e_c) = \frac{f_{x,y} \cdot e_c}{||f_{x,y}|| \, ||e_c||}
\end{equation}

The initial dense prediction assigns each pixel to the class with highest similarity:

\begin{equation}
\hat{c}(x, y) = \arg\max_{c} S(x, y, c)
\end{equation}

This produces a dense semantic segmentation map purely from CLIP features, without requiring any mask proposals. However, while SCLIP's CSA improves spatial coherence, the boundaries may still lack precision compared to dedicated segmentation models.

\subsection{Novel SAM2 Refinement Layer}

Our key contribution is a novel refinement layer that combines SCLIP's semantic understanding with SAM2's superior boundary quality through prompted segmentation and majority voting.

\subsubsection{Prompted Segmentation Strategy}

Instead of using SAM2's automatic mask generation (which produces 100-300 masks from a 48×48 grid), we develop a guided prompting strategy that extracts representative points from SCLIP's predictions:

\begin{enumerate}
    \item \textbf{Connected Components Analysis:} For each predicted class, identify connected regions in SCLIP's dense prediction using morphological operations

    \item \textbf{Point Extraction:} Extract centroids from each connected component, ensuring minimum spatial separation (20 pixels) to avoid redundant prompts

    \item \textbf{SAM2 Prompting:} Pass extracted points to SAM2's predictor, which generates 3 masks per point at different granularities (tight, medium, loose)

    \item \textbf{Majority Voting:} For each SAM2 mask, compute overlap with SCLIP prediction. Keep masks where $\geq$60\% of pixels match the predicted class

    \item \textbf{Mask Combination:} Combine retained masks using logical OR to produce final refined segmentation
\end{enumerate}

This prompted approach achieves 2× speedup over automatic mask generation ($\sim$60 targeted points vs 2,304 grid points) while maintaining segmentation quality through semantic-guided prompting.

\subsubsection{Majority Voting for Semantic Consistency}

The majority voting mechanism ensures SAM2's masks align with SCLIP's semantic understanding. For each mask $M$ generated by SAM2, we compute:

\begin{equation}
\text{coverage}(M, c) = \frac{|\{p \in M : \hat{c}(p) = c\}|}{|M|}
\end{equation}

where $\hat{c}(p)$ is SCLIP's predicted class at pixel $p$. We retain mask $M$ for class $c$ if $\text{coverage}(M, c) \geq 0.6$, filtering out masks that don't align with SCLIP's semantic predictions.

This intelligent fusion combines:
\begin{itemize}
    \item \textbf{SCLIP's semantic accuracy:} Correct class assignment from vision-language understanding
    \item \textbf{SAM2's boundary quality:} Precise object delineation from specialized segmentation model
\end{itemize}

\subsection{Generative Editing Integration}

Once refined segmentation masks are obtained, we optionally integrate Stable Diffusion v2 Inpainting \cite{rombach2022high} for text-driven image modification. The inpainting model operates in the latent space of a variational autoencoder (VAE), enabling high-resolution generation with computational efficiency.

We employ several techniques to ensure coherent results:
\begin{itemize}
    \item \textbf{Mask dilation:} Expand mask boundaries by 5-10 pixels for smoother blending
    \item \textbf{Classifier-free guidance:} Use guidance scale of 7.5 to balance prompt following and realism
    \item \textbf{Prompt engineering:} Reformulate user queries into detailed prompts suitable for the diffusion model
\end{itemize}

This completes our primary dense SCLIP + SAM2 refinement pipeline, enabling fully annotation-free open-vocabulary segmentation with high-quality boundaries.

\section{Alternative Exploration: Proposal-Based Segmentation}
\label{sec:proposal_based}

As a complementary exploration, we also investigated a proposal-based paradigm that operates inversely to our primary dense approach. Instead of generating pixel-wise semantic predictions first and then refining with SAM2, this alternative method first generates class-agnostic mask proposals using SAM2's automatic mask generation, then scores each proposal using CLIP to determine its semantic class.

\subsection{Methodology Overview}

The proposal-based approach consists of three main stages:

\begin{enumerate}
    \item \textbf{Automatic Mask Generation:} SAM2 generates 100-300 class-agnostic mask proposals across multiple scales without requiring any prompts. The model uses a 32×32 grid of point prompts to ensure comprehensive coverage of all potential objects in the image.

    \item \textbf{CLIP-based Mask Scoring:} Each mask region is cropped, resized, and encoded using CLIP's image encoder. The resulting embedding is compared against text embeddings for all classes using cosine similarity to determine semantic relevance.

    \item \textbf{Multi-Scale Voting:} To improve robustness to object scale, each mask is evaluated at three resolutions (224px, 336px, 512px) and scored using weighted voting. This addresses CLIP's sensitivity to input resolution.
\end{enumerate}

\subsection{Key Technical Components}

\subsubsection{SAM2 Configuration}

We employ SAM2's automatic mask generation mode with parameters optimized for comprehensive coverage:

\begin{itemize}
    \item \textbf{Points per side:} 32 (generating a 32×32 grid)
    \item \textbf{Predicted IoU threshold:} 0.88
    \item \textbf{Stability score threshold:} 0.95
    \item \textbf{Model variant:} \texttt{sam2\_hiera\_large}
\end{itemize}

This configuration typically produces 100-300 mask candidates per image, ensuring both small details and large scene elements are captured.

\subsubsection{Multi-Scale CLIP Scoring}

For each mask $M_i$, we compute similarity scores at three scales:

\begin{equation}
S_i^{(s)} = \text{sim}(\text{CLIP}_{\text{img}}(M_i^{(s)}), \text{CLIP}_{\text{text}}(c))
\end{equation}

where $s \in \{224, 336, 512\}$ denotes the resize resolution. The final score combines all scales:

\begin{equation}
S_i = 0.2 \cdot S_i^{(224)} + 0.5 \cdot S_i^{(336)} + 0.3 \cdot S_i^{(512)}
\end{equation}

The weights reflect that 336px (CLIP's standard resolution) typically provides the most reliable features.

\subsubsection{Background Suppression}

To reduce false positives from uniform regions, we compute background scores using negative prompts (``background'', ``nothing'', ``empty space'') and subtract them from object scores:

\begin{equation}
S_i^{\text{final}} = S_i - \alpha \cdot S_{\text{bg}}
\end{equation}

with $\alpha = 0.3$ to balance object detection sensitivity.

\subsubsection{Multi-Instance Selection}

A key challenge is determining how many masks to select for each query. We employ an adaptive strategy:

\begin{enumerate}
    \item Filter masks by size (discard those <0.1\% of image area)
    \item Apply score threshold (0.15-0.20 depending on mask size)
    \item Use non-maximum suppression (70\% IoU threshold) to avoid duplicates
    \item Select multiple non-overlapping instances when appropriate
\end{enumerate}

This enables the system to correctly handle queries referring to single objects (``car''), multiple instances (``person'' in a crowd), or object parts (``tire'').

\subsection{Integration with Stable Diffusion}

The proposal-based approach integrates seamlessly with generative editing. Since masks are already class-specific and high-quality, they can be directly fed to Stable Diffusion v2 Inpainting for text-driven image modification:

\begin{itemize}
    \item \textbf{Inference steps:} 50
    \item \textbf{Guidance scale:} 7.5
    \item \textbf{Mask dilation:} 5 pixels for smooth blending
\end{itemize}

This enables interactive applications where users specify objects via text and modify them through natural language instructions.

\subsection{Complementary Strengths}

While our primary dense SCLIP + SAM2 approach excels at holistic scene understanding and ``stuff'' classes, the proposal-based method offers distinct advantages:

\begin{itemize}
    \item \textbf{Speed:} 2-4 seconds per image (6-7× faster than dense approach)
    \item \textbf{Discrete objects:} Superior performance on well-defined objects with clear boundaries
    \item \textbf{Precise boundaries:} SAM2's automatic masks provide exceptional edge quality
    \item \textbf{Generative editing:} Natural integration with inpainting for interactive applications
\end{itemize}

However, it faces challenges with:
\begin{itemize}
    \item \textbf{Stuff classes:} Amorphous regions (sky, grass, water) lack clear boundaries for proposal generation
    \item \textbf{Semantic fragmentation:} May produce inconsistent labels for continuous semantic regions
    \item \textbf{Computational overhead:} Generating hundreds of proposals is slower than single dense prediction pass
\end{itemize}

This complementary nature motivates our comparative analysis in the following section, where we examine when each paradigm is most effective.

\section{Comparative Analysis and Method Selection}

The two approaches represent complementary paradigms in open-vocabulary segmentation:

\subsection{Proposal-Based (SAM2+CLIP) Strengths}
\begin{itemize}
    \item \textbf{Speed:} 2-4s per image (6.75$\times$ faster than dense prediction)
    \item \textbf{Discrete objects:} Excels on PASCAL VOC (69.3\% mIoU) with well-defined objects
    \item \textbf{Generative integration:} Seamless connection to Stable Diffusion for image editing
    \item \textbf{Multi-instance handling:} Adaptive selection strategy for variable object counts
    \item \textbf{Precise boundaries:} SAM2's high-quality masks provide accurate object delineation
\end{itemize}

\subsection{Dense Prediction (SCLIP+SAM2) Strengths}
\begin{itemize}
    \item \textbf{Stuff classes:} Excels on COCO-Stuff (49.52\% mIoU), 83\% better than ITACLIP (27.0\%)
    \item \textbf{Semantic consistency:} CSA attention produces spatially coherent predictions
    \item \textbf{Fine-grained understanding:} Pixel-level classification captures subtle semantic variations
    \item \textbf{Training-free:} Purely CLIP-based, no dependence on external mask generators for core prediction
    \item \textbf{Dense semantic scenes:} Better for datasets with many overlapping semantic regions
\end{itemize}

\subsection{Method Selection Guidelines}

Based on our empirical evaluation, we recommend:

\begin{itemize}
    \item \textbf{Use Proposal-Based for:}
    \begin{itemize}
        \item Datasets dominated by discrete objects (VOC, Objects365)
        \item Applications requiring speed (<5s per image)
        \item Interactive image editing scenarios
        \item Multi-instance object detection and manipulation
    \end{itemize}

    \item \textbf{Use Dense Prediction for:}
    \begin{itemize}
        \item Datasets with many stuff classes (COCO-Stuff, ADE20K)
        \item Semantic scene understanding tasks
        \item Applications prioritizing fine-grained semantic consistency
        \item Scenarios where boundary precision is less critical than semantic coverage
    \end{itemize}
\end{itemize}

\subsection{Hybrid Potential}

Future work could explore hybrid approaches that combine both methodologies:
\begin{itemize}
    \item Use proposal-based for thing classes (discrete objects)
    \item Use dense prediction for stuff classes (amorphous regions)
    \item Ensemble predictions using confidence-weighted averaging
    \item Adaptive method selection based on query type (object vs. stuff)
\end{itemize}

In summary, this chapter has presented two complementary open-vocabulary segmentation methodologies, each with distinct strengths. The proposal-based approach achieves state-of-the-art results on discrete object datasets and enables interactive editing, while the dense prediction approach excels on stuff-heavy datasets with superior semantic consistency. Together, they demonstrate the versatility of CLIP-based segmentation across diverse application scenarios.
