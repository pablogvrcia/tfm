\chapter{Introduction}
\pagenumbering{arabic}

\section{Motivation}

Semantic segmentation has progressed significantly in recent years, enabling machines to assign semantic labels to individual pixels in visual scenes. However, traditional semantic segmentation models are constrained by a closed vocabulary, meaning they can only recognize objects explicitly present in their training data. This limitation reduces their applicability in real-world scenarios where novel objects are frequently encountered. For example, a self-driving car trained to recognize "car," "pedestrian," and "traffic light" might fail to identify a "scooter" or a "delivery robot," potentially leading to hazardous situations.

This limitation of closed-vocabulary models has motivated the development of open-vocabulary semantic segmentation. Open-vocabulary approaches bridge the gap between visual perception and human language by leveraging vision-language models such as CLIP (Contrastive Language-Image Pre-training) \cite{radford2021learning}, which learns to represent both text and images in a shared embedding space. These systems can interpret natural language descriptions and perform zero-shot segmentation—that is, segment objects and concepts not seen during training. For instance, a system could process the description "a person walking a dog" and segment both the person and the dog in a zero-shot manner, even without having encountered this specific combination during training.

Furthermore, integrating generative AI models such as Stable Diffusion \cite{rombach2022high} enables modification of images based on segmented objects. This capability supports applications in image editing, where users can describe desired changes ("make the sky blue" or "add a hat to the person"), and the system modifies the image accordingly. In content creation, artists and designers can generate scenes by combining segmented objects from different images or generating new objects based on textual descriptions. Potential applications span diverse domains, including human-computer interaction, augmented reality, and robotics. Figure \ref{fig:system_overview} illustrates the integration of these components—CLIP for vision-language understanding, SAM 2 for precise segmentation, and Stable Diffusion for realistic generation—into a unified system for language-driven image manipulation.

\begin{figure}[h]
\centering
\includegraphics[width=0.95\textwidth]{Imagenes/1. System Overview Concept.png}
\caption{Overview of the proposed open-vocabulary semantic segmentation and generative editing system. The system combines vision-language understanding (CLIP), precise segmentation (SAM 2: Segment Anything Model 2), and realistic generation (Stable Diffusion) to enable flexible, language-driven image manipulation.}
\label{fig:system_overview}
\end{figure}

\section{Problem Statement}

This thesis tackles the challenge of developing an open-vocabulary semantic segmentation system that seamlessly integrates with a generative AI model. The system aims to overcome the limitations of traditional closed-vocabulary methods by achieving the following objectives:

\begin{itemize}
\item \textbf{Segmenting unseen objects and concepts:} The system should accurately segment objects and concepts that were not explicitly present in the training data, enabling it to generalize to novel scenarios and handle a wider range of visual inputs. This objective is crucial for real-world applications where encountering unseen objects is inevitable. For instance, a robot navigating a cluttered environment should be able to segment and identify various objects, even if it has not been explicitly trained on them.
\item \textbf{Interpreting natural language descriptions:} The system should be able to understand and interpret natural language descriptions, allowing users to specify the objects or concepts they want to segment using human-readable language. This objective enhances the user-friendliness and flexibility of the system. Instead of relying on predefined categories or labels, users can express their segmentation intentions in natural language, making the system more intuitive and accessible.
\item \textbf{Realistically modifying images:} The system should seamlessly integrate with a generative AI model to modify images based on the segmented objects. This capability enables realistic inpainting \cite{yu2018generative}, object manipulation, and other creative applications. By combining the segmentation output with the generative power of AI models, the system can realistically fill in missing parts of an image, replace objects with different ones, or even generate entirely new objects based on textual descriptions.
\end{itemize}


\section{Contribution}

This thesis presents a novel approach to open-vocabulary semantic segmentation that achieves both computational efficiency and competitive accuracy on PASCAL VOC 2012. The contributions are organized into a primary algorithmic innovation and several supporting enhancements:

\vspace{0.3cm}
\noindent\textbf{Primary Contribution:}

\begin{itemize}
\item \textbf{CLIP-guided intelligent prompting for SAM2 segmentation:} The core contribution is a novel prompting strategy that extracts intelligent prompt points from SCLIP's \cite{sclip2024} high-confidence regions to guide SAM2's mask generation. This semantic-aware approach achieves a \textbf{massive reduction in computational cost}, using only a bunch of semantically-informed prompt points instead of thousands of uniformly distributed points, while reaching \textbf{competitive mIoU on PASCAL VOC 2012} and outperforming recent training-free open-vocabulary methods such as ITACLIP. The method assigns class labels directly from SCLIP's predictions at each prompt location, avoiding the complexity of majority voting or multi-scale evaluation. This annotation-free approach enables segmentation of both discrete objects and amorphous ``stuff'' classes (sky, road, vegetation) without requiring any labeled training data.
\item \textbf{Integration with Stable Diffusion for text-driven editing:} The segmentation system is integrated with Stable Diffusion v2 \cite{rombach2022high} to enable text-driven image editing, creating a complete pipeline from language-based semantic understanding to image manipulation. This integration supports practical applications including object removal, replacement, and style transfer.
\item \textbf{Extension to video segmentation and edition:} The CLIP-guided prompting approach extends to video processing through two capabilities: (1) video segmentation by leveraging SAM2's temporal tracking, with SCLIP-guided analysis performed only on the first frame and SAM2 propagating masks across all frames; (2) text-driven video inpainting using WAN 2.1 VACE \cite{vace2025}, enabling object removal and replacement in videos.
\end{itemize}

\vspace{0.3cm}
\noindent\textbf{Secondary Contributions:}

\begin{itemize}
\item \textbf{Descriptor files for multi-term class representations:} The system incorporates descriptor files that provide multiple textual variations for each class (e.g., ``person in shirt'', ``person in jeans'', ``person wearing glasses''), improving robustness to intra-class appearance variations and enhancing CLIP's ability to match diverse visual manifestations of semantic categories.

\item \textbf{Template strategy optimization:} CLIP requires text prompts to match visual features. Different strategies for generating these prompts—ranging are evaluated (e.g., ``a photo of a [CLASS]'', ``a rendering of a [CLASS]'', ``a close-up of a [CLASS]'') to identify the most effective templates for maximizing segmentation accuracy or inference time.

\item \textbf{Computational optimizations for practical deployment:} Implementation of FP16 mixed precision, \texttt{torch.compile} optimization, and batched prompting enables deployment on consumer hardware (6GB VRAM), making the approach practical for real-world applications.

\item \textbf{Comprehensive evaluation on PASCAL VOC 2012:} The system is rigorously evaluated on PASCAL VOC 2012 across 21 object categories, with detailed per-class analysis and multiple metrics (mIoU, pixel accuracy, F1 score, precision, recall, boundary F1).

\item \textbf{Comparative analysis demonstrating efficiency-accuracy trade-off:} Experimental validation compares the SCLIP-guided prompting approach against pure dense prediction methods (SCLIP, MaskCLIP, ITACLIP), demonstrating that semantic guidance provides quality improvements through SAM2's superior boundary delineation on the PASCAL VOC 2012 benchmark.
\end{itemize}

\section{Thesis Structure}

The remainder of this thesis is structured as follows:

\begin{itemize}
\item \textbf{Chapter 2 (Background and Related Work):} Reviews the foundational concepts and prior work in semantic segmentation, vision-language models (CLIP), mask generation models (SAM and SAM 2), and generative AI for inpainting (Stable Diffusion).

\item \textbf{Chapter 3 (Methodology):} Describes the proposed system architecture, including the CLIP-guided intelligent prompting strategy, descriptor files, template optimization, computational optimizations, and extensions to generative editing and video processing.

\item \textbf{Chapter 4 (Experiments and Evaluation):} Presents the experimental setup, evaluation metrics, and results on PASCAL VOC 2012, with comparative analysis against existing open-vocabulary segmentation methods.

\item \textbf{Chapter 5 (Conclusions and Future Work):} Summarizes the key contributions, discusses limitations, and outlines potential future research directions.

\item \textbf{Appendices:} Provide detailed technical explanations of CLIP, SAM2, and Stable Diffusion architectures (Appendix A), and implementation details (Appendix B).
\end{itemize}