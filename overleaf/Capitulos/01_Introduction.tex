\chapter{Introduction}
\pagenumbering{arabic}

\section{Motivation}

Semantic segmentation has progressed significantly in recent years, enabling machines to assign semantic labels to individual pixels in visual scenes. However, traditional semantic segmentation models are constrained by a closed vocabulary, meaning they can only recognize objects explicitly present in their training data. This limitation reduces their applicability in real-world scenarios where novel objects are frequently encountered. For example, a self-driving car trained to recognize "car," "pedestrian," and "traffic light" might fail to identify a "scooter" or a "delivery robot," potentially leading to hazardous situations.

This limitation of closed-vocabulary models has motivated the development of open-vocabulary semantic segmentation. Open-vocabulary approaches bridge the gap between visual perception and human language by leveraging vision-language models such as CLIP (Contrastive Language-Image Pre-training) \cite{radford2021learning}, which learns to represent both text and images in a shared embedding space. These systems can interpret natural language descriptions and perform zero-shot segmentation—that is, segment objects and concepts not seen during training. For instance, a system could process the description "a person walking a dog" and segment both the person and the dog in a zero-shot manner, even without having encountered this specific combination during training.

Furthermore, integrating generative AI models such as Stable Diffusion \cite{rombach2022high} enables modification of images based on segmented objects. This capability supports applications in image editing, where users can describe desired changes ("make the sky blue" or "add a hat to the person"), and the system modifies the image accordingly. In content creation, artists and designers can generate scenes by combining segmented objects from different images or generating new objects based on textual descriptions. Potential applications span diverse domains, including human-computer interaction, augmented reality, and robotics. Figure \ref{fig:system_overview} illustrates the integration of these components—CLIP for vision-language understanding.

\begin{figure}[h]
\centering
\includegraphics[width=0.95\textwidth]{Imagenes/1. System Overview Concept.png}
\caption{Overview of the proposed open-vocabulary semantic segmentation and generative editing system. The system combines vision-language understanding, precise segmentation, and realistic generation.}
\label{fig:system_overview}
\end{figure}

\section{Problem Statement}

This thesis tackles the challenge of developing an open-vocabulary semantic segmentation system that seamlessly integrates with a generative AI model. The system aims to overcome the limitations of traditional closed-vocabulary methods by achieving the following objectives:

\begin{itemize}
\item \textbf{Segmenting unseen objects and concepts:} The system should accurately segment objects and concepts that were not explicitly present in the training data, enabling it to generalize to novel scenarios and handle a wider range of visual inputs. This objective is crucial for real-world applications where encountering unseen objects is inevitable. For instance, a robot navigating a cluttered environment should be able to segment and identify various objects, even if it has not been explicitly trained on them.
\item \textbf{Interpreting natural language descriptions:} The system should be able to understand and interpret natural language descriptions, allowing users to specify the objects or concepts they want to segment using human-readable language. This objective enhances the user-friendliness and flexibility of the system. Instead of relying on predefined categories or labels, users can express their segmentation intentions in natural language, making the system more intuitive and accessible.
\item \textbf{Realistically modifying images:} The system should seamlessly integrate with a generative AI model to modify images based on the segmented objects. This capability enables realistic inpainting \cite{yu2018generative}, object manipulation, and other creative applications. By combining the segmentation output with the generative power of AI models, the system can realistically fill in missing parts of an image, replace objects with different ones, or even generate entirely new objects based on textual descriptions.
\end{itemize}


\section{Contribution}

The core contribution is an intelligent prompting strategy that extracts prompt points from SCLIP's \cite{sclip2024} high-confidence regions to guide SAM2's mask generation. This reduces computational cost by using semantically-informed prompts instead of exhaustive grid sampling, while achieving competitive results on PASCAL VOC 2012. Class labels are assigned directly from SCLIP's predictions, enabling segmentation without labeled training data.

The approach extends to generative editing by integrating Stable Diffusion v2 \cite{rombach2022high} for text-driven image manipulation and WAN 2.1 VACE \cite{vace2025} for video inpainting. For video segmentation, SAM2's temporal tracking propagates masks across frames after SCLIP analyzes only the first frame.

Additional enhancements include descriptor files for handling intra-class appearance variations and template optimization for CLIP feature matching. Evaluation on PASCAL VOC 2012 demonstrates that semantic guidance provides quality improvements through SAM2's boundary delineation compared to pure dense prediction methods.

\section{Thesis Structure}

The remainder of this thesis is structured as follows:

\begin{itemize}
\item \textbf{Chapter 2 (Background and Related Work):} Reviews the foundational concepts and prior work in semantic segmentation, vision-language models (CLIP), mask generation models (SAM and SAM 2), and generative AI for inpainting (Stable Diffusion).

\item \textbf{Chapter 3 (Methodology):} Describes the proposed system architecture, including the CLIP-guided intelligent prompting strategy, descriptor files, template optimization, computational optimizations, and extensions to generative editing and video processing.

\item \textbf{Chapter 4 (Experiments and Evaluation):} Presents the experimental setup, evaluation metrics, and results on PASCAL VOC 2012, with comparative analysis against existing open-vocabulary segmentation methods.

\item \textbf{Chapter 5 (Conclusions and Future Work):} Summarizes the key contributions, discusses limitations, and outlines potential future research directions.

\item \textbf{Appendices:} Provide detailed technical explanations of CLIP, SAM2, and Stable Diffusion architectures (Appendix A), and implementation details (Appendix B).
\end{itemize}