\chapter{Background and Related Work} 
\label{ch:no_lineal}

This chapter provides a comprehensive overview of the foundational concepts and related work relevant to this thesis. It delves into the core areas of semantic segmentation, language models for vision, open-vocabulary semantic segmentation, mask generation models, and generative AI models for inpainting. In addition, it discusses recent advances in prompt learning, captioning approaches leveraging vision-language models, and large-scale open-vocabulary segmentation frameworks, ensuring a broad context for the subsequent chapters.

\section{Semantic Segmentation}
Semantic segmentation is a fundamental problem in computer vision that involves assigning a semantic label to each pixel in an image. Unlike image-level classification or object detection, semantic segmentation aims to produce a dense, pixel-wise prediction, thereby providing a fine-grained understanding of the scene. Early attempts at semantic segmentation often relied on handcrafted features and probabilistic models \cite{shotton2009textonboost}, but these approaches struggled with complex scenes and diverse object appearances.

The introduction of deep learning techniques revolutionized the field. Fully Convolutional Networks (FCNs) \cite{long2015fully} repurposed classification backbones \cite{krizhevsky2012imagenet,simonyan2014very,he2016deep} to produce dense predictions. Encoder-decoder architectures like U-Net \cite{ronneberger2015u} and SegNet \cite{badrinarayanan2017segnet} recovered high-resolution details through skip connections and stored pooling indices, respectively. Subsequent models focused on capturing richer context and multi-scale representations. PSPNet \cite{zhao2017pyramid} aggregated global context using pyramid pooling, while DeepLab \cite{chen2018encoder} employed atrous convolutions for flexible receptive fields. HRNet \cite{sun2019deep} maintained high-resolution features throughout the network to preserve spatial details.

Standard datasets such as PASCAL VOC \cite{everingham2010pascal} and MS COCO \cite{lin2014microsoft} drove progress and benchmarked performance. Yet, these methods typically operated under a closed-set assumption, relying on predefined categories. As real-world applications demand models that recognize novel objects, the limitations of closed vocabularies became evident, motivating the shift toward open-vocabulary semantic segmentation.

\section{Language Models for Vision}
Integrating language understanding into vision models extends their applicability and flexibility. Early efforts, such as DeViSE \cite{frome2013devise}, aligned visual features with semantic word embeddings to enable zero-shot image classification. This idea evolved into more complex systems for image captioning \cite{karpathy2015deep}, which learned to describe images with natural language sentences.

A significant breakthrough came with large-scale vision-language pretraining. CLIP \cite{radford2021learning} learned powerful joint embeddings for images and text from massive unlabeled web data, enabling zero-shot classification and a flexible semantic interface. ALIGN \cite{jia2021scaling} further scaled this approach, while BLIP \cite{li2022blip} unified vision-language understanding and generation under a single pretraining framework. Flamingo \cite{alayrac2022flamingo} explored few-shot adaptation scenarios by integrating large language models with visual backbones.

Parallel to these efforts, research into prompt learning refined how language models interface with vision tasks. Zhou et al. \cite{zhou2022learning} introduced methods to learn optimal prompts for vision-language models, improving their adaptability to downstream tasks. Mokady et al. \cite{mokady2021clipcap} leveraged CLIP features to guide image captioning (CLIPCap), showcasing how text prefixes conditioned on CLIP embeddings could steer generation toward semantically aligned outputs.

These vision-language innovations laid the groundwork for open-vocabulary segmentation, allowing models to understand and respond to arbitrary, user-defined textual queries.

\section{Open-Vocabulary Semantic Segmentation}
Open-vocabulary semantic segmentation aims to move beyond fixed taxonomies, enabling the segmentation of arbitrary concepts specified by language prompts. Initial works like zero-shot semantic segmentation \cite{bucher2019zero} connected pixels to semantic embeddings from large language models, but often lacked the rich representation power of contemporary vision-language systems.

With the advent of CLIP and related models, robust open-vocabulary segmentation became possible. LSeg \cite{li2022language} adapted CLIP embeddings to segmentation, allowing queries for arbitrary textual categories. GroupViT \cite{xu2022groupvit} demonstrated that semantic segmentation capabilities can emerge from text supervision alone, while OpenSeg \cite{ghiasi2022scaling} leveraged large-scale image-level labels to scale open-vocabulary segmentation.

Building on CLIP's success, several methods focused on extracting dense predictions from vision-language models. CLIPSeg \cite{luddecke2022clipseg} enabled segmentation using both text and image prompts by extending CLIP with a transformer-based decoder. MaskCLIP \cite{zhou2022extract} showed that dense labels can be extracted from CLIP without additional training, achieving compelling zero-shot segmentation results. Its extension, MasQCLIP \cite{xu2023masqclip}, further improved performance on universal image segmentation tasks. ZegCLIP \cite{zhang2022zegclip} and OVSeg \cite{liang2023ovseg} explored different strategies for adapting frozen vision-language models to segmentation.

Further refinements have expanded the toolkit for open-vocabulary segmentation. Ghiasi et al. \cite{ghiasi2022open} introduced mask-adapted CLIP models, improving performance in challenging open-vocabulary scenarios by integrating segmentation masks into the vision-language pipeline. X-Decoder \cite{zou2023xdecoder} unified pixel-level and token-level decoding, achieving state-of-the-art results across multiple segmentation tasks. ODISE \cite{xu2023odise} innovatively combined text-to-image diffusion models with discriminative models for open-vocabulary panoptic segmentation. More recently, CAT-Seg \cite{cho2024catseg} introduced cost aggregation techniques to improve segmentation quality, while LMSeg \cite{LMSeg2024} exemplifies state-of-the-art approaches that harness large-scale models and advanced techniques to further enhance open-vocabulary segmentation quality and generalization.

\section{Mask Generation Models}
Mask generation models produce accurate object or region delineations and serve as a backbone for many segmentation systems. Mask R-CNN \cite{he2017mask} extended object detection frameworks to instance segmentation, while Mask2Former \cite{cheng2022mask2former} unified semantic, instance, and panoptic segmentation using a transformer-based design.

The Segment Anything Model (SAM) \cite{kirillov2023segment} marked a significant shift toward prompt-driven segmentation. Trained on a vast and diverse dataset (SA-1B with over 1 billion masks), SAM can segment virtually any object when provided with a suitable prompt—points, boxes, or text—making it particularly versatile for zero-shot generalization. Building upon this foundation, SAM 2 \cite{ravi2024sam2} extended these capabilities to video segmentation, introducing a memory mechanism that enables consistent object tracking across frames. SAM 2 achieves superior accuracy while requiring fewer interactions and operates at real-time speeds (approximately 44 frames per second), making it highly practical for both image and video applications.

By integrating SAM or SAM 2 with open-vocabulary embeddings from CLIP or related models, one can achieve promptable, zero-shot segmentation of arbitrary categories. This synergy of mask generation with vision-language models unlocks flexible and dynamic segmentation capabilities essential for downstream applications.

\section{Generative AI Models for Inpainting}
Generative inpainting models fill masked image regions with plausible, contextually coherent content. Before deep learning, patch-based methods \cite{criminisi2004region} searched for suitable patches to fill holes, but lacked semantic understanding. Context Encoders \cite{pathak2016context} introduced a learning-based approach, using convolutional neural networks and adversarial training to predict missing regions. Subsequent improvements like Partial Convolutions \cite{liu2018image}, Gated Convolutions \cite{yu2019free}, and attention-based models \cite{yu2018generative} enhanced robustness and image fidelity.

The latest generation of inpainting models leverages powerful generative architectures and large-scale training. Stable Diffusion \cite{rombach2022high} employs latent diffusion models to produce high-resolution, semantically consistent completions guided by textual prompts. DALL·E 2 \cite{ramesh2022hierarchical} similarly enables text-driven modifications, allowing users to describe desired changes in natural language. Integrating such generative models with open-vocabulary segmentation and promptable mask generation (e.g., SAM) enables unprecedented levels of interactivity: users can identify segments of interest and instruct the model to add, remove, or alter objects via textual commands.

This combination of open-vocabulary segmentation and generative inpainting lays the foundation for next-generation image editing tools, capable of fluidly responding to a broad range of user-defined concepts and transformations.

\section{Video Inpainting Models}
Video inpainting extends the challenge of image inpainting to the temporal domain, requiring not only semantically plausible content generation but also temporal consistency across frames. Unlike single-image inpainting, video inpainting must maintain coherent object motion, preserve temporal smoothness, and avoid flickering artifacts. This section reviews the evolution of video inpainting methods from classical flow-based approaches to modern diffusion-based models, with particular emphasis on recent developments in 2023-2025.

\subsection{Flow-Based Video Inpainting (2020-2022)}
Early deep learning approaches to video inpainting leveraged optical flow to propagate information from neighboring frames. STTN \cite{zeng2020learning} introduced spatio-temporal transformers that utilize attention mechanisms for implicit relation reasoning among embedded features, facilitating context propagation across frames. However, STTN operated independently on each frame without explicit motion modeling.

FGVC (Flow-Guided Video Completion) \cite{xu2019deep} deployed flow completion networks and propagation algorithms separately, pioneering the use of optical flow for guided inpainting. However, its per-pixel forward flow tracing lacked sub-pixel accuracy, leading to spatial misalignment and blurry results.

E2FGVI (End-to-End Flow-Guided Video Inpainting) \cite{li2022towards} addressed these limitations by proposing an end-to-end trainable framework that jointly optimizes flow completion and content hallucination. Presented at CVPR 2022, E2FGVI achieved state-of-the-art performance with processing speeds of 0.12 seconds per frame on 432×240 videos, nearly 15× faster than previous flow-based methods.

FGT (Flow-Guided Transformer) \cite{zhang2022fgt}, introduced at ECCV 2022, innovatively leverages motion discrepancy exposed by optical flows to instruct attention retrieval in transformers. FGT++ \cite{zhang2023fgt++}, released in 2023 as the journal extension, achieved significantly improved results by combining flow guidance with transformer-based generation.

\subsection{Hybrid Propagation and Transformer Methods (2023)}
ProPainter \cite{zhou2023propainter}, presented at ICCV 2023, marked a significant advancement by introducing dual-domain propagation that combines the advantages of image warping and feature propagation, exploiting global correspondences reliably. The method proposes a mask-guided sparse video Transformer that achieves high efficiency by discarding unnecessary and redundant tokens. ProPainter outperforms prior methods by a large margin of 1.46 dB in PSNR while maintaining appealing efficiency. This work represents the state-of-the-art in traditional (non-diffusion) video inpainting approaches.

For specialized scenarios, Deep Stereo Video Inpainting \cite{wu2023deep} (CVPR 2023) introduced SVINet, the first approach for stereo video inpainting utilizing deep convolutional neural networks. The key challenge addressed was maintaining stereo consistency between left and right views while performing temporal inpainting.

\subsection{Diffusion-Based Video Inpainting (2025)}
The emergence of diffusion models has revolutionized video inpainting in 2025, bringing generative capabilities previously limited to images into the video domain. These methods leverage the powerful priors learned by large-scale diffusion models to generate highly realistic and temporally coherent video content.

\subsubsection{DiffuEraser}
DiffuEraser \cite{diffueraser2025}, introduced in January 2025, adapts stable diffusion architectures to video inpainting by incorporating prior information to provide initialization and weak conditioning, which helps mitigate noisy artifacts and suppress hallucinations. Experimental results demonstrate that this method outperforms state-of-the-art techniques in both content completeness and temporal consistency while maintaining acceptable efficiency.

\subsubsection{VipDiff}
VipDiff \cite{vipdiff2025}, also released in January 2025, proposes a training-free framework for conditioning pre-trained image-level diffusion models to generate spatially-temporally coherent and diverse video inpainting results. This approach is particularly valuable as it leverages existing image diffusion models without requiring expensive video-specific training.

\subsubsection{DiTPainter}
DiTPainter \cite{ditpainter2025}, presented in April-May 2025, introduces an efficient diffusion transformer network designed specifically for video inpainting. Unlike previous methods that initialize from large pretrained models, DiTPainter is trained from scratch with a focus on video-specific architectures. The method can address videos with arbitrary lengths and can be applied to video decaptioning and video completion tasks with acceptable time costs. Experiments demonstrate that DiTPainter outperforms existing video inpainting algorithms with higher quality and better spatial-temporal consistency.

\subsubsection{EraserDiT}
EraserDiT \cite{eraserdit2025}, released in June 2025, synergistically combines the advantages of diffusion models and transformer architectures to maintain long-term temporal consistency while ensuring high-quality inpainting results. The integration of Diffusion Transformers (DiT) represents the cutting edge of video generation technology, addressing the blurring and temporal inconsistencies that plague earlier methods when dealing with large masks.

\subsection{All-in-One Video Creation and Editing: VACE}
VACE (Video All-in-One Creation and Editing) \cite{vace2025}, presented at ICCV 2025, represents a paradigm shift toward unified video generation frameworks. Developed by Alibaba's Tongyi Lab, VACE integrates multiple video tasks—including reference-to-video generation (R2V), video-to-video editing (V2V), and masked video-to-video editing (MV2V)—into a single all-in-one framework.

\subsubsection{Technical Architecture}
VACE introduces the Video Condition Unit (VCU), a unified interface that organizes diverse video task inputs (editing, reference, masking) into a consistent representation. Through a Context Adapter structure, the system injects different task concepts into the model using formalized representations of temporal and spatial dimensions, enabling flexible handling of arbitrary video synthesis tasks.

\subsubsection{WAN 2.1 VACE Implementation}
The WAN 2.1 VACE model is available in two variants: VACE-1.3B and VACE-14B, with the latter containing 14 billion parameters for fine granularity in video detail synthesis. The model supports resolutions from 480p to 720p and can process videos of varying lengths (typically up to 81 frames in the 4n+1 format required by the architecture).

Key capabilities include:
\begin{itemize}
    \item \textbf{Masked Video Inpainting (MV2V):} Intelligently fills missing regions in videos, removes unwanted objects, and achieves high-quality video content reconstruction with temporal consistency
    \item \textbf{Reference-Guided Generation (R2V):} Generates video sequences conditioned on reference images
    \item \textbf{Style Transfer (V2V):} Applies style transformations to existing videos while preserving motion
    \item \textbf{Text-Guided Editing:} Modifies video content based on natural language prompts
\end{itemize}

\subsubsection{Performance and Availability}
Extensive experiments demonstrate that VACE achieves performance on par with task-specific models across various subtasks, while offering the significant advantage of a unified framework. The model was released in May 2025 and is publicly available on HuggingFace and ModelScope, enabling researchers and practitioners to leverage state-of-the-art video inpainting capabilities.

\subsection{Summary and Trends}
The evolution of video inpainting from 2020 to 2025 reveals several key trends:
\begin{itemize}
    \item \textbf{From flow-based to diffusion-based:} Early methods relied heavily on optical flow for temporal consistency, while recent approaches leverage diffusion models' generative capabilities
    \item \textbf{Efficiency improvements:} Processing times have decreased from seconds per frame (E2FGVI: 0.12s) to near real-time with optimized transformer architectures
    \item \textbf{Unified frameworks:} The shift from task-specific models to all-in-one frameworks (VACE) enables more flexible and practical applications
    \item \textbf{Quality gains:} Diffusion-based methods produce significantly higher-quality results with better temporal consistency and fewer artifacts compared to earlier approaches
\end{itemize}

These advancements enable practical video editing applications, including object removal, content restoration, and creative manipulation guided by natural language prompts. The integration of video inpainting with open-vocabulary segmentation (detailed in Chapter 2) provides a complete pipeline for text-driven video editing.
