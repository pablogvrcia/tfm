\chapter{Background and Related Work}
\label{ch:no_lineal}

This chapter reviews prior work in open-vocabulary semantic segmentation, focusing on CLIP-based dense prediction methods and the specific techniques we integrate in our framework. Detailed technical background on Vision Transformers and self-attention mechanisms is provided in Annex A.

\section{Semantic Segmentation}

Traditional semantic segmentation assigns class labels to each pixel in an image. Deep learning approaches evolved from Fully Convolutional Networks (FCNs) \cite{long2015fully} through encoder-decoder architectures (U-Net \cite{ronneberger2015u}, SegNet \cite{badrinarayanan2017segnet}) to sophisticated multi-scale methods (PSPNet \cite{zhao2017pyramid}, DeepLab \cite{chen2018encoder}). However, these closed-vocabulary methods are limited to predefined categories from datasets like PASCAL VOC \cite{everingham2010pascal} and MS COCO \cite{lin2014microsoft}, failing to generalize to novel objects.

\section{Vision-Language Models}

CLIP \cite{radford2021learning} revolutionized zero-shot classification by learning joint image-text embeddings from web-scale data. CLIP's Vision Transformer encodes images into a shared embedding space with text, enabling semantic matching without category-specific training. Subsequent models (ALIGN \cite{jia2021scaling}, BLIP \cite{li2022blip}) refined this approach, establishing vision-language pretraining as the foundation for open-vocabulary tasks.

\section{Open-Vocabulary Semantic Segmentation}

Adapting CLIP for dense prediction has been explored through various approaches. MaskCLIP \cite{zhou2022extract} extracts dense labels from frozen CLIP features via text-image similarity matching, achieving zero-shot segmentation. CLIPSeg \cite{luddecke2022clipseg} adds a transformer decoder to CLIP for improved spatial resolution. LSeg \cite{li2022language} and GroupViT \cite{xu2022groupvit} train specialized architectures for language-guided segmentation.

\textbf{SCLIP} \cite{sclip2024} introduces Cross-layer Self-Attention (CSA), replacing the final layer's standard $QK^T$ attention with $QQ^T + KK^T$ to enhance spatial coherence. This simple modification improves dense prediction quality by encouraging patches with similar semantic content to mutually reinforce through both query and key similarities. SCLIP serves as our baseline framework (22.77\% mIoU on COCO-Stuff-164k).

\section{Enhancement Techniques Integrated in This Thesis}

This section reviews the specific methods we systematically integrate into our SCLIP-based framework.

\subsection{Phase 1: Spatial Enhancement}

\textbf{LoftUp} (ICCV 2025): Upsamples CLIP's coarse 14×14 feature grids to 28×28 through learned interpolation, preserving semantic information while doubling spatial resolution. Expected improvement: +2-4\% mIoU.

\textbf{ResCLIP} (CVPR 2025): Introduces Residual Cross-correlation Self-Attention (RCS) and Semantic Feedback Refinement (SFR). RCS enhances spatial coherence by measuring patch-to-patch similarity, while SFR performs multi-scale coarse-to-fine refinement. Expected improvement: +8-13\% mIoU.

\textbf{DenseCRF}: Classical Conditional Random Field post-processing enforces appearance consistency and boundary smoothness through pairwise pixel potentials. Expected improvement: +1-2\% mIoU, +3-5\% boundary F1.

\subsection{Phase 2A: Human Parsing Enhancement}

\textbf{CLIPtrase} (ECCV 2024): Recalibrates CLIP's self-attention through correlation matrix enhancement, improving local feature awareness crucial for articulated human poses. Expected improvement: +5-10\% mIoU for person class.

\textbf{CLIP-RC} (CVPR 2024): Extracts and preserves regional/local clues to combat CLIP's global feature dominance, essential for capturing body part details. Expected improvement: +8-12\% mIoU for person class.

\subsection{Phase 2B: Prompt Engineering}

\textbf{PixelCLIP templates}: Curated dense prediction prompts optimized for segmentation rather than generic classification. Top-7 strategy achieves 11.4× speedup with +2-3\% mIoU improvement over 80-template ensembles.

\textbf{Adaptive templates}: Class-type aware prompts distinguish between "stuff" (amorphous regions like sky, road) and "things" (discrete objects). Material-aware templates handle compound classes (wall-brick, floor-marble).
