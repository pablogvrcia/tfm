\chapter{Background and Related Work} 
\label{ch:no_lineal}

This chapter provides a comprehensive overview of the foundational concepts and related work relevant to this thesis. It delves into the core areas of semantic segmentation, language models for vision, open-vocabulary semantic segmentation, mask generation models, and generative AI models for inpainting. In addition, it discusses recent advances in prompt learning, captioning approaches leveraging vision-language models, and large-scale open-vocabulary segmentation frameworks, ensuring a broad context for the subsequent chapters.

\section{Semantic Segmentation}
Semantic segmentation is a fundamental problem in computer vision that involves assigning a semantic label to each pixel in an image. Unlike image-level classification or object detection, semantic segmentation aims to produce a dense, pixel-wise prediction, thereby providing a fine-grained understanding of the scene. Early attempts at semantic segmentation often relied on handcrafted features and probabilistic models \cite{shotton2009textonboost}, but these approaches struggled with complex scenes and diverse object appearances.

The introduction of deep learning techniques revolutionized the field. Fully Convolutional Networks (FCNs) \cite{long2015fully} repurposed classification backbones \cite{krizhevsky2012imagenet,simonyan2014very,he2016deep} to produce dense predictions. Encoder-decoder architectures like U-Net \cite{ronneberger2015u} and SegNet \cite{badrinarayanan2017segnet} recovered high-resolution details through skip connections and stored pooling indices, respectively. Subsequent models focused on capturing richer context and multi-scale representations. PSPNet \cite{zhao2017pyramid} aggregated global context using pyramid pooling, while DeepLab \cite{chen2018encoder} employed atrous convolutions for flexible receptive fields. HRNet \cite{sun2019deep} maintained high-resolution features throughout the network to preserve spatial details.

Standard datasets such as PASCAL VOC \cite{everingham2010pascal} and MS COCO \cite{lin2014microsoft} drove progress and benchmarked performance. Yet, these methods typically operated under a closed-set assumption, relying on predefined categories. As real-world applications demand models that recognize novel objects, the limitations of closed vocabularies became evident, motivating the shift toward open-vocabulary semantic segmentation.

\section{Language Models for Vision}
Integrating language understanding into vision models extends their applicability and flexibility. Early efforts, such as DeViSE \cite{frome2013devise}, aligned visual features with semantic word embeddings to enable zero-shot image classification. This idea evolved into more complex systems for image captioning \cite{karpathy2015deep}, which learned to describe images with natural language sentences.

A significant breakthrough came with large-scale vision-language pretraining. CLIP \cite{radford2021learning} learned powerful joint embeddings for images and text from massive unlabeled web data, enabling zero-shot classification and a flexible semantic interface. ALIGN \cite{jia2021scaling} further scaled this approach, while BLIP \cite{li2022blip} unified vision-language understanding and generation under a single pretraining framework. Flamingo \cite{alayrac2022flamingo} explored few-shot adaptation scenarios by integrating large language models with visual backbones.

Parallel to these efforts, research into prompt learning refined how language models interface with vision tasks. Zhou et al. \cite{zhou2022learning} introduced methods to learn optimal prompts for vision-language models, improving their adaptability to downstream tasks. Mokady et al. \cite{mokady2021clipcap} leveraged CLIP features to guide image captioning (CLIPCap), showcasing how text prefixes conditioned on CLIP embeddings could steer generation toward semantically aligned outputs.

These vision-language innovations laid the groundwork for open-vocabulary segmentation, allowing models to understand and respond to arbitrary, user-defined textual queries.

\section{Open-Vocabulary Semantic Segmentation}
Open-vocabulary semantic segmentation aims to move beyond fixed taxonomies, enabling the segmentation of arbitrary concepts specified by language prompts. Initial works like zero-shot semantic segmentation \cite{bucher2019zero} connected pixels to semantic embeddings from large language models, but often lacked the rich representation power of contemporary vision-language systems.

\begin{figure}[h]
\centering
\fbox{\parbox{0.9\textwidth}{\centering
\vspace{1cm}
\textbf{[PLACEHOLDER: Evolution of Segmentation Approaches]}\\[0.5cm]
\textit{This figure should show timeline/progression of segmentation paradigms:}\\[0.3cm]
\begin{tabular}{l}
\textbf{Three columns showing the evolution:}\\[0.2cm]
\textbf{Column 1 - Closed-Vocabulary (2015-2020):}\\
\quad Example: FCN, DeepLabV3+, PSPNet\\
\quad Diagram: Fixed set of classes (20-150 categories)\\
\quad Input: Image → Output: Segmentation with predefined labels\\
\quad \textit{Limitation: Cannot segment unseen classes}\\[0.2cm]
\textbf{Column 2 - Early Open-Vocabulary (2020-2022):}\\
\quad Example: LSeg, GroupViT, CLIPSeg\\
\quad Diagram: Image + Text prompt → Vision-Language Model → Segmentation\\
\quad \textit{Advantage: Zero-shot capability, but lower accuracy}\\[0.2cm]
\textbf{Column 3 - Modern Hybrid (2023-2024):}\\
\quad Example: X-Decoder, ODISE, CAT-Seg, Ours\\
\quad Diagram: Image + Text → Multiple Foundation Models → High-quality segmentation\\
\quad \textit{Combines: Mask quality + Language flexibility}\\[0.3cm]
\end{tabular}
\textit{Use example images showing segmentation results for each paradigm.}\\
\textit{Include arrows showing progression and key innovations at each stage.}
\vspace{1cm}
}}
\caption{Evolution of semantic segmentation approaches from closed-vocabulary to open-vocabulary paradigms. Modern methods combine the accuracy of specialized models with the flexibility of vision-language understanding.}
\label{fig:segmentation_evolution}
\end{figure}

With the advent of CLIP and related models, robust open-vocabulary segmentation became possible. LSeg \cite{li2022language} adapted CLIP embeddings to segmentation, allowing queries for arbitrary textual categories. GroupViT \cite{xu2022groupvit} demonstrated that semantic segmentation capabilities can emerge from text supervision alone, while OpenSeg \cite{ghiasi2022scaling} leveraged large-scale image-level labels to scale open-vocabulary segmentation.

Building on CLIP's success, several methods focused on extracting dense predictions from vision-language models. CLIPSeg \cite{luddecke2022clipseg} enabled segmentation using both text and image prompts by extending CLIP with a transformer-based decoder. MaskCLIP \cite{zhou2022extract} showed that dense labels can be extracted from CLIP without additional training, achieving compelling zero-shot segmentation results. Its extension, MasQCLIP \cite{xu2023masqclip}, further improved performance on universal image segmentation tasks. ZegCLIP \cite{zhang2022zegclip} and OVSeg \cite{liang2023ovseg} explored different strategies for adapting frozen vision-language models to segmentation.

Further refinements have expanded the toolkit for open-vocabulary segmentation. Ghiasi et al. \cite{ghiasi2022open} introduced mask-adapted CLIP models, improving performance in challenging open-vocabulary scenarios by integrating segmentation masks into the vision-language pipeline. X-Decoder \cite{zou2023xdecoder} unified pixel-level and token-level decoding, achieving state-of-the-art results across multiple segmentation tasks. ODISE \cite{xu2023odise} innovatively combined text-to-image diffusion models with discriminative models for open-vocabulary panoptic segmentation. More recently, CAT-Seg \cite{cho2024catseg} introduced cost aggregation techniques to improve segmentation quality, while LMSeg \cite{LMSeg2024} exemplifies state-of-the-art approaches that harness large-scale models and advanced techniques to further enhance open-vocabulary segmentation quality and generalization.

\section{Mask Generation Models}
Mask generation models produce accurate object or region delineations and serve as a backbone for many segmentation systems. Mask R-CNN \cite{he2017mask} extended object detection frameworks to instance segmentation, while Mask2Former \cite{cheng2022mask2former} unified semantic, instance, and panoptic segmentation using a transformer-based design.

The Segment Anything Model (SAM) \cite{kirillov2023segment} marked a significant shift toward prompt-driven segmentation. Trained on a vast and diverse dataset (SA-1B with over 1 billion masks), SAM can segment virtually any object when provided with a suitable prompt—points, boxes, or text—making it particularly versatile for zero-shot generalization. Building upon this foundation, SAM 2 \cite{ravi2024sam2} extended these capabilities to video segmentation, introducing a memory mechanism that enables consistent object tracking across frames. SAM 2 achieves superior accuracy while requiring fewer interactions and operates at real-time speeds (approximately 44 frames per second), making it highly practical for both image and video applications.

By integrating SAM or SAM 2 with open-vocabulary embeddings from CLIP or related models, one can achieve promptable, zero-shot segmentation of arbitrary categories. This synergy of mask generation with vision-language models unlocks flexible and dynamic segmentation capabilities essential for downstream applications.

\section{Generative AI Models for Inpainting}
Generative inpainting models fill masked image regions with plausible, contextually coherent content. Before deep learning, patch-based methods \cite{criminisi2004region} searched for suitable patches to fill holes, but lacked semantic understanding. Context Encoders \cite{pathak2016context} introduced a learning-based approach, using convolutional neural networks and adversarial training to predict missing regions. Subsequent improvements like Partial Convolutions \cite{liu2018image}, Gated Convolutions \cite{yu2019free}, and attention-based models \cite{yu2018generative} enhanced robustness and image fidelity.

The latest generation of inpainting models leverages powerful generative architectures and large-scale training. Stable Diffusion \cite{rombach2022high} employs latent diffusion models to produce high-resolution, semantically consistent completions guided by textual prompts. DALL·E 2 \cite{ramesh2022hierarchical} similarly enables text-driven modifications, allowing users to describe desired changes in natural language. Integrating such generative models with open-vocabulary segmentation and promptable mask generation (e.g., SAM) enables unprecedented levels of interactivity: users can identify segments of interest and instruct the model to add, remove, or alter objects via textual commands.

This combination of open-vocabulary segmentation and generative inpainting lays the foundation for next-generation image editing tools, capable of fluidly responding to a broad range of user-defined concepts and transformations.
