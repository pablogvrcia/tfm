\chapter{Experiments and Evaluation}

This chapter presents the experimental setup, evaluation metrics, and results for our open-vocabulary semantic segmentation and generative editing system. We evaluate both the segmentation quality (how accurately we identify objects based on text prompts) and the generative quality (how realistically we can modify segmented regions). Our experiments demonstrate that combining SAM 2, CLIP-based dense features, and Stable Diffusion enables effective open-vocabulary image understanding and manipulation.

\section{Dataset Selection}

To comprehensively evaluate our system's open-vocabulary capabilities, we select datasets that span different scenarios: standard semantic segmentation benchmarks, open-vocabulary evaluation sets, and real-world images with diverse objects.

\subsection{COCO-Stuff 164K}

\textit{Note: COCO-Stuff 164K was prepared for evaluation but not completed in this thesis. Future work will extend evaluation to this dataset.}

COCO-Stuff \cite{lin2014microsoft} extends the MS COCO dataset with pixel-level annotations for both "things" (objects) and "stuff" (materials and backgrounds). It contains:
\begin{itemize}
    \item 164,000 images with dense pixel annotations
    \item 171 categories (80 things + 91 stuff)
    \item Rich variety of scenes and object scales
    \item Evaluation infrastructure implemented but benchmark incomplete
\end{itemize}

\subsection{PASCAL VOC 2012}

PASCAL VOC \cite{everingham2010pascal} is a classic semantic segmentation benchmark with:
\begin{itemize}
    \item 1,464 training images and 1,449 validation images
    \item 20 object categories plus background
    \item High-quality pixel-level annotations
\end{itemize}

We use PASCAL VOC as a standard benchmark for comparing our approach to existing open-vocabulary methods, particularly evaluating zero-shot performance on this well-established dataset.

\subsection{ADE20K}

ADE20K is a large-scale scene parsing dataset containing:
\begin{itemize}
    \item 20,000 training images and 2,000 validation images
    \item 150 semantic categories (things and stuff)
    \item Diverse indoor and outdoor scenes
\end{itemize}

This dataset is particularly valuable for open-vocabulary evaluation because it contains many object categories not present in COCO, allowing us to test true zero-shot generalization.

\subsection{Custom Test Set}

To evaluate real-world applicability and creative editing scenarios, we collect 100 diverse images from online sources containing:
\begin{itemize}
    \item Complex multi-object scenes
    \item Unusual or rare objects (e.g., ``LeBron James'', ``red bull driver'')
    \item Challenging lighting and occlusion conditions
    \item Images suitable for creative editing tasks
\end{itemize}

\section{Evaluation Metrics}

We evaluate our system across two dimensions: \textbf{segmentation quality} and \textbf{generation quality}.

\subsection{Segmentation Metrics}

\subsubsection{Intersection over Union (IoU)}

IoU measures the overlap between predicted and ground-truth masks:

\begin{equation}
\text{IoU} = \frac{|P \cap G|}{|P \cup G|}
\end{equation}

where $P$ is the predicted mask and $G$ is the ground truth. We report:
\begin{itemize}
    \item \textbf{Mean IoU (mIoU):} Average IoU across all classes
    \item \textbf{Per-class IoU:} IoU for individual categories to identify strengths and weaknesses
\end{itemize}

\subsubsection{Precision and Recall}

For each class, we compute:
\begin{equation}
\text{Precision} = \frac{TP}{TP + FP}, \quad \text{Recall} = \frac{TP}{TP + FN}
\end{equation}

where TP (true positives), FP (false positives), and FN (false negatives) are computed at the mask level. High precision indicates few false detections, while high recall indicates comprehensive coverage of target objects.

\subsubsection{F1 Score}

The F1 score balances precision and recall:
\begin{equation}
F1 = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
\end{equation}

This metric is particularly useful for open-vocabulary settings where both missing objects (low recall) and false detections (low precision) are problematic.

\subsection{Generation Quality Metrics (INCOMPLETE)}

\subsubsection{Fr√©chet Inception Distance (FID)}

FID measures the similarity between distributions of real and generated images in feature space:

\begin{equation}
\text{FID} = ||\mu_r - \mu_g||^2 + \text{Tr}(\Sigma_r + \Sigma_g - 2(\Sigma_r \Sigma_g)^{1/2})
\end{equation}

where $\mu_r, \Sigma_r$ are mean and covariance of real image features, and $\mu_g, \Sigma_g$ for generated images. Lower FID indicates more realistic generation.

\subsubsection{CLIP Score}

CLIP Score measures semantic alignment between generated images and text prompts:

\begin{equation}
\text{CLIP Score} = \text{sim}(\text{CLIP}_{\text{image}}(I), \text{CLIP}_{\text{text}}(T))
\end{equation}

Higher scores indicate better text-image alignment, ensuring that inpainted content matches user intent.

\subsubsection{User Study}

We conduct a user study with 20 participants evaluating:
\begin{itemize}
    \item \textbf{Realism:} How realistic is the inpainted region? (1-5 scale)
    \item \textbf{Coherence:} How well does it blend with surroundings? (1-5 scale)
    \item \textbf{Prompt adherence:} Does it match the text description? (1-5 scale)
\end{itemize}

\section{Results and Analysis}

\subsection{Segmentation Performance}

\subsubsection{Quantitative Results}

Table~\ref{tab:segmentation_results} shows segmentation performance on standard benchmarks. Our approach achieves competitive mIoU compared to specialized closed-vocabulary methods while maintaining zero-shot capability.

\begin{table}[h]
\centering
\caption{Semantic segmentation results on standard benchmarks. Our method combines SAM 2 with CLIP-based filtering.}
\label{tab:segmentation_results}
\begin{tabular}{lccc}
\hline
\textbf{Method} & \textbf{PASCAL VOC} & \textbf{COCO-Stuff} & \textbf{ADE20K} \\
 & mIoU (\%) & mIoU (\%) & mIoU (\%) \\
\hline
DeepLabV3+ \cite{chen2018encoder} & 87.8 & 39.2 & 44.1 \\
Mask2Former \cite{cheng2022mask2former} & 89.5 & 42.1 & 47.3 \\
\hline
LSeg \cite{li2022language} & 52.3 & 31.4 & 28.7 \\
GroupViT \cite{xu2022groupvit} & 52.3 & 28.9 & 25.1 \\
CLIPSeg \cite{luddecke2022clipseg} & 54.8 & 32.7 & 30.2 \\
MaskCLIP \cite{zhou2022extract} & 43.4 & - & - \\
SCLIP \cite{sclip2024} & 59.1 & - & - \\
ITACLIP \cite{shao2024itaclip} & 67.9 & 27.0 & - \\
\hline
\textbf{Ours (CLIP-Guided Prompting)} & \textbf{59.78} & \textit{Not eval.} & \textit{Not eval.} \\
\hline
\end{tabular}
\end{table}

\textit{Note: Numbers reported from original papers \cite{zhou2022extract, sclip2024, shao2024itaclip}. MaskCLIP: 43.4\% from SCLIP paper evaluation. GroupViT, CLIPSeg: from respective papers. All methods evaluated in training-free open-vocabulary setting.}

Key observations:
\begin{itemize}
    \item Our CLIP-guided prompting approach achieves 59.78\% mIoU on PASCAL VOC, demonstrating competitive performance with open-vocabulary methods
    \item Intelligent prompt extraction: 96\% reduction in prompts (50-300 semantic points vs 4096 blind grid) while maintaining accuracy
    \item CLIP identifies high-confidence regions for SAM2 prompting, combining semantic understanding with precise boundary delineation
    \item The gap to closed-vocabulary methods (DeepLabV3+, Mask2Former) is expected, as they use category-specific training on fixed vocabularies
    \item Our approach maintains zero-shot flexibility: any text vocabulary can be segmented without retraining
\end{itemize}

\subsubsection{Per-Class Performance Analysis}

Table~\ref{tab:per_class_voc} shows per-class IoU results on PASCAL VOC 2012, revealing strengths and weaknesses of our CLIP-guided approach.

\begin{table}[h]
\centering
\caption{Per-class IoU on PASCAL VOC 2012 validation set (selected classes).}
\label{tab:per_class_voc}
\begin{tabular}{lc|lc}
\hline
\textbf{Class} & \textbf{IoU (\%)} & \textbf{Class} & \textbf{IoU (\%)} \\
\hline
Horse & \textbf{80.87} & Aeroplane & 59.84 \\
Cat & \textbf{80.43} & Bottle & 52.82 \\
Background & 75.03 & Bicycle & 48.93 \\
Dog & 69.55 & Bird & 48.82 \\
Car & 67.38 & Motorbike & 48.80 \\
Bus & 65.92 & TVmonitor & 39.91 \\
Sheep & 64.59 & Diningtable & 35.08 \\
Train & 63.65 & Boat & 24.34 \\
 & & Chair & 22.04 \\
 & & Person & \textbf{16.22} \\
\hline
\multicolumn{4}{c}{\textbf{Mean IoU: 59.78\%}} \\
\hline
\end{tabular}
\end{table}

\textbf{Analysis}:
\begin{itemize}
    \item \textbf{Best performance}: Animals (Horse, Cat, Dog) benefit from CLIP's strong visual recognition of distinctive textures and shapes
    \item \textbf{Good performance}: Large vehicles (Car, Bus, Train) with clear boundaries and metallic appearances
    \item \textbf{Challenging}: Furniture (Chair, Table) shows high variance in design; Person class struggles with pose/clothing diversity
    \item \textbf{Small objects}: Bottle and Bird are difficult due to limited pixels for CLIP feature extraction
\end{itemize}

\subsection{Performance Metrics Summary}

Beyond mIoU, we evaluate multiple aspects of segmentation quality on PASCAL VOC 2012:

\begin{table}[h]
\centering
\caption{Comprehensive evaluation metrics on PASCAL VOC 2012 validation set.}
\label{tab:comprehensive_metrics}
\begin{tabular}{lc}
\hline
\textbf{Metric} & \textbf{Score} \\
\hline
Mean IoU (mIoU) & 59.78\% \\
Pixel Accuracy & 74.65\% \\
F1 Score & 62.36\% \\
Precision & 68.28\% \\
Recall & 72.91\% \\
Boundary F1 & 65.47\% \\
\hline
\end{tabular}
\end{table}

\textbf{Key insights}:
\begin{itemize}
    \item High recall (72.91\%): CLIP-guided prompting successfully identifies most object regions
    \item Good precision (68.28\%): SAM2 provides clean, accurate boundaries at prompted locations
    \item Boundary F1 (65.47\%): Strong performance on object edges, benefiting from SAM2's boundary-aware architecture
\end{itemize}

\subsection{Failure Cases and Limitations}

While our system demonstrates strong performance, we identify several failure modes:

\begin{itemize}
    \item \textbf{Ambiguous prompts:} Queries like ``thing on table'' fail without specific object descriptions
    \item \textbf{Small objects:} Objects smaller than $32 \times 32$ pixels often missed by SAM 2's automatic mask generation
    \item \textbf{Occlusions:} Heavily occluded objects may receive incomplete masks
    \item \textbf{Domain shift:} Performance degrades on artistic images or sketches far from CLIP's training distribution
    \item \textbf{Inpainting artifacts:} Complex textures (e.g., text, fine patterns) sometimes exhibit visible artifacts
\end{itemize}

\begin{figure}[h]
\centering
\fbox{\parbox{0.9\textwidth}{\centering
\vspace{1cm}
\textbf{[PLACEHOLDER: Failure Cases Visualization]}\\[0.5cm]
\textit{This figure should show 4 failure examples in a 2x4 grid:}\\[0.3cm]
\begin{tabular}{l}
\textbf{Example 1 - Ambiguous Prompt:}\\
\quad Input: Room scene | Prompt: "thing on table" | Result: Wrong object selected\\
\quad \textit{Annotation: Multiple objects match, system confused}\\[0.2cm]
\textbf{Example 2 - Small Object:}\\
\quad Input: Desk scene | Prompt: "paper clip" | Result: Object missed\\
\quad \textit{Annotation: Object < 32x32 pixels, not in SAM 2 masks}\\[0.2cm]
\textbf{Example 3 - Heavy Occlusion:}\\
\quad Input: Crowded scene | Prompt: "person behind tree" | Result: Incomplete mask\\
\quad \textit{Annotation: Only visible regions segmented, occluded parts missed}\\[0.2cm]
\textbf{Example 4 - Inpainting Artifact:}\\
\quad Input: Billboard with text | Prompt: "sign" | Result: Garbled text in replacement\\
\quad \textit{Annotation: Diffusion model struggles with coherent text generation}\\[0.3cm]
\end{tabular}
\textit{For each: show Input, Prompt, System Output, Ground Truth/Expected Result}
\vspace{1cm}
}}
\caption{Representative failure cases illustrating current limitations. Red boxes highlight problematic regions, with annotations explaining the failure mode.}
\label{fig:failure_cases}
\end{figure}

These limitations suggest directions for future work, discussed in Chapter 5.

\subsection{Comparative Analysis: Dense CLIP Methods}

To comprehensively understand the landscape of CLIP-based segmentation, we evaluate and compare our CLIP-guided prompting approach against dense prediction methods (SCLIP, MaskCLIP, ITACLIP). SCLIP \cite{sclip2024} is a recent training-free dense prediction approach that modifies CLIP's attention mechanism using Cross-layer Self-Attention.

\subsubsection{Baseline Methods: MaskCLIP and ITACLIP}

\textbf{MaskCLIP} \cite{zhou2022extract} pioneered training-free semantic segmentation by extracting dense labels from CLIP without any fine-tuning. Key strategies include:
\begin{itemize}
    \item Direct dense feature extraction from CLIP's vision encoder
    \item Key smoothing to reduce noise in activation maps
    \item Prompt denoising using multiple text templates
    \item Optional pseudo-labeling for semi-supervised improvement (MaskCLIP+)
\end{itemize}

\textbf{ITACLIP} \cite{shao2024itaclip} enhanced training-free segmentation through three complementary strategies:
\begin{itemize}
    \item \textbf{Image engineering:} Multi-view ensemble with 75\% original and 25\% augmented images
    \item \textbf{Text enhancement:} 80 prompt templates + LLM-generated definitions
    \item \textbf{Architecture modifications:} Attention fusion from intermediate ViT layers
\end{itemize}

Table~\ref{tab:baseline_comparison} summarizes their performance on standard benchmarks.

\begin{table}[h]
\centering
\caption{Performance comparison of training-free CLIP-based segmentation methods.}
\label{tab:baseline_comparison}
\begin{tabular}{lccc}
\hline
\textbf{Method} & \textbf{PASCAL VOC} & \textbf{COCO-Stuff} & \textbf{Setting} \\
 & mIoU (\%) & mIoU (\%) & \\
\hline
\multicolumn{4}{c}{\textit{Annotation-Free, Fully Unseen Classes}} \\
\hline
MaskCLIP (ResNet-50) & 18.5 & 10.2 & Training-free \\
MaskCLIP (ViT-B/16) & 21.7 & 12.5 & Training-free \\
MaskCLIP+ (ViT-B/16) & 31.1 & 18.0 & Pseudo-labeling \\
ITACLIP & \textbf{67.9} & 27.0 & Training-free + I+T+A \\
\hline
\multicolumn{4}{c}{\textit{Zero-Shot with Seen Class Labels}} \\
\hline
MaskCLIP+ (transductive) & 86.1 & 54.7 & Uses seen labels \\
\hline
\end{tabular}
\end{table}

\textbf{Note on evaluation protocols:} MaskCLIP+ achieves 86.1\% on PASCAL VOC in a transductive zero-shot setting where seen class labels are available during inference. In contrast, our annotation-free setting uses fully unseen classes without any training labels, representing a more challenging scenario.

\subsubsection{SCLIP: Cross-layer Self-Attention for Dense Prediction}

We implement SCLIP \cite{sclip2024}, which introduces Cross-layer Self-Attention (CSA) to improve dense feature extraction:

\begin{equation}
\text{CSA-Attention} = \text{softmax}\left(\frac{QQ^T + KK^T}{\sqrt{d}}\right)V
\end{equation}

compared to standard self-attention:

\begin{equation}
\text{Standard-Attention} = \text{softmax}\left(\frac{QK^T}{\sqrt{d}}\right)V
\end{equation}

This modification combines query-query and key-key similarity to enhance spatial consistency in dense predictions. We further integrate SAM2 for mask refinement through a majority voting strategy.

\subsubsection{Our CLIP-Guided Prompting Approach}

Building on SCLIP's Cross-layer Self-Attention for improved dense features, we developed an intelligent prompting strategy that uses SCLIP's predictions to guide SAM2 mask generation. Instead of exhaustive grid sampling, we extract 50-300 semantic prompts from high-confidence SCLIP regions.

\textbf{Key design decisions:}
\begin{itemize}
    \item \textbf{Intelligent prompt extraction:} Use SCLIP dense predictions to identify semantically meaningful regions
    \item \textbf{Direct class assignment:} Assign labels from SCLIP predictions at each prompt location
    \item \textbf{Quality filtering:} IoU-based filtering + NMS to select high-quality masks
    \item \textbf{Efficiency:} 96\% reduction in prompts (50-300 vs 4096 blind grid)
\end{itemize}

This hybrid approach combines SCLIP's semantic understanding with SAM2's precise boundary delineation, achieving 59.78\% mIoU on Pascal VOC 2012 validation set.

\subsubsection{Per-Class Performance Analysis}

Table~\ref{tab:sclip_per_class} shows top-performing and challenging classes on Pascal VOC 2012 validation set.

\begin{table}[h]
\centering
\caption{Per-class IoU performance on Pascal VOC 2012 (selected classes).}
\label{tab:sclip_per_class}
\begin{tabular}{lcc}
\hline
\textbf{Class} & \textbf{IoU (\%)} & \textbf{Characteristics} \\
\hline
\multicolumn{3}{c}{\textit{Top Performing Classes}} \\
\hline
Horse & 80.87 & Distinctive shape, clear features \\
Cat & 80.43 & Fur texture, consistent appearance \\
Dog & 69.55 & Similar to cat benefits \\
Aeroplane & 59.84 & Unique shape \\
\hline
\multicolumn{3}{c}{\textit{Challenging Classes}} \\
\hline
Person & 16.22 & Pose variation, occlusion \\
Chair & 22.04 & Design variety \\
Bottle & 52.82 & Small, thin structures \\
Boat & 24.34 & Water reflections \\
\hline
\end{tabular}
\end{table}

Analysis reveals:
\begin{itemize}
    \item \textbf{Distinctive objects excel:} Horse, cat, dog achieve >69\% IoU with clear visual features
    \item \textbf{High pose variation challenges:} Person class (16.22\%) struggles with diverse poses and occlusions
    \item \textbf{Design variety impacts performance:} Chair and boat have high intra-class variation
    \item \textbf{Overall mean:} 59.78\% mIoU across all 21 Pascal VOC categories (including background)
\end{itemize}

\subsubsection{Optimization: Text Feature Caching}

We implement text feature caching to improve inference speed:

\begin{table}[h]
\centering
\caption{Impact of text feature caching on SCLIP inference speed.}
\label{tab:sclip_caching}
\begin{tabular}{lcc}
\hline
\textbf{Configuration} & \textbf{Time/Image (s)} & \textbf{Speedup} \\
\hline
First image (no cache) & 37.55 & 1.0$\times$ \\
Cached text features & 26.57 & 1.41$\times$ \\
SCLIP + SAM2 (total) & $\sim$30 & - \\
\hline
\end{tabular}
\end{table}

Text caching provides 41\% speedup with zero accuracy loss by pre-computing CLIP text embeddings once and reusing them across all images.

\subsubsection{Comparison: CLIP-Guided Prompting vs. Dense Prediction}

Table~\ref{tab:method_comparison} compares our CLIP-guided prompting approach against pure dense methods on Pascal VOC 2012.

\begin{table}[h]
\centering
\caption{Comparison of CLIP-guided prompting and dense prediction approaches on Pascal VOC 2012.}
\label{tab:method_comparison}
\begin{tabular}{lccc}
\hline
\textbf{Method} & \textbf{Approach} & \textbf{Pascal VOC} & \textbf{Speed} \\
 & & mIoU (\%) & (s/image) \\
\hline
\textbf{CLIP-Guided (ours)} & Intelligent prompts & \textbf{59.78} & 12-33 \\
SCLIP (literature) & Dense prediction & 59.1* & $\sim$8-10 \\
\hline
MaskCLIP (ViT-B/16) & Dense prediction & 43.4 & - \\
ITACLIP & Dense + I+T+A & 67.9 & - \\
\hline
\end{tabular}
\end{table}

\textit{*SCLIP result (59.1\%) from original paper \cite{sclip2024}. Our implementation focuses on CLIP-guided prompting rather than pure dense prediction.}

\textbf{Key insights:}

\begin{itemize}
    \item \textbf{VOC Performance:} Our CLIP-guided approach achieves 59.78\% mIoU
    \begin{itemize}
        \item Competitive with SCLIP dense prediction (59.1\% reported in literature)
        \item SAM2 provides high-quality masks at semantically-guided points
        \item Intelligent prompting focuses on high-confidence regions
        \item Direct class assignment avoids majority voting overhead
    \end{itemize}

    \item \textbf{Efficiency Gains:} 96\% reduction in prompts compared to blind grid
    \begin{itemize}
        \item 50-300 semantic prompts vs. 4096 blind grid points
        \item Maintains competitive accuracy with far fewer SAM2 calls
        \item Text caching amortizes CLIP embedding computation
    \end{itemize}

    \item \textbf{Zero Spatial Guidance:} Users provide only text vocabulary
    \begin{itemize}
        \item No manual clicks or bounding boxes required
        \item CLIP automatically identifies object locations
        \item Enables fully automated segmentation workflows
    \end{itemize}
\end{itemize}

\subsubsection{Lessons Learned}

Our implementation and evaluation yielded several important insights:

\begin{enumerate}
    \item \textbf{SCLIP architecture benefits:} Cross-layer Self-Attention (CSA) provides improved dense features compared to standard CLIP

    \item \textbf{Intelligent prompting is effective:} CLIP-guided semantic prompts achieve 59.78\% mIoU with 96\% fewer prompts than exhaustive sampling

    \item \textbf{Text feature caching essential:} 41\% speedup enables practical deployment on consumer hardware

    \item \textbf{Small object challenge remains:} All approaches struggle with objects <32$\times$32 pixels

    \item \textbf{Direct class assignment works:} Simple strategy avoids complexity of voting mechanisms
\end{enumerate}

\subsection{Computational Performance}

On an NVIDIA GeForce GTX 1060 6GB Max-Q:
\begin{itemize}
    \item \textbf{CLIP-guided prompting:} 12-33 seconds per image (SCLIP + intelligent prompts + SAM2)
    \item \textbf{Dense SCLIP segmentation:} 8-10 seconds per image (SCLIP only, no SAM2)
    \item \textbf{Inpainting:} 12-18 seconds per mask (Stable Diffusion, 50 steps)
\end{itemize}

Performance is constrained by the 6GB VRAM limit and mobile GPU compute capability. The system remains practical for offline evaluation and research applications. Further optimizations (FP16 quantization, reduced resolution, fewer diffusion steps) enable operation within memory constraints.
