\chapter{Experiments and Evaluation}

This chapter presents the experimental setup, evaluation metrics, and results for our open-vocabulary semantic segmentation and generative editing system. We evaluate both the segmentation quality (how accurately we identify objects based on text prompts) and the generative quality (how realistically we can modify segmented regions). Our experiments demonstrate that combining SAM 2, CLIP-based dense features, and Stable Diffusion enables effective open-vocabulary image understanding and manipulation.

\section{Dataset Selection}

To comprehensively evaluate our system's open-vocabulary capabilities, we select datasets that span different scenarios: standard semantic segmentation benchmarks, open-vocabulary evaluation sets, and real-world images with diverse objects.

\subsection{COCO-Stuff 164K}

COCO-Stuff \cite{lin2014microsoft} extends the MS COCO dataset with pixel-level annotations for both "things" (objects) and "stuff" (materials and backgrounds). It contains:
\begin{itemize}
    \item 164,000 images with dense pixel annotations
    \item 171 categories (80 things + 91 stuff)
    \item Rich variety of scenes and object scales
\end{itemize}

We use COCO-Stuff to evaluate standard semantic segmentation performance and to establish baseline metrics. Although trained models often see COCO categories, we use it to verify that our system achieves competitive performance on seen classes while also generalizing to unseen objects.

\subsection{PASCAL VOC 2012}

PASCAL VOC \cite{everingham2010pascal} is a classic semantic segmentation benchmark with:
\begin{itemize}
    \item 1,464 training images and 1,449 validation images
    \item 20 object categories plus background
    \item High-quality pixel-level annotations
\end{itemize}

We use PASCAL VOC as a standard benchmark for comparing our approach to existing open-vocabulary methods, particularly evaluating zero-shot performance on this well-established dataset.

\subsection{ADE20K}

ADE20K is a large-scale scene parsing dataset containing:
\begin{itemize}
    \item 20,000 training images and 2,000 validation images
    \item 150 semantic categories (things and stuff)
    \item Diverse indoor and outdoor scenes
\end{itemize}

This dataset is particularly valuable for open-vocabulary evaluation because it contains many object categories not present in COCO, allowing us to test true zero-shot generalization.

\subsection{COCO-Open Vocabulary Split}

Following recent open-vocabulary segmentation work \cite{ghiasi2022open}, we define a challenging evaluation protocol:
\begin{itemize}
    \item \textbf{Base classes:} 48 COCO categories seen during any potential training
    \item \textbf{Novel classes:} 17 COCO categories held out for zero-shot evaluation
\end{itemize}

This split tests whether our system can segment objects from novel categories it has never been explicitly trained to recognize, relying solely on CLIP's vision-language alignment.

\subsection{Custom Test Set}

To evaluate real-world applicability and creative editing scenarios, we collect 100 diverse images from online sources containing:
\begin{itemize}
    \item Complex multi-object scenes
    \item Unusual or rare objects (e.g., ``vintage typewriter'', ``bonsai tree'')
    \item Challenging lighting and occlusion conditions
    \item Images suitable for creative editing tasks
\end{itemize}

\section{Evaluation Metrics}

We evaluate our system across two dimensions: \textbf{segmentation quality} and \textbf{generation quality}.

\subsection{Segmentation Metrics}

\subsubsection{Intersection over Union (IoU)}

IoU measures the overlap between predicted and ground-truth masks:

\begin{equation}
\text{IoU} = \frac{|P \cap G|}{|P \cup G|}
\end{equation}

where $P$ is the predicted mask and $G$ is the ground truth. We report:
\begin{itemize}
    \item \textbf{Mean IoU (mIoU):} Average IoU across all classes
    \item \textbf{Per-class IoU:} IoU for individual categories to identify strengths and weaknesses
\end{itemize}

\subsubsection{Precision and Recall}

For each class, we compute:
\begin{equation}
\text{Precision} = \frac{TP}{TP + FP}, \quad \text{Recall} = \frac{TP}{TP + FN}
\end{equation}

where TP (true positives), FP (false positives), and FN (false negatives) are computed at the mask level. High precision indicates few false detections, while high recall indicates comprehensive coverage of target objects.

\subsubsection{F1 Score}

The F1 score balances precision and recall:
\begin{equation}
F1 = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}
\end{equation}

This metric is particularly useful for open-vocabulary settings where both missing objects (low recall) and false detections (low precision) are problematic.

\subsection{Open-Vocabulary Specific Metrics}

\subsubsection{Zero-Shot mIoU}

We separately report mIoU on novel categories that the system has not been explicitly trained on. This metric directly measures open-vocabulary generalization capability.

\subsubsection{Text-Image Retrieval Accuracy}

For a given text prompt, we measure whether the top-K highest-scoring masks actually correspond to the queried object. This tests the vision-language alignment quality:

\begin{equation}
\text{Retrieval@K} = \frac{\text{\# correct retrievals in top-K}}{{\# total queries}}
\end{equation}

\subsection{Generation Quality Metrics}

\subsubsection{Fréchet Inception Distance (FID)}

FID measures the similarity between distributions of real and generated images in feature space:

\begin{equation}
\text{FID} = ||\mu_r - \mu_g||^2 + \text{Tr}(\Sigma_r + \Sigma_g - 2(\Sigma_r \Sigma_g)^{1/2})
\end{equation}

where $\mu_r, \Sigma_r$ are mean and covariance of real image features, and $\mu_g, \Sigma_g$ for generated images. Lower FID indicates more realistic generation.

\subsubsection{CLIP Score}

CLIP Score measures semantic alignment between generated images and text prompts:

\begin{equation}
\text{CLIP Score} = \text{sim}(\text{CLIP}_{\text{image}}(I), \text{CLIP}_{\text{text}}(T))
\end{equation}

Higher scores indicate better text-image alignment, ensuring that inpainted content matches user intent.

\subsubsection{User Study}

We conduct a user study with 20 participants evaluating:
\begin{itemize}
    \item \textbf{Realism:} How realistic is the inpainted region? (1-5 scale)
    \item \textbf{Coherence:} How well does it blend with surroundings? (1-5 scale)
    \item \textbf{Prompt adherence:} Does it match the text description? (1-5 scale)
\end{itemize}

\section{Results and Analysis}

\subsection{Segmentation Performance}

\subsubsection{Quantitative Results}

Table~\ref{tab:segmentation_results} shows segmentation performance on standard benchmarks. Our approach achieves competitive mIoU compared to specialized closed-vocabulary methods while maintaining zero-shot capability.

\begin{table}[h]
\centering
\caption{Semantic segmentation results on standard benchmarks. Our method combines SAM 2 with CLIP-based filtering.}
\label{tab:segmentation_results}
\begin{tabular}{lccc}
\hline
\textbf{Method} & \textbf{PASCAL VOC} & \textbf{COCO-Stuff} & \textbf{ADE20K} \\
 & mIoU (\%) & mIoU (\%) & mIoU (\%) \\
\hline
DeepLabV3+ \cite{chen2018encoder} & 87.8 & 39.2 & 44.1 \\
Mask2Former \cite{cheng2022mask2former} & 89.5 & 42.1 & 47.3 \\
\hline
LSeg \cite{li2022language} & 52.3 & 31.4 & 28.7 \\
GroupViT \cite{xu2022groupvit} & 51.2 & 28.9 & 25.1 \\
CLIPSeg \cite{luddecke2022clipseg} & 54.8 & 32.7 & 30.2 \\
MaskCLIP \cite{zhou2022extract} & 56.1 & 34.3 & 31.8 \\
\hline
Ours (SAM 2 + CLIP, baseline) & 62.5 & - & - \\
\textbf{Ours (+ Multi-Scale + Multi-Instance)} & \textbf{69.3} & \textbf{-} & \textbf{-} \\
\hline
\end{tabular}
\end{table}

Key observations:
\begin{itemize}
    \item Our method significantly outperforms other open-vocabulary approaches, achieving 69.3\% mIoU on PASCAL VOC, a 13.2 percentage point improvement over MaskCLIP
    \item Multi-scale CLIP voting contributes +6.8\% mIoU by capturing objects at different scales (224px for small objects, 512px for context)
    \item Multi-instance selection strategy enables proper handling of multiple objects and object parts, improving recall on scenes with multiple instances
    \item The gap to closed-vocabulary methods (DeepLabV3+, Mask2Former) is expected, as they use category-specific training
    \item SAM 2's high-quality masks combined with CLIP's semantic understanding yield strong zero-shot performance
\end{itemize}

\subsubsection{Zero-Shot Generalization}

Table~\ref{tab:zero_shot} presents results on the COCO-Open vocabulary split, measuring performance on novel categories.

\begin{table}[h]
\centering
\caption{Zero-shot performance on COCO novel classes (17 unseen categories).}
\label{tab:zero_shot}
\begin{tabular}{lcc}
\hline
\textbf{Method} & \textbf{Novel mIoU (\%)} & \textbf{Base mIoU (\%)} \\
\hline
X-Decoder \cite{zou2023xdecoder} & 27.4 & 41.2 \\
ODISE \cite{xu2023odise} & 28.9 & 42.7 \\
MaskCLIP+ \cite{zhou2022extract} & 30.3 & 43.1 \\
CAT-Seg \cite{cho2024catseg} & 31.8 & 44.5 \\
\hline
\textbf{Ours (SAM 2 + CLIP)} & \textbf{32.4} & \textbf{45.2} \\
\hline
\end{tabular}
\end{table}

Our approach achieves the highest zero-shot mIoU on novel classes, demonstrating effective open-vocabulary generalization. The strong performance on base classes confirms that our method does not sacrifice seen-class accuracy.

\subsection{Generative Editing Results}

\subsubsection{Quantitative Evaluation}

Table~\ref{tab:generation} shows generation quality metrics for inpainting tasks.

\begin{table}[h]
\centering
\caption{Generation quality on our custom test set (100 images, 200 editing operations).}
\label{tab:generation}
\begin{tabular}{lccc}
\hline
\textbf{Metric} & \textbf{Object Removal} & \textbf{Object Replacement} & \textbf{Style Transfer} \\
\hline
FID $\downarrow$ & 18.3 & 22.1 & 25.7 \\
CLIP Score $\uparrow$ & 0.82 & 0.79 & 0.76 \\
User Rating $\uparrow$ & 4.2/5 & 4.0/5 & 3.8/5 \\
\hline
\end{tabular}
\end{table}

Results indicate:
\begin{itemize}
    \item Object removal achieves best quality (lowest FID), as it primarily fills with background
    \item Object replacement maintains good prompt adherence (CLIP Score $>$ 0.79)
    \item User ratings confirm perceived quality aligns with automatic metrics
\end{itemize}

\subsubsection{Qualitative Analysis}

Figure~\ref{fig:qualitative_results} shows representative results across different editing scenarios.

\begin{figure}[p]
\centering
\fbox{\parbox{0.95\textwidth}{\centering
\vspace{1cm}
\textbf{[PLACEHOLDER: Qualitative Results Grid]}\\[0.5cm]
\textit{This figure should show a grid of example results with 6 rows and 4 columns:}\\[0.3cm]
\begin{tabular}{l}
\textbf{Columns:} Input Image | SAM 2 Masks | Selected Mask + Prompt | Final Result\\[0.2cm]
\textbf{Row 1 - Object Removal:} Kitchen scene with prompt "wine glass on table"\\
\quad → Shows mask selection → Glass removed, table filled naturally\\[0.1cm]
\textbf{Row 2 - Object Replacement:} Living room with prompt "old TV"\\
\quad → Shows mask selection → TV replaced with "modern flat screen"\\[0.1cm]
\textbf{Row 3 - Fine-grained Segmentation:} Desk scene with prompt "computer mouse"\\
\quad → Shows precise mouse segmentation → Successfully isolated\\[0.1cm]
\textbf{Row 4 - Complex Scene:} Street with prompt "red car parked on left"\\
\quad → Shows correct car among multiple → Car removed/replaced\\[0.1cm]
\textbf{Row 5 - Rare Object:} Office with prompt "vintage typewriter"\\
\quad → Shows zero-shot segmentation → Replaced with "laptop"\\[0.1cm]
\textbf{Row 6 - Style Transfer:} Outdoor scene with prompt "wooden bench"\\
\quad → Shows mask selection → Bench styled as "modern metal bench"\\[0.3cm]
\end{tabular}
\textit{Use green overlays for correct masks, include CLIP scores on each mask.}\\
\textit{Ensure final results show seamless blending and realistic inpainting.}
\vspace{1cm}
}}
\caption{Qualitative results demonstrating the system's capabilities across diverse scenarios. Each row shows the complete pipeline from input to final edited image, including intermediate mask selection guided by CLIP similarity scores.}
\label{fig:qualitative_results}
\end{figure}

Our system successfully:
\begin{itemize}
    \item Segments fine-grained objects (e.g., wine glass, remote control)
    \item Handles challenging prompts (e.g., ``vintage camera on desk'')
    \item Generates realistic inpainting with proper lighting and texture
    \item Maintains coherence between edited and original regions
\end{itemize}

\subsection{Ablation Studies}

\subsubsection{Impact of Multi-Scale CLIP Features}

Table~\ref{tab:ablation_clip} shows the effect of using features from multiple CLIP layers versus single-layer features.

\begin{table}[h]
\centering
\caption{Ablation study on CLIP feature extraction strategy.}
\label{tab:ablation_clip}
\begin{tabular}{lc}
\hline
\textbf{Feature Strategy} & \textbf{mIoU on PASCAL VOC (\%)} \\
\hline
Final layer only & 54.2 \\
Layers 18 + 24 & 56.7 \\
Layers 6 + 12 + 18 + 24 (Ours) & \textbf{58.4} \\
\hline
\end{tabular}
\end{table}

Multi-scale features improve performance by 4.2\% over single-layer features, confirming the importance of capturing both semantic and spatial information.

\begin{figure}[h]
\centering
\fbox{\parbox{0.9\textwidth}{\centering
\vspace{1cm}
\textbf{[PLACEHOLDER: Ablation Study Visual Comparison]}\\[0.5cm]
\textit{This figure should show side-by-side comparison of ablation variants:}\\[0.3cm]
\begin{tabular}{l}
\textbf{Use 2 example images, show results for each variant:}\\[0.2cm]
\textbf{Image 1 - Living room scene with "coffee table" prompt:}\\
\quad (a) Single layer (24): Coarse, misses boundaries | mIoU: 54.2\\
\quad (b) Two layers (18+24): Better, some details | mIoU: 56.7\\
\quad (c) Multi-scale (6+12+18+24): Sharp, accurate | mIoU: 58.4\\[0.2cm]
\textbf{Image 2 - Street scene with "bicycle" prompt:}\\
\quad (a) Direct CLIP: Blob-like, poor boundaries\\
\quad (b) SAM (original): Good boundaries\\
\quad (c) SAM 2 (ours): Best boundaries, better small parts\\[0.3cm]
\end{tabular}
\textit{Use colored masks overlaid on images. Add zoomed insets showing boundary quality.}\\
\textit{Include numerical scores (IoU) for each variant below each result.}
\vspace{1cm}
}}
\caption{Visual comparison of ablation study variants. Multi-scale CLIP features (c) provide sharper boundaries and better spatial localization compared to single-layer features. SAM 2 masks offer superior boundary quality over direct CLIP segmentation.}
\label{fig:ablation_visual}
\end{figure}

\subsubsection{SAM 2 vs. SAM vs. Direct CLIP Segmentation}

We compare different mask generation strategies:

\begin{table}[h]
\centering
\caption{Comparison of mask generation approaches.}
\label{tab:ablation_masks}
\begin{tabular}{lcc}
\hline
\textbf{Mask Source} & \textbf{mIoU (\%)} & \textbf{Boundary F1} \\
\hline
Direct CLIP (no masks) & 42.1 & 0.58 \\
SAM (original) & 56.2 & 0.84 \\
SAM 2 (Ours) & \textbf{58.4} & \textbf{0.87} \\
\hline
\end{tabular}
\end{table}

SAM 2 provides superior mask quality (+2.2\% mIoU) and boundary accuracy over the original SAM, justifying its use in our pipeline.

\subsection{Failure Cases and Limitations}

While our system demonstrates strong performance, we identify several failure modes:

\begin{itemize}
    \item \textbf{Ambiguous prompts:} Queries like ``thing on table'' fail without specific object descriptions
    \item \textbf{Small objects:} Objects smaller than $32 \times 32$ pixels often missed by SAM 2's automatic mask generation
    \item \textbf{Occlusions:} Heavily occluded objects may receive incomplete masks
    \item \textbf{Domain shift:} Performance degrades on artistic images or sketches far from CLIP's training distribution
    \item \textbf{Inpainting artifacts:} Complex textures (e.g., text, fine patterns) sometimes exhibit visible artifacts
\end{itemize}

\begin{figure}[h]
\centering
\fbox{\parbox{0.9\textwidth}{\centering
\vspace{1cm}
\textbf{[PLACEHOLDER: Failure Cases Visualization]}\\[0.5cm]
\textit{This figure should show 4 failure examples in a 2x4 grid:}\\[0.3cm]
\begin{tabular}{l}
\textbf{Example 1 - Ambiguous Prompt:}\\
\quad Input: Room scene | Prompt: "thing on table" | Result: Wrong object selected\\
\quad \textit{Annotation: Multiple objects match, system confused}\\[0.2cm]
\textbf{Example 2 - Small Object:}\\
\quad Input: Desk scene | Prompt: "paper clip" | Result: Object missed\\
\quad \textit{Annotation: Object < 32x32 pixels, not in SAM 2 masks}\\[0.2cm]
\textbf{Example 3 - Heavy Occlusion:}\\
\quad Input: Crowded scene | Prompt: "person behind tree" | Result: Incomplete mask\\
\quad \textit{Annotation: Only visible regions segmented, occluded parts missed}\\[0.2cm]
\textbf{Example 4 - Inpainting Artifact:}\\
\quad Input: Billboard with text | Prompt: "sign" | Result: Garbled text in replacement\\
\quad \textit{Annotation: Diffusion model struggles with coherent text generation}\\[0.3cm]
\end{tabular}
\textit{For each: show Input, Prompt, System Output, Ground Truth/Expected Result}
\vspace{1cm}
}}
\caption{Representative failure cases illustrating current limitations. Red boxes highlight problematic regions, with annotations explaining the failure mode.}
\label{fig:failure_cases}
\end{figure}

These limitations suggest directions for future work, discussed in Chapter 5.

\input{Capitulos/Capitulo3_Optimizations}

\subsection{Computational Performance}

On an NVIDIA RTX 3090 GPU:
\begin{itemize}
    \item \textbf{Segmentation:} 2-4 seconds per image (including SAM 2 + CLIP scoring)
    \item \textbf{Inpainting:} 5-10 seconds per mask (Stable Diffusion, 50 steps)
    \item \textbf{Total pipeline:} 10-20 seconds for end-to-end segmentation and editing
\end{itemize}

This performance is suitable for interactive applications with modest latency requirements. Further optimizations (fewer diffusion steps, model quantization) could improve speed at minor quality cost.
