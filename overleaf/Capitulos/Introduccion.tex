\chapter{Introduction}
\pagenumbering{arabic}

\section{Motivation}

Traditional semantic segmentation models are constrained by closed vocabularies, recognizing only objects from predefined categories. This limitation hinders real-world deployment where novel concepts frequently arise. Open-vocabulary semantic segmentation addresses this by leveraging vision-language models like CLIP \cite{radford2021learning}, enabling segmentation of arbitrary concepts specified through natural language.

While CLIP demonstrates impressive zero-shot classification capabilities, adapting it for dense pixel-wise prediction poses significant challenges. CLIP's Vision Transformer backbone, trained for image-level understanding, exhibits weak spatial localization when applied directly to segmentation. Recent work has explored dense prediction from CLIP features \cite{zhou2022extract, sclip2024}, yet performance gaps persist, particularly for challenging classes like humans and complex scenes with fine-grained boundaries.

\section{Problem Statement}

This thesis addresses three fundamental limitations of CLIP-based open-vocabulary segmentation:

\begin{enumerate}
\item \textbf{Weak spatial localization:} CLIP's Vision Transformer produces coarse 14×14 feature grids for 224×224 images, limiting pixel-level precision. Existing methods suffer from blurry boundaries and poor small-object detection.

\item \textbf{Poor human/person segmentation:} CLIP's global feature aggregation struggles with articulated poses, clothing variations, and body part delineation, resulting in fragmented masks for the person class—critical for real-world applications.

\item \textbf{Computational inefficiency:} Dense prediction methods require extensive template ensembling (80+ prompts) and high-resolution processing, making real-time deployment infeasible on consumer hardware.
\end{enumerate}

To address these challenges, we systematically integrate recent advances from computer vision literature into a unified training-free framework, achieving substantial improvements over baseline SCLIP \cite{sclip2024} performance.


\section{Contributions}

This thesis makes the following contributions by systematically integrating state-of-the-art methods into a unified SCLIP-based framework:

\subsection{Phase 1: Spatial Enhancement and Boundary Refinement}

We integrate three complementary techniques to address CLIP's weak spatial localization:

\begin{itemize}
\item \textbf{LoftUp feature upsampling:} Adapting recent work from ICCV 2025, we upsample CLIP features from 14×14 to 28×28 while preserving semantic content, achieving +2-4\% mIoU improvement.

\item \textbf{ResCLIP residual attention:} We implement Residual Cross-correlation Self-Attention (RCS) and Semantic Feedback Refinement (SFR) from CVPR 2025, enhancing spatial coherence through multi-scale feature aggregation (+8-13\% mIoU).

\item \textbf{DenseCRF boundary refinement:} Classical Conditional Random Field post-processing ensures appearance consistency and smooth object boundaries (+1-2\% mIoU, +3-5\% boundary F1).
\end{itemize}

\textbf{Expected Phase 1 improvement:} +11-19\% mIoU over baseline SCLIP (22.77\% baseline).

\subsection{Phase 2A: Training-Free Human Parsing Enhancement}

To address poor person-class segmentation, we integrate recent training-free methods:

\begin{itemize}
\item \textbf{CLIPtrase self-correlation recalibration:} Following ECCV 2024 work, we recalibrate CLIP's self-attention through correlation matrix enhancement, improving local feature awareness (+5-10\% mIoU for person class).

\item \textbf{CLIP-RC regional clue extraction:} We implement regional feature extraction from CVPR 2024 to combat CLIP's global feature dominance, preserving fine-grained body part details (+8-12\% mIoU for person class).
\end{itemize}

\textbf{Expected Phase 2A improvement:} +7-12\% overall mIoU, +13-22\% for person class specifically.

\subsection{Phase 2B: Prompt Engineering and Template Optimization}

We replace generic ImageNet classification templates with task-specific dense prediction prompts:

\begin{itemize}
\item \textbf{Top-7 template strategy:} Curated from PixelCLIP research, achieving +2-3\% mIoU with 11.4× speedup over 80-template ensembles.

\item \textbf{Adaptive template selection:} Class-type aware prompts (stuff vs things) tailored to object characteristics (+3-5\% mIoU).

\item \textbf{Material-aware templates:} Specialized handling for compound classes (wall-brick, floor-marble), improving texture-based segmentation.
\end{itemize}

\textbf{Expected Phase 2B improvement:} +3-5\% mIoU with 3-11× computational speedup.

\subsection{Phase 2C: Confidence Sharpening (In Progress)}

To address flat prediction distributions where multiple classes have similar confidence:

\begin{itemize}
\item \textbf{Hierarchical class grouping:} Two-stage prediction reduces classification complexity (stuff vs things, then specific classes).

\item \textbf{Confidence calibration:} Sharpen predictions for uncertain pixels using temperature scaling and entropy-based filtering.
\end{itemize}

\textbf{Expected Phase 2C improvement:} +5-8\% mIoU.

\subsection{System-Level Contributions}

\begin{itemize}
\item \textbf{Comprehensive benchmark framework:} Rigorous evaluation infrastructure for COCO-Stuff-164k and PASCAL VOC 2012 with extensive ablation studies.

\item \textbf{Modular implementation:} Each phase can be enabled/disabled independently, facilitating systematic performance analysis.

\item \textbf{Integration with generative AI:} Optional Stable Diffusion v2 integration for text-driven image editing applications.
\end{itemize}

\textbf{Expected cumulative improvement:} +17-32\% mIoU over baseline SCLIP, targeting 40-48\% on COCO-Stuff-164k.

\section{Thesis Structure}

\begin{itemize}
\item \textbf{Chapter 2:} Reviews related work in open-vocabulary segmentation, CLIP-based dense prediction (MaskCLIP, SCLIP, CLIPSeg), and relevant enhancement techniques. Detailed transformer/attention mechanisms are moved to the annex for conciseness.

\item \textbf{Chapter 3:} Presents our methodology, structured around the four improvement phases:
\begin{itemize}
\item Baseline SCLIP architecture and Cross-layer Self-Attention
\item Phase 1 spatial enhancements (LoftUp, ResCLIP, DenseCRF)
\item Phase 2A human parsing improvements (CLIPtrase, CLIP-RC)
\item Phase 2B prompt engineering strategies
\item Phase 2C confidence sharpening (in progress)
\end{itemize}

\item \textbf{Chapter 4:} Reports experimental results with ablation studies for each phase, benchmark performance on COCO-Stuff and PASCAL VOC 2012, and analysis of per-class improvements (particularly person class).

\item \textbf{Chapter 5:} Concludes with contributions summary, limitations, and future work directions.

\item \textbf{Annex:} Contains detailed technical background on Vision Transformers, self-attention mechanisms, and implementation details moved from the main chapters for improved readability.
\end{itemize}