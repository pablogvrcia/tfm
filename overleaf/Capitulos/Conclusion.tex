\chapter{Conclusions and Future Work}

This thesis presents SCLIP-guided prompting for open-vocabulary semantic segmentation, combining CLIP's dense predictions with SAM2's precise boundaries through intelligent prompt extraction. The approach achieves 68.09\% mIoU on PASCAL VOC 2012 while enabling zero-shot segmentation and generative editing applications. This chapter summarizes the technical contributions, discusses limitations, and identifies future research directions.

\section{Summary of Contributions}

The core contribution is an intelligent prompting strategy that extracts semantically-guided prompt locations from SCLIP's dense predictions to direct SAM2's segmentation capacity. Rather than exhaustive spatial sampling, the method identifies high-confidence semantic regions through confidence filtering and connected component analysis, then uses region centroids as SAM2 prompts. This achieves 68.09\% mIoU on PASCAL VOC 2012, a 0.19 percentage point improvement over ITACLIP (67.9\%), establishing competitive performance among training-free open-vocabulary methods.

The ablation studies quantify individual component contributions: SCLIP-guided SAM2 prompting provides the largest improvement (+17.58pp mIoU over SCLIP baseline), followed by multi-term descriptor files (+7.39pp) that capture intra-class variation, and DenseCRF boundary refinement (+0.92pp). The ImageNet-80 template strategy provides optimal balance between semantic richness and computational overhead. These components combine cumulatively to achieve the final 68.08\% mIoU performance in the incremental build-up experiment.

The system extends to video through SAM2's temporal tracking, enabling zero-shot video editing by processing only the first frame with SCLIP guidance and propagating masks temporally. Integration with Stable Diffusion v2 Inpainting and WAN 2.1 VACE demonstrates text-driven generative editing applications for both images and videos, completing the zero-shot segmentation-to-editing pipeline.

\section{Discussion}

The results situate this work within the broader trajectory of open-vocabulary computer vision. Traditional closed-vocabulary methods achieve higher accuracy on predefined classes (Mask2Former: 89.5\% on PASCAL VOC), but fail completely on unseen categories. The approximate 21 percentage point gap on PASCAL VOC represents the current cost of zero-shot flexibility, though this gap has narrowed from approximately 30 points in earlier work (MaskCLIP: 43.4\%, GroupViT: 52.3\%).

The intelligent prompting strategy demonstrates that semantic guidance from vision-language models can focus segmentation capacity on relevant regions more efficiently than uniform spatial sampling. The training-free nature enables arbitrary vocabulary extension without dataset-specific fine-tuning, distinguishing this approach from methods requiring gradient-based optimization or supervised training on segmentation datasets.

\section{Limitations}

Several systematic failure modes constrain current performance. Small objects below approximately $32 \times 32$ pixels are frequently missed due to SCLIP's 14Ã—14 feature resolution and SAM2's point sampling density. Heavily occluded objects (>50\% occluded) receive incomplete masks covering only visible regions, creating issues for downstream tasks like object removal where complete extent is needed. Performance degrades on images far from CLIP's training distribution, reflecting the web-scraped photographic bias in vision-language pretraining data. Stable Diffusion inpainting struggles with coherent text rendering, perspective consistency, and lighting/shadow generation for complex scenes.

The precision-recall tradeoff (81.13\% precision, 68.45\% recall on PASCAL VOC) indicates bias toward conservative predictions, avoiding false positives at the cost of missing instances. Classes with high shape variability (furniture, deformable objects) suffer from both CLIP recognition errors and SAM2 boundary ambiguity. Ambiguous text prompts without spatial grounding (e.g., "thing on table") fail due to semantic underspecification.

\section{Future Research Directions}

\subsection{3D Scene Understanding}

Extending open-vocabulary segmentation to 3D would enable applications in robotics, AR/VR, and autonomous systems. Potential approaches include lifting 2D masks to 3D using depth estimation or multi-view geometry, integrating with neural radiance fields (NeRFs) for view-consistent editing, and exploring 3D foundation models like LERF \cite{lerf2023} for direct 3D-language alignment. The challenge lies in maintaining open-vocabulary flexibility while addressing 3D geometric consistency and multi-view coherence.

\subsection{Interactive Refinement}

Current inference operates in a single forward pass without user feedback. Interactive systems could support iterative mask refinement through additional text prompts or spatial corrections (brush strokes, bounding boxes), compositional queries that disambiguate multiple candidates (e.g., "the cat that is sleeping, not the one sitting"), and explanations for segmentation decisions. Learning from user corrections could improve future predictions while maintaining zero-shot generalization.

\subsection{Improved Vision-Language Models}

Segmentation quality fundamentally depends on vision-language alignment. Better dense prediction architectures beyond adapted CLIP, larger models trained on more diverse data, and contrastive objectives explicitly optimized for pixel-level correspondence could reduce the gap to supervised methods. Recent multimodal models (GPT-4V, Gemini) demonstrate stronger vision-language understanding, though their applicability to dense prediction tasks requires investigation.

\subsection{Computational Efficiency}

Deployment on resource-constrained devices (mobile, embedded systems) requires substantial efficiency improvements. Model distillation could train smaller models that mimic foundation model behavior, quantization and pruning could reduce memory and compute requirements, and progressive computation strategies could dynamically trade latency for accuracy. The current 2-3 second per-image inference on consumer hardware remains impractical for real-time applications.

\section{Closing Remarks}

This work demonstrates that intelligent prompting strategies can effectively guide segmentation models using semantic information from vision-language models, achieving competitive training-free performance on standard benchmarks. The 68.09\% mIoU on PASCAL VOC 2012 represents incremental progress over recent open-vocabulary methods (ITACLIP: 67.9\%), while the approximately 21 percentage point gap to supervised approaches (Mask2Former: 89.5\%) indicates substantial room for improvement.

The modular architecture enables component upgrades as better foundation models become available, and the zero-shot capability supports arbitrary vocabulary extension without retraining. Future work should address small object detection, domain adaptation, and computational efficiency to enable broader deployment across diverse application domains.
