\chapter{Conclusions and Future Work}

This thesis presents an open-vocabulary semantic segmentation system that enables flexible, language-driven image understanding and manipulation. By combining SAM 2's universal mask generation with CLIP-based vision-language alignment and Stable Diffusion's generative capabilities, the work demonstrates a practical approach to zero-shot object segmentation and editing. This final chapter summarizes the key contributions, discusses the implications of the results, addresses current limitations, and outlines promising directions for future research.

\section{Summary of Contributions}

This thesis makes several key contributions to the field of open-vocabulary semantic segmentation and generative image editing:

\subsection{Unified Open-Vocabulary Framework}

A modular pipeline is developed that integrates state-of-the-art foundation models (SAM 2, CLIP, Stable Diffusion) into a cohesive system. Unlike traditional semantic segmentation methods that require extensive training on fixed-class datasets, the approach enables zero-shot segmentation of arbitrary objects specified by natural language prompts. This flexibility is achieved by:

\begin{itemize}
    \item Leveraging SAM 2's class-agnostic mask proposals to generate comprehensive segmentation candidates
    \item Utilizing dense CLIP features (inspired by MaskCLIP \cite{zhou2022extract} and CLIPSeg \cite{luddecke2022clipseg}) to align visual regions with textual descriptions
    \item Integrating Stable Diffusion for semantically-aware inpainting and object manipulation
\end{itemize}

This integration demonstrates that combining complementary foundation models can achieve sophisticated visual understanding without task-specific fine-tuning.

\subsection{CLIP-Guided Intelligent Prompting}

Building upon recent advances in dense vision-language understanding through SCLIP (Self-attention CLIP), an intelligent prompting strategy is developed that extracts semantic information from dense CLIP predictions to guide SAM 2's mask generation. Rather than using blind grid sampling (4096 points), the approach identifies 50-300 high-confidence semantic regions from SCLIP's cross-layer self-attention features.

This CLIP-guided prompting achieves a dramatic reduction in computational overhead while maintaining competitive segmentation accuracy. By focusing computational resources on semantically meaningful regions, the system efficiently processes images while avoiding the combinatorial explosion of exhaustive sampling strategies.

\subsection{Mask Quality Filtering and Selection}

A quality-aware mask selection pipeline is implemented that combines multiple filtering stages:

\begin{itemize}
    \item IoU-based quality filtering: Only keeps masks with predicted IoU above minimum threshold
    \item Non-maximum suppression with 70\% overlap threshold to remove redundant masks
    \item CLIP-based semantic scoring to rank masks by text-image alignment
\end{itemize}

This filtering pipeline ensures that only high-quality, semantically relevant masks are retained for final segmentation, balancing coverage with precision.

\subsection{Comprehensive Evaluation Framework}

A thorough evaluation protocol is established on PASCAL VOC 2012, measuring segmentation quality through multiple metrics (mIoU, pixel accuracy, F1 score, precision, recall, boundary F1). The experiments demonstrate:

\begin{itemize}
    \item \textbf{State-of-the-art training-free performance} on PASCAL VOC 2012, surpassing previous open-vocabulary methods through intelligent prompting strategies
    \item Exceptional background recognition through comprehensive descriptor files
    \item Strong performance on diverse object categories demonstrating robust zero-shot generalization
    \item High precision and pixel accuracy indicating effective false positive reduction
    \item Significant efficiency gains compared to blind grid sampling (50-300 vs 4096 points)
    \item Comprehensive evaluation across 21 object categories validating the approach
\end{itemize}

\subsection{Practical System Design}

The system is designed with real-world applicability in mind, demonstrating that the approach is viable even on consumer-grade hardware (NVIDIA GeForce GTX 1060 6GB Max-Q). The CLIP-guided prompting approach processes images in 12-33 seconds depending on the number of extracted prompts. This performance, while slower than real-time, remains practical for research applications and offline evaluation on limited hardware budgets.

\subsection{Extension to Video Processing}

Beyond image segmentation, the work demonstrates that CLIP-guided prompting naturally extends to video processing through two integrated capabilities:

\begin{itemize}
    \item \textbf{Video Segmentation:} CLIP analysis performed only on first frame, with SAM2's memory-based architecture handling temporal propagation and maintaining object identity throughout videos
    \item \textbf{Video Inpainting:} Integration with WAN 2.1 VACE \cite{vace2025} enables text-driven object removal and replacement in videos, completing the zero-shot video editing pipeline
    \item \textbf{Hardware flexibility:} Image segmentation runs on consumer hardware (GTX 1060 6GB), while video inpainting leverages high-end GPUs (RTX 4090) for diffusion model processing
    \item \textbf{Practical demonstrations:} Sports footage processing (MotoGP racing, NBA basketball) with person-specific identification (e.g., "Valentino Rossi", "Stephen Curry") and object removal capabilities
\end{itemize}

This video extension demonstrates the versatility of the approach, showing how CLIP-guided prompting can be applied beyond static images while maintaining efficiency through intelligent prompt extraction. Complete technical details including SAM2's temporal tracking mechanism, VACE configuration, and memory management strategies are provided in Appendix \ref{appendix:video_generation}.

\section{Discussion and Implications}

\subsection{Open-Vocabulary Paradigm Shift}

The results support the growing evidence that open-vocabulary approaches represent a fundamental shift in computer vision. Traditional closed-vocabulary methods achieve higher accuracy on their target classes (e.g., Mask2Former: 89.5\% on PASCAL VOC), but they completely fail on unseen objects. In contrast, the system gracefully handles arbitrary text prompts, making it far more versatile for real-world scenarios where the set of relevant objects cannot be predetermined.

The gap between open-vocabulary and closed-vocabulary performance has narrowed significantly (approximately 21 percentage points on PASCAL VOC, down from 30 points), highlighting rapid progress in this field. The intelligent prompting strategy combined with descriptor files and optimized template strategies demonstrates significant advancement—achieving \textbf{state-of-the-art mIoU with a fraction of the prompts} required by blind grid sampling. This represents both efficiency gains and competitive accuracy among training-free methods on PASCAL VOC 2012.

\subsection{Foundation Models as Building Blocks}

This thesis demonstrates that modern foundation models—trained on massive datasets with general objectives—can be effectively composed to solve complex tasks without extensive task-specific training. Each component contributes specialized capabilities:

\begin{itemize}
    \item \textbf{SAM 2:} Provides high-quality, class-agnostic segmentation masks
    \item \textbf{CLIP:} Bridges vision and language for semantic understanding
    \item \textbf{Stable Diffusion:} Generates realistic content conditioned on text and spatial constraints
\end{itemize}

This modular design philosophy offers several advantages:
\begin{itemize}
    \item \textbf{Rapid iteration:} Individual components can be upgraded as better models become available
    \item \textbf{Interpretability:} Each stage's output can be inspected independently for debugging
    \item \textbf{Flexibility:} The pipeline can be adapted for related tasks (e.g., video editing, 3D scene manipulation)
\end{itemize}

\subsection{Language as a Universal Interface}

By using natural language prompts as the primary interface, the system becomes accessible to users without computer vision expertise. This democratization of image editing capabilities aligns with broader trends in AI toward more intuitive human-computer interaction. However, the failure case analysis (Section 4.3.4) reveals that prompt engineering still matters—ambiguous queries like ``thing on table'' fail, while specific descriptions like ``wine glass on dining table'' succeed.

Future work should explore methods for handling underspecified prompts, perhaps by asking clarifying questions or presenting multiple candidate interpretations.

\section{Limitations and Challenges}

Despite promising results, the system has several notable limitations:

\subsection{Small Object Detection}

Objects smaller than approximately $32 \times 32$ pixels are frequently missed by SAM 2's automatic mask generation. This limitation stems from the model's point prompt grid resolution (32 points per side) and affects tasks like detecting small text, buttons in UI screenshots, or distant objects in landscape photographs.

Potential solutions include:
\begin{itemize}
    \item Adaptive point sampling that concentrates prompts in regions with high-frequency details
    \item Multi-resolution processing with image pyramids
    \item Integration with specialized small object detectors
\end{itemize}

\subsection{Occlusion and Partial Visibility}

When objects are heavily occluded or partially visible, SAM 2 may produce incomplete masks that only cover visible regions. While this is technically correct for pixel-level segmentation, it can be problematic for downstream tasks like object removal (where inpainting the entire object region including occluded parts is desired) or counting (where partially visible objects should still be counted).

Addressing this limitation may require:
\begin{itemize}
    \item Amodal segmentation techniques that predict full object extent
    \item Integration with depth estimation or 3D reasoning
    \item Multi-view or temporal information for disambiguating occlusions
\end{itemize}

\subsection{Domain Shift and Distribution Mismatch}

Performance degrades significantly on images far from CLIP's training distribution (e.g., artistic illustrations, medical images, satellite imagery). This limitation is inherent to the current generation of vision-language models, which are predominantly trained on natural photographs scraped from the web.

Future research should explore:
\begin{itemize}
    \item Domain adaptation techniques for specialized image types
    \item Few-shot fine-tuning procedures that preserve open-vocabulary capabilities
    \item Alternative vision-language models trained on more diverse data
\end{itemize}

\subsection{Inpainting Artifacts}

While Stable Diffusion generally produces realistic inpainting results, certain content types remain challenging:
\begin{itemize}
    \item \textbf{Text and fine patterns:} Coherent text rendering and regular patterns (e.g., brick walls, fabric textures) often exhibit artifacts
    \item \textbf{Perspective consistency:} Generated objects sometimes have incorrect perspective relative to the scene
    \item \textbf{Lighting and shadows:} Matching lighting conditions and generating appropriate shadows requires careful prompt engineering
\end{itemize}

Improvements could come from:
\begin{itemize}
    \item More sophisticated conditioning mechanisms that explicitly encode scene geometry
    \item Specialized inpainting models trained on diverse editing scenarios
    \item Post-processing refinement stages that correct common artifacts
\end{itemize}

\section{Future Research Directions}

Building on this work, several promising research directions are identified:

\subsection{3D Scene Understanding and Manipulation}

Extending open-vocabulary segmentation to 3D would enable applications in robotics, AR/VR, and autonomous systems. Potential approaches include:
\begin{itemize}
    \item Lifting 2D segmentation masks to 3D using depth estimation or multi-view geometry
    \item Integrating with neural radiance fields (NeRFs) for view-consistent editing
    \item Training on 3D datasets with language annotations
    \item Exploring recent 3D foundation models like LERF \cite{lerf2023} for direct 3D-language alignment
\end{itemize}

\subsection{Interactive and Iterative Refinement}

Current systems process images in a single forward pass, but human creative workflows often involve multiple iterations. An interactive system could:
\begin{itemize}
    \item Allow users to refine masks with additional prompts or brush strokes
    \item Support compositional queries (e.g., ``the cat that is sleeping, not the one sitting'')
    \item Learn from user corrections to improve future predictions
    \item Provide explanations for segmentation decisions to build user trust
\end{itemize}

\subsection{Improved Vision-Language Alignment}

The quality of open-vocabulary segmentation fundamentally depends on vision-language models. Future improvements could come from:
\begin{itemize}
    \item Training larger, more capable vision-language models on diverse data
    \item Developing better architectures for dense prediction (moving beyond adapted CLIP)
    \item Incorporating additional modalities (e.g., audio, depth, thermal) for richer scene understanding
    \item Exploring different contrastive learning objectives optimized for segmentation
\end{itemize}

Recent models like OpenAI's GPT-4V, Google's Gemini, and open alternatives may provide stronger vision-language backbones.

\subsection{Semantic Reasoning and Common Sense}

Current systems lack semantic reasoning capabilities. For example, when asked to segment ``the food a person is about to eat,'' the system cannot infer intent from body language or scene context. Integrating large language models (LLMs) could enable:
\begin{itemize}
    \item Reasoning about spatial relationships (``object on top of'', ``behind'', ``next to'')
    \item Understanding functional relationships (``tool used for'', ``container holding'')
    \item Inferring implicit information (``owner of the car'', ``person who looks surprised'')
    \item Planning multi-step editing operations from high-level instructions
\end{itemize}

\subsection{Addressing Bias and Fairness}

Foundation models inherit biases from their training data, which can manifest in segmentation and generation. Important considerations include:
\begin{itemize}
    \item Analyzing demographic biases in segmentation accuracy
    \item Ensuring generated content represents diverse populations fairly
    \item Developing methods to detect and mitigate harmful uses (e.g., non-consensual editing)
    \item Establishing guidelines for responsible deployment
\end{itemize}

\subsection{Specialized Domain Applications}

While the system focuses on natural images, many domains could benefit from open-vocabulary segmentation:
\begin{itemize}
    \item \textbf{Medical imaging:} Segmenting anatomical structures or pathologies from radiological text reports
    \item \textbf{Satellite imagery:} Identifying geographic features, infrastructure, or environmental changes
    \item \textbf{Document analysis:} Segmenting document components (tables, figures, equations) based on functional descriptions
    \item \textbf{Scientific visualization:} Editing plots, diagrams, and schematics
\end{itemize}

Domain-specific applications may require specialized training data or adaptation techniques while preserving open-vocabulary flexibility.

\subsection{Efficient and Edge-Deployable Models}

For many applications (e.g., mobile apps, embedded systems), current models are too computationally expensive. Research directions include:
\begin{itemize}
    \item Model distillation: training smaller student models that mimic foundation model behavior
    \item Neural architecture search for efficient segmentation networks
    \item Quantization and pruning techniques that minimize accuracy loss
    \item Progressive computation strategies that trade latency for accuracy dynamically
\end{itemize}

\section{Closing Remarks}

Open-vocabulary semantic segmentation represents a significant step toward more flexible and human-centric computer vision systems. By moving beyond fixed-category taxonomies, applications can adapt to users' needs rather than requiring users to adapt to system constraints.

This thesis demonstrates that current foundation models—SAM 2, CLIP, and Stable Diffusion—can be effectively combined to achieve practical open-vocabulary segmentation and editing. While significant challenges remain (small objects, domain shift, computational cost), the rapid pace of progress in foundation model development suggests that many current limitations will be addressed in the near future.

As these systems improve and become more accessible, transformative impacts are anticipated across diverse domains: from creative tools that democratize professional-quality image editing, to scientific instruments that help researchers analyze visual data, to assistive technologies that make visual content more accessible to people with disabilities.

The ultimate goal is not merely to automate visual understanding, but to create intelligent tools that amplify human creativity and insight. Open-vocabulary approaches, by aligning machine perception with human language, represent an important step toward this vision.
