\chapter{Methodology}
\label{ch:methodology}

This chapter details the methodology employed to develop the proposed open-vocabulary semantic segmentation system. Unlike traditional approaches that rely on closed-vocabulary classifiers, this work leverages recent advances in vision-language models and promptable segmentation to enable flexible, language-driven image understanding and manipulation.

The core contribution is a CLIP-guided prompting approach that combines SCLIP's dense predictions with SAM2's segmentation quality through intelligent prompt extraction. This fully annotation-free method extracts semantic prompt points from CLIP's high-confidence regions, using SAM2 to generate high-quality masks with direct class assignment.

Building upon insights from SCLIP \cite{sclip2024}, MaskCLIP \cite{zhou2022extract}, CLIPSeg \cite{luddecke2022clipseg}, and SAM 2 \cite{ravi2024sam2}, this chapter presents the CLIP-guided prompting methodology in detail.

\section{CLIP-Guided Prompting Approach}
\label{sec:clip_guided_approach}

The main contribution is a CLIP-guided prompting system that leverages SCLIP's Cross-layer Self-Attention mechanism to extract intelligent prompt points for SAM2 segmentation. This approach achieves fully annotation-free open-vocabulary segmentation by combining semantic understanding with high-quality boundary delineation through efficient, semantically-guided prompting.

\subsection{Motivation and Design Philosophy}

CLIP-guided prompting offers several key advantages for open-vocabulary segmentation:

\begin{itemize}
    \item \textbf{Efficient semantic guidance:} Instead of blindly prompting SAM2 at thousands of points (e.g., 4096 in a grid), CLIP identifies a sparse set of semantically meaningful locations (typically 50-300), drastically reducing computational cost while maintaining competitive accuracy.

    \item \textbf{Zero spatial user input:} Users provide only a text vocabulary; the system automatically determines where objects are located without requiring manual clicks or bounding boxes.

    \item \textbf{High-quality segmentation:} SAM2 provides precise object boundaries at each semantically-guided prompt point, combining CLIP's semantic understanding with SAM2's superior mask quality.

    \item \textbf{Training-free operation:} Uses frozen CLIP and SAM2 models without any fine-tuning, enabling zero-shot transfer to new domains.
\end{itemize}

The key insight is to use CLIP's dense predictions not as the final output, but as intelligent guidance for SAM2 prompting, achieving both efficiency and quality.

\subsection{System Overview}

The CLIP-guided prompting pipeline consists of four main stages:

\begin{enumerate}
    \item \textbf{Dense Semantic Prediction:} SCLIP extracts dense features using Cross-layer Self-Attention (CSA) and produces pixel-wise class predictions through similarity matching with text embeddings.

    \item \textbf{Intelligent Prompt Extraction:} Extract 50-300 representative points from high-confidence regions in SCLIP's predictions using connected component analysis and centroid computation.

    \item \textbf{SAM2 Segmentation:} Prompt SAM2 at each extracted point to generate high-quality instance masks with direct class assignment based on CLIP predictions.

    \item \textbf{Overlap Resolution:} Resolve overlapping masks through IoU-based filtering and confidence-based priority assignment.
\end{enumerate}

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.5\textwidth,keepaspectratio]{Imagenes/2. CLIP-Guided Prompting Pipeline Diagram.png}
\caption{Overview of the CLIP-guided prompting pipeline. The system uses CLIP's dense predictions to extract intelligent prompt points, achieving a massive reduction in prompts compared to blind grid sampling while maintaining competitive accuracy.}
\label{fig:clip_guided_pipeline}
\end{figure}

The key insight motivating this design is that CLIP's dense predictions identify where objects are located, enabling intelligent SAM2 prompting with far fewer points than blind grid sampling. By extracting points from SCLIP's high-confidence regions, SAM2 is guided to focus on semantically relevant areas, achieving both efficiency and accuracy.

\subsection{Technical Background}
\label{sec:technical_background}

The approach builds on two foundation models: \textbf{CLIP} \cite{radford2021learning} for vision-language alignment and \textbf{Vision Transformers (ViT)} \cite{dosovitskiy2020image} for dense feature extraction. 

\textbf{CLIP} learns a shared embedding space between images and text, enabling zero-shot recognition of unseen objects. \textbf{ViT} processes images as sequences of patches, capturing global context through self-attention.

For a detailed technical explanation of these architectures, including the mathematical formulation of self-attention and contrastive learning, please refer to \textbf{Appendix \ref{appendix:technical_foundations}}. This chapter focuses on their adaptation for the proposed open-vocabulary segmentation pipeline.

\subsection{SCLIP Dense Prediction with Cross-Layer Self-Attention}
\label{sec:sclip_dense_extraction}

\textit{Note: SCLIP \cite{sclip2024} is prior work adopted for dense feature extraction. \textbf{This thesis's contribution} is using these features for intelligent SAM2 prompting (Section \ref{sec:intelligent_prompting}) and enhancing them with descriptor files and template strategies (Sections \ref{sec:descriptor_files}, \ref{sec:template_strategies}).}

\subsubsection{Cross-Layer Self-Attention: The Key Modification}

SCLIP's main innovation is modifying CLIP's final attention layer (layer 12 of ViT-B/16) to use \textbf{Cross-layer Self-Attention (CSA)} instead of standard attention:

\textbf{Standard attention} (layers 1-11):
\begin{equation}
A_h^{\text{standard}} = \text{softmax}\left(\frac{Q_h K_h^T}{\sqrt{d_h}}\right)
\end{equation}

\textbf{Cross-layer Self-Attention} (layer 12):
\begin{equation}
\boxed{A_h^{\text{CSA}} = \text{softmax}\left(\frac{Q_h Q_h^T + K_h K_h^T}{\sqrt{d_h}}\right)}
\end{equation}

\textbf{Why this improves dense prediction:} The $Q_h Q_h^T$ and $K_h K_h^T$ terms measure patch-to-patch self-similarity, encouraging spatial grouping. Patches with similar queries or keys (e.g., all "sky" patches) mutually reinforce each other, producing spatially coherent features. Standard $Q_h K_h^T$ attention works well for classification but creates fragmented dense predictions.

\subsubsection{Dense Prediction Pipeline}

The complete SCLIP pipeline (see Figure \ref{fig:clip_guided_pipeline} for overview):

\textbf{1. Image encoding:} ViT-B/16 processes 224×224 images into 14×14 patch features ($F \in \mathbb{R}^{14 \times 14 \times 512}$)

\textbf{2. Text encoding:} For each class, encode text descriptors with templates (detailed in Sections \ref{sec:descriptor_files}, \ref{sec:template_strategies}) to get embeddings $e_c \in \mathbb{R}^{512}$

\textbf{3. Similarity computation:} Calculate dense cosine similarity between image patches and text embeddings at the 14×14 feature resolution:
\begin{equation}
S_{i,j,c} = F[i,j,:] \cdot e_c \in [-1, 1], \quad S \in \mathbb{R}^{14 \times 14 \times C}
\end{equation}
where $(i,j) \in [0,13] \times [0,13]$ indexes the 14×14 patch grid.

\textbf{4. Dense predictions:} Bilinear upsampling maps the 14×14 similarity tensor to full image resolution $H \times W$ (typically 224×224), yielding $S' \in \mathbb{R}^{H \times W \times C}$. We then apply softmax with temperature scaling ($T=40$) at each pixel location $(x,y)$:
\begin{equation}
P_{x,y,c} = \frac{\exp(T \cdot S'_{x,y,c})}{\sum_{c'} \exp(T \cdot S'_{x,y,c'})}, \quad \hat{c}(x,y) = \arg\max_c P_{x,y,c}
\end{equation}
where $(x,y) \in [0, H-1] \times [0, W-1]$ indexes pixel coordinates in the upsampled resolution.

This produces pixel-wise class predictions $\hat{c} \in \mathbb{R}^{H \times W}$ and confidence maps $P \in \mathbb{R}^{H \times W \times C}$ that feed into the intelligent prompting strategy.

\subsubsection{Descriptor Files: Multi-Term Class Representations}
\label{sec:descriptor_files}

This section describes the enhancement to SCLIP through descriptor files.

In practice, using a single class name (e.g., "person") limits CLIP's ability to recognize intra-class variations. \textbf{Descriptor files} address this limitation by providing multiple descriptive terms for each class, enabling CLIP to match different visual manifestations of the same semantic category.

\paragraph{Motivation and Design}

Standard approaches encode each class with a single term: ["person", "car", "dog"]. However, objects exhibit significant visual diversity:
\begin{itemize}
    \item \textbf{People} appear in various clothing (shirts, jeans, dresses)
    \item \textbf{Backgrounds} comprise multiple stuff classes (sky, wall, grass, road)
    \item \textbf{Compound objects} have material-specific variants (brick wall, wooden floor)
\end{itemize}

Descriptor files map each class index to a comma-separated list of descriptive terms, allowing CLIP to recognize these variations. This simple yet effective technique significantly improves segmentation quality without requiring retraining.

Our PASCAL VOC descriptor file provides rich descriptions for all 21 classes. For example, the background class includes terms like "sky", "wall", "tree", and "road", while the person class includes clothing variations. Full details of the descriptor file content are provided in Appendix \ref{sec:appendix_descriptors}.

Synonym expansion (diningtable → "table") and multi-word variants (tvmonitor → "television monitor", "tv monitor") help CLIP understand Pascal VOC's specific naming conventions.

\paragraph{Text Encoding with Descriptors}

When using descriptor files, Step 5 (Text Encoding) is modified:

\textbf{For each class $c$:}
\begin{enumerate}
    \item Load descriptor terms: $D_c = \{\text{term}_1, \text{term}_2, \ldots, \text{term}_{K_c}\}$
    \item For each term $d \in D_c$ and template $t_i$:
    \begin{equation}
    e_{c,d,i} = \text{CLIP}_{\text{text}}(t_i(d)) \in \mathbb{R}^{D}
    \end{equation}
    \item Average across all terms and templates:
    \begin{equation}
    e_c = \frac{1}{K_c \cdot M} \sum_{d \in D_c} \sum_{i=1}^{M} e_{c,d,i}
    \end{equation}
    \item L2 normalize: $e_c \leftarrow e_c / \|e_c\|_2$
\end{enumerate}

This produces a richer class embedding that captures multiple visual manifestations. For "person" with 7 descriptors and 80 templates, 560 text embeddings are averaged into a single robust class representation.

\paragraph{Impact on Performance}

Descriptor files contribute significantly to overall performance on PASCAL VOC:
\begin{itemize}
    \item \textbf{Background}: 86.90\% IoU - comprehensive coverage of stuff classes
    \item \textbf{Person}: 56.70\% IoU - improved through clothing-aware variants
    \item \textbf{TVmonitor}: 52.64\% IoU - benefits from synonym expansion
    \item \textbf{Overall}: Substantial improvement in segmentation quality
\end{itemize}

\subsubsection{Template Optimization Strategies}
\label{sec:template_strategies}

This section describes the evaluation and selection of template strategies.

While descriptor files address \textit{what} terms to encode, template strategies determine \textit{how} to frame those terms for CLIP. Standard approaches use 80 ImageNet templates \cite{zhou2022learning}, but recent research \cite{huang2024pixelclip, wysoczanska2024clipdiy} shows that carefully curated templates tailored for dense prediction can achieve better accuracy with 10× fewer templates.

\paragraph{Template Strategy Taxonomy}

We implement several template strategies, selectable via the system configuration:

\textbf{1. ImageNet-80 (Baseline)}
\begin{itemize}
    \item 80 diverse templates designed for ImageNet classification
    \item Examples: "a photo of a \{class\}", "a rendering of a \{class\}", "art of a \{class\}"
    \item Pros: Well-tested, comprehensive coverage
    \item Cons: Slow (80× text encoding cost), many templates irrelevant for segmentation
\end{itemize}

\textbf{2. Top-7 Dense Prediction} (PixelCLIP \cite{huang2024pixelclip}, recommended)
\begin{itemize}
    \item 7 templates selected via forward selection on segmentation tasks
    \item Examples: "a photo of a \{class\}", "a \{class\} in the scene", "the \{class\}"
    \item Pros: 11× faster than ImageNet-80, comparable accuracy
    \item Rationale: Spatial context ("in the scene") critical for dense prediction
\end{itemize}

\textbf{3. Spatial Context Templates} (MaskCLIP/DenseCLIP inspiration)
\begin{itemize}
    \item 7 templates emphasizing spatial/scene context
    \item Examples: "\{class\} in the scene", "segment the \{class\}", "there is a \{class\} in the scene"
    \item Pros: Explicitly guides CLIP toward localization, improved spatial understanding
    \item Use case: When spatial understanding is critical (indoor scenes, crowded images)
\end{itemize}

\textbf{4. Top-3 Ultra-Fast}
\begin{itemize}
    \item 3 most effective templates for minimal latency
    \item Examples: "a photo of a \{class\}", "a \{class\} in the scene", "the \{class\}"
    \item Pros: 27× faster than ImageNet-80, minimal accuracy loss
    \item Use case: Real-time applications, resource-constrained environments
\end{itemize}

\textbf{5. Adaptive (Stuff vs. Thing)}
\begin{itemize}
    \item Automatically selects template set based on class type
    \item Stuff classes (sky, grass, road): Emphasize continuity, use mass nouns
    \item Thing classes (car, person, cat): Emphasize countability, use articles
    \item Pros: Class-aware optimization as demonstrated by CLIP-DIY \cite{wysoczanska2024clipdiy}
    \item Example: "the sky" (stuff) vs. "a car" (thing)
\end{itemize}

\paragraph{Implementation Details}

Template selection is configured via Python code. The specific implementation of the recommended Top-7 strategy is detailed in Appendix \ref{sec:appendix_templates}.

The \textbf{ImageNet-80} strategy provides maximum accuracy by trading inference speed for comprehensive template coverage. For faster inference with minimal accuracy loss, Top-7 is recommended.

\paragraph{Performance Comparison}

Ablation study on PASCAL VOC 2012 (100 samples):
\begin{table}[h]
\centering
\begin{tabular}{lccc}
\hline
\textbf{Strategy} & \textbf{Templates} & \textbf{Speedup} & \textbf{mIoU (\%)} \\
\hline
ImageNet-80 & 80 & 1× (baseline) & \textbf{68.09} \\
Top-7 Dense & 7 & 11× & 67.2 \\
Spatial Context & 7 & 11× & 67.5 \\
Top-3 Fast & 3 & 27× & 66.8 \\
Adaptive & 5-7 & 11-16× & 67.8 \\
\hline
\end{tabular}
\caption{Template strategy comparison. ImageNet-80 achieves best accuracy; Top-7 offers best speed/accuracy tradeoff.}
\label{tab:template_strategies}
\end{table}


\subsection{Intelligent Prompt Extraction}
\label{sec:intelligent_prompting}

This is the core algorithmic contribution: using SCLIP's dense predictions to extract intelligent prompt points for SAM2, achieving substantial efficiency gains while outperforming recent training-free methods on PASCAL VOC 2012.

The intelligent prompt extraction uses SCLIP's semantic understanding to guide SAM2 prompting, achieving massive efficiency gains without sacrificing accuracy. This section details the complete pipeline from SCLIP's dense predictions to semantically meaningful prompt points.

\subsubsection{Motivation: From Dense Predictions to Sparse Prompts}

Naive approaches to combining CLIP with SAM often use exhaustive grid sampling: prompting SAM at every point in a dense grid (e.g., 64×64 = 4096 points) and using CLIP to classify the resulting masks. This has two major drawbacks:

\begin{itemize}
    \item \textbf{Computational inefficiency:} Generating 4096 SAM masks per image is prohibitively expensive (minutes per image even on modern GPUs)
    \item \textbf{Semantic blindness:} Grid points are placed uniformly without considering object locations, wasting computation on background regions
\end{itemize}

The intelligent extraction inverts this paradigm: SCLIP first identifies \textit{where} objects are likely to exist, then SAM2 is prompted only at those semantically meaningful locations. This achieves a significant reduction in prompts (from thousands to hundreds) while maintaining competitive accuracy.

\subsubsection{Complete Prompt Extraction Pipeline}

Given SCLIP's dense prediction outputs from the previous stage:
\begin{itemize}
    \item Segmentation map: $\hat{c} \in \mathbb{R}^{H \times W}$ (predicted class per pixel)
    \item Probability map: $P \in \mathbb{R}^{H \times W \times C}$ (confidence scores for $C$ classes)
\end{itemize}

We extract semantically meaningful prompt points through the following five-stage pipeline:

\paragraph{Stage 1: Per-Class Confidence Masking}

For each class $c \in \{1, \ldots, C\}$, a binary confidence mask is created identifying high-certainty regions:

\begin{equation}
M_c^{\text{conf}}(x,y) = \begin{cases}
1 & \text{if } \hat{c}(x,y) = c \text{ and } P_{x,y,c} > \tau_{\text{conf}} \\
0 & \text{otherwise}
\end{cases}
\end{equation}

where $\tau_{\text{conf}}$ is the confidence threshold (typically 0.7). This filters out low-confidence predictions that may be noise or ambiguous regions. The threshold is deliberately set high to ensure SAM2 prompts are placed only at locations where CLIP has strong semantic evidence.

\paragraph{Stage 2: Connected Component Analysis}

For each class's confidence mask $M_c^{\text{conf}}$, connected component labeling is applied to identify spatially disjoint object instances. This clustering step groups adjacent pixels belonging to the same semantic class into discrete regions:

\begin{equation}
\text{CC}(M_c^{\text{conf}}) \rightarrow \{R_c^1, R_c^2, \ldots, R_c^{N_c}\}
\end{equation}

where $R_c^i \subseteq \{(x,y) : M_c^{\text{conf}}(x,y) = 1\}$ represents the $i$-th connected region for class $c$.

We use 8-connectivity (considering diagonal neighbors) to ensure that objects touching at corners are grouped together. This connected component analysis efficiently identifies disjoint regions using standard graph-based labeling algorithms.

\textbf{Why clustering matters:} Without clustering, all pixels of class "car" would be treated as a single object. Clustering separates individual car instances, allowing SAM2 to generate distinct masks for each vehicle. For example, a street scene with 5 cars produces 5 separate connected components, each receiving its own prompt point.

\paragraph{Stage 3: Region Filtering by Size}

Many connected components are spurious detections (noise, texture patterns, shadows) that happen to exceed the confidence threshold locally. These are filtered using a minimum region size criterion:

\begin{equation}
\text{Keep } R_c^i \text{ if } |R_c^i| > \tau_{\text{area}}
\end{equation}

where $|R_c^i|$ denotes the number of pixels in region $R_c^i$, and $\tau_{\text{area}}$ is the minimum area threshold (typically 100 pixels).

This size filtering serves multiple purposes:
\begin{itemize}
    \item Removes noise and artifacts from SCLIP predictions
    \item Focuses computation on salient objects rather than tiny regions
    \item Prevents SAM2 from being distracted by spurious high-confidence pixels
\end{itemize}

For a 224×224 image (50,176 pixels), a threshold of 100 pixels means regions smaller than 0.2\% of the image area are ignored, which is reasonable for most semantic segmentation tasks.

\paragraph{Stage 4: Centroid Computation}

For each valid connected component $R_c^i$, a representative prompt point is extracted by computing the spatial centroid:

\begin{equation}
\mathbf{p}_c^i = (\bar{x}_c^i, \bar{y}_c^i) = \left( \frac{1}{|R_c^i|} \sum_{(x,y) \in R_c^i} x, \frac{1}{|R_c^i|} \sum_{(x,y) \in R_c^i} y \right)
\end{equation}

The centroid represents the geometric center of the region and typically falls near the object's interior, making it an ideal point prompt for SAM2. Unlike random sampling or boundary-based selection, centroids are:
\begin{itemize}
    \item \textbf{Stable:} Robust to minor segmentation boundary variations
    \item \textbf{Interior:} Likely to fall inside the object rather than on edges
    \item \textbf{Representative:} Capture the region's spatial extent
\end{itemize}

We round centroid coordinates to the nearest integer pixel location: $\mathbf{p}_c^i = (\text{round}(\bar{x}_c^i), \text{round}(\bar{y}_c^i))$.

\paragraph{Stage 5: Prompt Metadata Aggregation}

For each extracted prompt point $\mathbf{p}_c^i$, metadata is aggregated to inform downstream processing:

\begin{equation}
\text{Prompt}_c^i = \{
\mathbf{p}_c^i, \quad c, \quad \text{conf}_c^i, \quad |R_c^i|
\}
\end{equation}

where:
\begin{itemize}
    \item $\mathbf{p}_c^i \in \mathbb{R}^2$: Centroid coordinates $(x, y)$
    \item $c \in \{1, \ldots, C\}$: Class index (e.g., 1="car", 2="person")
    \item $\text{conf}_c^i = P_{\bar{x}_c^i, \bar{y}_c^i, c}$: CLIP confidence at centroid
    \item $|R_c^i| \in \mathbb{N}$: Region area in pixels
\end{itemize}

This metadata serves multiple purposes:
\begin{itemize}
    \item Class index $c$ enables direct class assignment to SAM2 masks (explained in next section)
    \item Confidence score allows prioritizing high-certainty prompts during processing
    \item Region size can be used for overlap resolution (larger regions take precedence)
\end{itemize}

\paragraph{Visual Illustration of the 5-Stage Pipeline}

The complete intelligent prompt extraction process is illustrated in Figure \ref{fig:prompt_extraction_process}, which shows how the pipeline progressively refines CLIP's dense predictions into sparse semantic prompts. Figure \ref{fig:real_prompt_extraction} demonstrates the same process on a real PASCAL VOC image, highlighting the dramatic reduction from 4096 grid points to just 6 semantic prompts while maintaining class accuracy.

The pipeline transforms dense CLIP confidence maps (Stage 0) through binary thresholding (Stage 1), connected component analysis (Stage 2), size filtering (Stage 3), and centroid extraction (Stages 4-5) to produce the final set of intelligent prompts for SAM2. This approach achieves 96-99\% prompt reduction compared to exhaustive grid sampling while focusing computational resources on semantically meaningful regions.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{Imagenes/prompt_extraction_pipeline.png}
\caption{Intelligent prompt extraction pipeline showing the 5-stage process: CLIP confidence map → thresholding → connected components → size filtering → centroid computation. This synthetic example demonstrates 96\% prompt reduction (4 prompts vs 4096 grid points).}
\label{fig:prompt_extraction_process}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{Imagenes/real_prompt_extraction_example.png}
\caption{Real PASCAL VOC example showing 6 semantic prompts extracted from an airport scene with aeroplane and person, achieving 99.85\% reduction vs blind grid sampling (6 vs 4096 points, 683× speedup).}
\label{fig:real_prompt_extraction}
\end{figure}

\subsubsection{Algorithm Summary and Efficiency Analysis}

The complete prompt extraction algorithm processes all classes in parallel:

\begin{equation}
\text{Prompts} = \bigcup_{c=1}^{C} \left\{ \text{Prompt}_c^i : R_c^i \text{ valid after filtering} \right\}
\end{equation}

Algorithm~\ref{alg:intelligent_prompting} provides the complete pseudocode for the intelligent prompt extraction process.

\begin{algorithm}[H]
\caption{Intelligent Prompt Extraction from SCLIP Predictions}
\label{alg:intelligent_prompting}
\begin{algorithmic}[1]
\State \textbf{Input:} Segmentation map $\hat{c} \in \mathbb{R}^{H \times W}$, probability map $P \in \mathbb{R}^{H \times W \times C}$
\State \textbf{Parameters:} Confidence threshold $\tau_{\text{conf}} = 0.7$, minimum area $\tau_{\text{area}} = 100$
\State \textbf{Output:} Prompt set $\mathcal{P} = \{(\mathbf{p}_i, c_i, \text{conf}_i, \text{area}_i)\}$
\State
\State $\mathcal{P} \gets \emptyset$ \Comment{Initialize empty prompt set}
\State
\For{each class $c = 1$ to $C$}
    \State \textbf{// Stage 1: Confidence Masking}
    \State $M_c^{\text{conf}}(x,y) \gets \mathbb{1}[\hat{c}(x,y) = c \text{ and } P_{x,y,c} > \tau_{\text{conf}}]$
    \State
    \State \textbf{// Stage 2: Connected Component Analysis}
    \State $\{R_c^1, R_c^2, \ldots, R_c^{N_c}\} \gets \text{ConnectedComponents}(M_c^{\text{conf}})$
    \State
    \State \textbf{// Stage 3-5: Region Filtering, Centroid Extraction, Metadata}
    \For{each region $R_c^i$ in $\{R_c^1, \ldots, R_c^{N_c}\}$}
        \If{$|R_c^i| > \tau_{\text{area}}$} \Comment{Size filtering}
            \State $\bar{x} \gets \frac{1}{|R_c^i|} \sum_{(x,y) \in R_c^i} x$ \Comment{Centroid x}
            \State $\bar{y} \gets \frac{1}{|R_c^i|} \sum_{(x,y) \in R_c^i} y$ \Comment{Centroid y}
            \State $\mathbf{p}_c^i \gets (\text{round}(\bar{x}), \text{round}(\bar{y}))$
            \State $\text{conf}_c^i \gets P_{\bar{x}, \bar{y}, c}$ \Comment{Confidence at centroid}
            \State $\mathcal{P} \gets \mathcal{P} \cup \{(\mathbf{p}_c^i, c, \text{conf}_c^i, |R_c^i|)\}$
        \EndIf
    \EndFor
\EndFor
\State
\State \Return $\mathcal{P}$ \Comment{Typically 50-300 prompts}
\end{algorithmic}
\end{algorithm}

\textbf{Typical prompt counts:} For Pascal VOC 2012 images with 21 classes (20 objects + background), the extraction produces:
\begin{itemize}
    \item \textbf{Simple scenes:} 50-100 prompts (sparse objects, large background regions)
    \item \textbf{Complex scenes:} 200-300 prompts (multiple object instances per class)
    \item \textbf{Average:} $\sim$150 prompts per image
\end{itemize}

\textbf{Efficiency gains:} Compared to 64×64 grid sampling (4096 prompts):
\begin{equation}
\text{Reduction} = 1 - \frac{150}{4096} = 96.3\%
\end{equation}

This 27× reduction in SAM2 queries translates to 3-4× end-to-end speedup in practice (accounting for fixed costs like image encoding), enabling practical deployment on consumer hardware.

\subsubsection{Hyperparameter Selection}

Two key hyperparameters control the prompt extraction process:

\begin{itemize}
    \item \textbf{Confidence threshold $\tau_{\text{conf}}$:} Higher values (0.7-0.9) produce fewer but higher-quality prompts by requiring strong CLIP evidence. Lower values (0.3-0.5) increase recall at the cost of more false positives. The default of 0.7 balances precision and recall.

    \item \textbf{Minimum area $\tau_{\text{area}}$:} Larger values (200-500) filter more aggressively, reducing noise but potentially missing small objects. Smaller values (50-100) retain more detections but include spurious regions. The default of 100 pixels is appropriate for objects of interest in most semantic segmentation benchmarks.
\end{itemize}

These thresholds can be adjusted based on application requirements: surveillance systems may prefer high recall (lower thresholds), while precision-critical applications (medical imaging) may demand higher thresholds.

\subsubsection{Direct Class Assignment}

Unlike methods that require voting or post-processing, the approach performs direct class assignment:

For each prompt point $p_i$ with extracted class label $c_i$:

\begin{equation}
\text{SAM2}(p_i) \rightarrow M_i \text{ where } M_i \text{ is assigned class } c_i
\end{equation}

SAM2 generates a high-quality instance mask $M_i$ at prompt $p_i$, and the class $c_i$ determined by SCLIP's prediction at that location is directly assigned. This simple yet effective strategy combines:

\begin{itemize}
    \item \textbf{SCLIP's semantic understanding:} Correct class assignment from vision-language features
    \item \textbf{SAM 2's segmentation quality:} Precise object boundaries from specialized segmentation model
    \item \textbf{Efficient processing:} No need for majority voting or multi-scale evaluation
\end{itemize}

\begin{figure}[h]
\centering
\includegraphics[width=0.95\textwidth]{Imagenes/3. Comparison between SCLIP dense prediction baseline and the proposed CLIP-guided prompting approach.png}
\caption{Comparison between SCLIP dense prediction baseline and the proposed CLIP-guided prompting approach. Four representative examples demonstrate key advantages: superior boundary quality through SAM2 integration, improved small object detection, and cleaner instance separation in cluttered scenes.}
\label{fig:comparison_baseline}
\end{figure}

\subsection{Computational Optimizations}
\label{sec:computational_optimizations}

Beyond the algorithmic improvements (descriptor files, template strategies, intelligent prompting), several low-level computational optimizations significantly improve inference speed and memory efficiency without affecting accuracy. These optimizations are critical for practical deployment, especially on resource-constrained hardware.

\subsubsection{Computational Complexity Analysis}

Before discussing implementation optimizations, we analyze the computational complexity of the CLIP-guided prompting approach compared to baseline methods.

For an image of resolution $H \times W$ with $C$ classes and CLIP embedding dimension $D = 512$:

\textbf{SCLIP dense prediction:} $\mathcal{O}(H \times W \times C \times D)$ for computing similarity between all pixels and all class embeddings.

\textbf{Intelligent prompt extraction:} $\mathcal{O}(H \times W)$ for connected component analysis and centroid computation, dominated by the linear scan through confidence masks.

\textbf{SAM2 segmentation:} $\mathcal{O}(N_{\text{prompts}} \times T_{\text{SAM2}})$ where $N_{\text{prompts}} \in [50, 300]$ is the number of extracted prompts and $T_{\text{SAM2}}$ is the per-prompt processing time.

\textbf{Baseline grid sampling:} $\mathcal{O}(N_{\text{grid}} \times T_{\text{SAM2}})$ where $N_{\text{grid}} = 4096$ for a $64 \times 64$ grid.

\textbf{Theoretical speedup:} $\frac{N_{\text{grid}}}{N_{\text{prompts}}} = \frac{4096}{50\text{-}300} \approx 13\text{-}80\times$

In practice, the speedup is lower due to fixed costs (image encoding, feature extraction), but the intelligent prompting approach consistently achieves $3\text{-}4\times$ end-to-end speedup on PASCAL VOC 2012 images.

\subsubsection{Mixed Precision Inference (FP16)}

Modern GPUs achieve substantially higher throughput when operating on 16-bit floating-point (FP16) data compared to standard 32-bit (FP32). The implementation leverages PyTorch's automatic mixed precision (AMP) to accelerate inference while preserving numerical stability. Code details are available in Appendix \ref{sec:appendix_optimizations}.

The \texttt{autocast} context automatically identifies operations that benefit from FP16 (e.g., matrix multiplications, convolutions) while keeping precision-sensitive operations (e.g., softmax, normalization) in FP32. This hybrid approach provides approximately $1.5\text{-}2\times$ faster inference on modern GPUs (Ampere, Ada Lovelace architectures), 50\% reduction in activation memory, and no measurable impact on segmentation quality.

\subsubsection{Just-In-Time Compilation}

PyTorch 2.0+ introduces \texttt{torch.compile()}, which fuses operations, eliminates Python overhead, and generates optimized CUDA kernels. The implementation applies compilation to both the CLIP vision encoder and text encoder using \texttt{mode="reduce-overhead"}. See Appendix \ref{sec:appendix_optimizations} for details.

While compilation incurs a one-time overhead (10-20 seconds), subsequent inferences benefit from kernel fusion, memory optimization, and graph-level optimization, providing an additional $1.2\text{-}1.5\times$ improvement on top of FP16 optimizations.

\subsubsection{Batched SAM2 Prompting}

Standard SAM2 usage prompts one point at a time, requiring separate forward passes for each prompt. With 50-300 prompts per image, this creates significant overhead. The implementation processes multiple points simultaneously in batches (default size 32), exploiting GPU parallelism to achieve $2\text{-}4\times$ speedup over sequential prompting. Implementation details in Appendix \ref{sec:appendix_optimizations}.

\subsubsection{Combined Impact}

\begin{table}[!htbp]
\centering
\begin{tabular}{lcc}
\hline
\textbf{Configuration} & \textbf{Time/Image (s)} & \textbf{Relative Speedup} \\
\hline
Baseline (FP32, no compile, sequential) & 45-60 & 1.0× \\
+ FP16 & 25-35 & 1.5-1.8× \\
+ FP16 + torch.compile & 18-28 & 2.1-2.5× \\
+ FP16 + torch.compile + batching & 12-20 & 3.0-3.8× \\
\hline
\end{tabular}
\caption{Cumulative impact of computational optimizations on inference time.}
\label{tab:computational_optimizations}
\end{table}

These optimizations enable practical deployment on consumer hardware (e.g., NVIDIA GeForce GTX 1060 6GB) where the best configuration achieves 12-33 seconds per image for the complete CLIP-guided SAM2 pipeline.

\subsection{Boundary Refinement}
\label{sec:boundary_refinement}

This enhancement improves boundary quality in the final segmentation.

While SAM2 provides high-quality instance boundaries, the final merged segmentation can exhibit minor boundary artifacts at class transitions, particularly in regions where multiple SAM2 masks overlap or where SCLIP predictions have low confidence. To address this, the system incorporates \textbf{Dense Conditional Random Field (CRF)} post-processing \cite{krahenbuhl2011efficient} as a refinement stage.

\textbf{Dense CRF formulation:} Given the merged segmentation logits $P \in \mathbb{R}^{H \times W \times C}$ and original image $I$, Dense CRF refines boundaries by minimizing an energy function that encourages:
\begin{itemize}
    \item \textbf{Label consistency:} Adjacent pixels with similar colors should have similar labels
    \item \textbf{Spatial smoothness:} Encourages coherent regions while preserving object boundaries
    \item \textbf{Appearance-based affinity:} Uses bilateral filtering to respect color edges
\end{itemize}

The energy function combines unary potentials (from CLIP predictions) with pairwise potentials (spatial and appearance terms):
\begin{equation}
E(y) = \sum_{i} \psi_u(y_i) + \sum_{i<j} \psi_p(y_i, y_j)
\end{equation}

where $\psi_u$ are unary potentials from SCLIP logits, and $\psi_p$ are pairwise potentials combining Gaussian spatial kernels and bilateral appearance kernels.

\textbf{Implementation:} The refinement uses the pydensecrf library \cite{krahenbuhl2011efficient} with the following parameters tuned for open-vocabulary segmentation:
\begin{itemize}
    \item \textbf{Inference iterations:} 10 steps of mean-field approximation
    \item \textbf{Spatial weight:} 3.0 (smoothness strength)
    \item \textbf{Bilateral weight:} 5.0 (appearance affinity)
    \item \textbf{Spatial std:} 3 pixels (local smoothing radius)
    \item \textbf{Color std:} 5 (bilateral filter color tolerance)
\end{itemize}

\textbf{Performance impact:} Dense CRF adds approximately 0.5-1.0 seconds per image but provides measurable quality improvements:
\begin{itemize}
    \item \textbf{mIoU improvement:} +1-2 percentage points on PASCAL VOC 2012
    \item \textbf{Boundary F1:} +3-5 percentage points (improves edge alignment)
    \item \textbf{Qualitative benefits:} Smoother class transitions, reduced salt-and-pepper noise
\end{itemize}

The Dense CRF refinement is particularly effective for:
\begin{itemize}
    \item \textbf{Stuff classes:} Smooths segmentation of amorphous regions (sky, grass, road)
    \item \textbf{Class boundaries:} Sharpens transitions between adjacent objects
    \item \textbf{Low-confidence regions:} Resolves ambiguities through spatial context
\end{itemize}

\textbf{Note:} While computationally more expensive, this refinement is essential for achieving near state-of-the-art results and is considered an integral part of the high-accuracy pipeline configuration.

\subsection{Generative Editing Integration}

Once refined segmentation masks are obtained from images, Stable Diffusion v2 Inpainting \cite{rombach2022high} is optionally integrated for text-driven image modification. The inpainting model operates in the latent space of a variational autoencoder (VAE), enabling high-resolution generation with computational efficiency.

Several techniques are employed to ensure coherent results:
\begin{itemize}
    \item \textbf{Mask dilation:} Expand mask boundaries by 5-10 pixels for smoother blending
    \item \textbf{Classifier-free guidance:} Use guidance scale of 7.5 to balance prompt following and realism
    \item \textbf{Prompt engineering:} Reformulate user queries into detailed prompts suitable for the diffusion model
\end{itemize}

This completes the CLIP-guided prompting pipeline, enabling fully annotation-free open-vocabulary segmentation with high-quality boundaries and massive efficiency gains through intelligent semantic guidance.

\subsection{Video Generation Extension}

While the primary focus of this thesis is image segmentation evaluated on PASCAL VOC 2012, the CLIP-guided prompting approach extends naturally to video processing. The video extension demonstrates two capabilities:

\begin{enumerate}
    \item \textbf{Video Segmentation:} CLIP analyzes only the first frame to identify objects, then SAM2's memory-based tracking propagates masks across all frames without per-frame CLIP inference
    \item \textbf{Video Inpainting:} WAN 2.1 VACE \cite{vace2025} enables text-driven object removal and replacement in videos using the generated masks
\end{enumerate}

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.95\textwidth]{Imagenes/7. Video Generation.png}
\caption{Video generation pipeline. Top: original frames. Middle: CLIP-guided SAM2 segmentation. Bottom: VACE inpainting results.}
\label{fig:video_generation}
\end{figure}

The complete video processing pipeline (Figure~\ref{fig:video_generation}) enables zero-shot video editing workflows such as removing specific objects from sports footage or surveillance videos using only natural language descriptions.

For complete technical details including SAM2's temporal tracking mechanism, VACE configuration parameters, memory management strategies, and implementation considerations, see \textbf{Appendix~\ref{appendix:video_generation}}.
