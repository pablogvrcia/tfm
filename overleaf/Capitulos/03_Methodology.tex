\chapter{Methodology}
\label{ch:methodology}

This chapter presents the SCLIP-guided prompting methodology for open-vocabulary semantic segmentation. The approach combines SCLIP's \cite{sclip2024} dense predictions with SAM2's \cite{ravi2024sam2} precise boundaries through intelligent prompt extraction, achieving fully annotation-free segmentation with massive efficiency gains over grid-based prompting. The generated masks enable text-driven image inpainting via Stable Diffusion \cite{rombach2022high} and video inpainting via VACE \cite{vace2025}.

\section{SCLIP-Guided Prompting Approach}
\label{sec:clip_guided_approach}

\subsection{Motivation and Design Philosophy}

SCLIP-guided prompting addresses a fundamental inefficiency in alternative SAM2 prompting strategies. Naive approaches use exhaustive grid sampling: prompting SAM at uniformly distributed points without considering image content. This blind prompting suffers from computational inefficiency (generating SAM masks at every grid location is prohibitively expensive) and semantic blindness (wasting computation on background regions).

SCLIP-guided prompting inverts this paradigm: SCLIP identifies where objects exist, enabling SAM2 to focus only on meaningful regions. Figure~\ref{fig:blind_vs_intelligent} illustrates this difference—blind grid prompting distributes prompts uniformly, while the SCLIP-guided approach concentrates them on actual object instances.

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.95\textwidth]{Imagenes/comparison_blind_vs_intelligent_2panel.png}
\caption{Comparison between blind grid prompting (left) and SCLIP-guided intelligent prompting (right). Grid sampling places prompts uniformly without semantic awareness, while the SCLIP-guided approach extracts prompts only at semantically meaningful locations.}
\label{fig:blind_vs_intelligent}
\end{figure}

\FloatBarrier

The approach requires zero spatial user input: users provide only a text vocabulary, and the system automatically determines object locations. SAM2 then provides precise boundaries at each semantically-guided prompt point.

\subsection{System Overview}

An schematic overview of the complete SCLIP-guided prompting pipeline is presented in Figure~\ref{fig:clip_guided_pipeline}. The diagram illustrates the four main stages of the approach: SCLIP first generates dense semantic predictions with confidence maps for each class in the vocabulary; these predictions are then processed to extract intelligent prompt points, dramatically reducing the number of required prompts from thousands to just a handful; SAM2 is then prompted at each extracted point to generate high-quality instance masks with direct class assignment; finally, overlapping masks are resolved through IoU-based filtering to produce the final segmentation.

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.7\textwidth,keepaspectratio]{Imagenes/2.PipelineDiagram.png}
\caption{Schematic overview of the SCLIP-guided prompting pipeline. The system uses CLIP's dense predictions to extract intelligent prompt points.}
\label{fig:clip_guided_pipeline}
\end{figure}

\FloatBarrier

\subsection{SCLIP Dense Prediction}
\label{sec:sclip_dense_extraction}

The first stage of the pipeline uses SCLIP \cite{sclip2024} to generate dense semantic predictions and confidence maps for each class in the user-defined vocabulary. Figure~\ref{fig:sclip_pipeline} illustrates the complete SCLIP dense prediction process.

\begin{figure}[!htbp]
\centering
\includegraphics[width=1\textwidth,keepaspectratio]{Imagenes/pipeline_1_1.png}
\caption{SCLIP dense prediction pipeline.}
\label{fig:sclip_pipeline}
\end{figure}

\FloatBarrier

The proposed approach builds on CLIP (Contrastive Language-Image Pre-training) \cite{radford2021learning}, a foundation model that learns a shared embedding space between images and text through contrastive learning on 400 million image-text pairs. CLIP's key capability is open-vocabulary recognition: given an image and arbitrary text descriptions, it can match visual content to semantic concepts without requiring task-specific training.

CLIP is particularly suitable for open-vocabulary segmentation due to three key properties. First, it enables zero-shot transfer by recognizing objects from text descriptions alone, without requiring annotated segmentation masks. Second, it supports flexible vocabularies, handling any user-defined class set rather than being limited to predefined categories. Third, CLIP's Vision Transformer (ViT) backbone processes images as patch sequences, providing spatial feature maps that can be adapted for dense pixel-wise predictions.

However, CLIP was designed for image-level classification, not dense prediction. SCLIP addresses this limitation by adapting CLIP's attention mechanism for semantic segmentation, as illustrated in the pipeline of Figure~\ref{fig:sclip_pipeline}. For detailed technical explanations of CLIP's contrastive learning objective and Vision Transformer architecture, please refer to Appendix \ref{appendix:technical_foundations}.

SCLIP's core innovation is a simple modification: only the final attention layer (layer 12) of CLIP's ViT-B/16 encoder is changed, while layers 1-11 remain unchanged. Standard attention computes:
\begin{equation}
A_h^{\text{standard}} = \text{softmax}\left(\frac{Q_h K_h^T}{\sqrt{d_h}}\right)
\end{equation}

Layer 12 instead uses Correlative Self-Attention (CSA):
\begin{equation}
\boxed{A_h^{\text{CSA}} = \text{softmax}\left(\frac{Q_h Q_h^T}{\sqrt{d_h}}\right) + \text{softmax}\left(\frac{K_h K_h^T}{\sqrt{d_h}}\right)}
\end{equation}

Instead of computing $Q_h K_h^T$, CSA computes $Q_h Q_h^T$ and $K_h K_h^T$ separately under independent softmax operations, then sums them. This measures patch-to-patch similarity based on query and key representations independently. Patches with similar content (e.g., all sky patches) mutually reinforce each other through high $Q_h Q_h^T$ and $K_h K_h^T$ scores, producing spatially coherent features where semantically similar regions cluster together. Standard $Q_h K_h^T$ attention creates holistic image representations ideal for classification but produces fragmented features for pixel-wise tasks.

SCLIP processes images using sliding window inference. Input images are resized to 336 pixels on the shorter side, then a 224×224 window slides across with stride 112, creating overlapping crops. Each crop is processed independently and predictions are averaged to produce the final segmentation.

For each 224×224 window, ViT-B/16 divides the input into 16×16 pixel patches, creating a 14×14 grid (224÷16 = 14). Each patch becomes a 512-dimensional feature vector, yielding $F \in \mathbb{R}^{14 \times 14 \times 512}$. Simultaneously, text descriptors for each class are encoded with templates to produce class embeddings $e_c \in \mathbb{R}^{512}$.

Dense cosine similarity is computed between image patches and class embeddings:
\begin{equation}
S_{i,j,c} = F[i,j,:] \cdot e_c \in [-1, 1], \quad S \in \mathbb{R}^{14 \times 14 \times C}
\end{equation}
where $(i,j) \in [0,13] \times [0,13]$ indexes patches and $c$ indexes classes.

The 14×14 similarity map is upsampled to 224×224 via bilinear interpolation, then temperature-scaled softmax ($T=40$) converts similarities to probabilities:
\begin{equation}
P_{x,y,c} = \frac{\exp(T \cdot S'_{x,y,c})}{\sum_{c'} \exp(T \cdot S'_{x,y,c'})}, \quad \hat{c}(x,y) = \arg\max_c P_{x,y,c}
\end{equation}

This produces pixel-wise predictions $\hat{c} \in \mathbb{R}^{H \times W}$ and confidence maps $P \in \mathbb{R}^{H \times W \times C}$ for the intelligent prompting strategy (Section~\ref{sec:intelligent_prompting}).

Optionally, Dense Conditional Random Field (CRF) \cite{krahenbuhl2011efficient} post-processing can be applied to refine prediction boundaries. The CRF encourages spatial smoothness and label consistency while respecting color edges through bilateral filtering. Figure~\ref{fig:densecrf_comparison} illustrates how DenseCRF cleans the dense predictions by smoothing noisy boundaries while preserving sharp transitions at object edges. When using SCLIP-guided SAM prompting, this refinement indirectly benefits the final output by producing cleaner segmentation maps, which in turn generate more accurate prompt point locations for SAM2.

\begin{figure}[h]
\centering
\includegraphics[width=0.95\textwidth]{Imagenes/motogp_densecrf_comparison.png}
\caption{Dense Conditional Random Field boundary refinement. The top-right panel highlights pixels modified by DenseCRF in red, showing how it smooths segmentation boundaries while respecting color edges in the original image.}
\label{fig:densecrf_comparison}
\end{figure}

\FloatBarrier

To improve CLIP's semantic understanding, the system uses descriptor files that map each class to multiple descriptive terms rather than a single name. This handles intra-class variation and ambiguous class names that may not align with CLIP's pretraining vocabulary. Template strategies determine how these terms are framed for CLIP, with options ranging from 80 ImageNet templates to optimized sets of 3-7 templates for faster inference. For each class $c$ with descriptor terms $D_c$ and templates $\{t_1, \ldots, t_M\}$, text embeddings are computed as:
\begin{equation}
e_c = \frac{1}{K_c \cdot M} \sum_{d \in D_c} \sum_{i=1}^{M} \text{CLIP}_{\text{text}}(t_i(d)), \quad e_c \leftarrow e_c / \|e_c\|_2
\end{equation}

\subsection{Intelligent Prompt Extraction}
\label{sec:intelligent_prompting}

This is the core algorithmic contribution: using SCLIP's dense predictions to extract intelligent prompt points for SAM2. This section details the complete pipeline from SCLIP's dense predictions to semantically meaningful prompt points. Figure~\ref{fig:sam_pipeline} illustrates the overall process.

\begin{figure}[!htbp]
\centering
\includegraphics[width=\textwidth,height=0.4\textheight,keepaspectratio]{Imagenes/pipeline_1_2.png}
\caption{SCLIP-guided prompting with intelligent prompt extraction. SCLIP's dense predictions are processed through a pipeline to extract semantically meaningful prompt point.}
\label{fig:sam_pipeline}
\end{figure}

\FloatBarrier

Given SCLIP's dense predictions—segmentation map $\hat{c} \in \mathbb{R}^{H \times W}$ and probability map $P \in \mathbb{R}^{H \times W \times C}$—the approach extracts intelligent prompts and generates the final segmentation. Low-confidence predictions are first filtered by retaining only pixels where SCLIP's confidence for the predicted class exceeds threshold $\tau_{\text{conf}}$ (higher values produce fewer but higher-quality prompts, while lower values increase recall at the cost of false positives):

\begin{equation}
M^{\text{conf}}(x,y) = \mathbb{1}[P_{x,y,\hat{c}(x,y)} > \tau_{\text{conf}}]
\end{equation}

The filtered confidence map undergoes 8-connected component analysis to identify spatially disjoint object instances, yielding regions $\{R^1, R^2, \ldots, R^N\}$. Spurious detections are removed by discarding regions with pixel count $|R^i| < \tau_{\text{area}}$ (larger values filter more aggressively but may miss small objects). For each valid region, the centroid serves as the prompt point for SAM2:

\begin{equation}
\mathbf{p}^i = \left( \frac{1}{|R^i|} \sum_{(x,y) \in R^i} x, \frac{1}{|R^i|} \sum_{(x,y) \in R^i} y \right)
\end{equation}

Each prompt inherits the class predicted by SCLIP at that location, enabling direct class assignment without requiring voting or multi-scale evaluation. SAM2 then generates instance masks for all prompts, combining SCLIP's semantic understanding with SAM2's precise object boundaries. Since multiple prompts may detect the same physical object, overlapping masks of identical class are merged using IoU-based filtering with threshold $\tau_{\text{IoU}}$: masks are sorted by SCLIP confidence in descending order, and for each mask, if it overlaps with a previously kept mask of the same class with IoU exceeding the threshold, the lower-confidence mask is discarded. Finally, cross-class conflicts are resolved by constructing the segmentation map pixel-by-pixel. All remaining masks are sorted by confidence, and pixels are assigned sequentially with higher-confidence masks overwriting lower ones in spatial overlaps. Figure \ref{fig:real_prompt_extraction} illustrates this process.

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.95\textwidth]{Imagenes/real_prompt_extraction_example.png}
\caption{Real PASCAL VOC example showing semantic prompts extracted from an airport scene with aeroplane and person instances.}
\label{fig:real_prompt_extraction}
\end{figure}

To illustrate the effectiveness of the presented method, Figure \ref{fig:comparison_baseline} demonstrates the advantages of SCLIP-guided prompting over SCLIP's dense prediction baseline. Four representative examples highlight key improvements: superior boundary quality through SAM2 integration and cleaner instance separation in cluttered scenes. The intelligent prompting strategy effectively leverages SCLIP's semantic understanding to guide SAM2, resulting in more accurate and visually appealing segmentations compared to the dense prediction baseline.

Algorithm~\ref{alg:intelligent_prompting} summarizes the complete prompt extraction and mask generation process.

\begin{algorithm}[H]
\caption{SCLIP-Guided Prompt Extraction and SAM2 Segmentation}
\label{alg:intelligent_prompting}
\begin{algorithmic}[1]
\State \textbf{Input:} Segmentation map $\hat{c} \in \mathbb{R}^{H \times W}$, probability map $P \in \mathbb{R}^{H \times W \times C}$
\State \textbf{Parameters:} $\tau_{\text{conf}}$, $\tau_{\text{area}}$, $\tau_{\text{IoU}}$
\State \textbf{Output:} Final segmentation $\hat{y} \in \mathbb{R}^{H \times W}$
\State
\State \textbf{// 1. Confidence filtering}
\State $M^{\text{conf}}(x,y) \gets \mathbb{1}[P_{x,y,\hat{c}(x,y)} > \tau_{\text{conf}}]$
\State
\State \textbf{// 2. Clustering and region filtering}
\State $\{R^1, R^2, \ldots, R^N\} \gets \text{ConnectedComponents}(M^{\text{conf}})$
\State $\mathcal{R}_{\text{valid}} \gets \{R^i : |R^i| > \tau_{\text{area}}\}$
\State
\State \textbf{// 3. Centroid extraction}
\State $\mathcal{P} \gets \emptyset$
\For{each region $R^i \in \mathcal{R}_{\text{valid}}$}
    \State $\mathbf{p}^i \gets \left( \frac{1}{|R^i|} \sum_{(x,y) \in R^i} x, \frac{1}{|R^i|} \sum_{(x,y) \in R^i} y \right)$
    \State $c^i \gets \hat{c}(\mathbf{p}^i)$ \Comment{Inherit class from SCLIP}
    \State $\text{conf}^i \gets P_{\mathbf{p}^i, c^i}$
    \State $\mathcal{P} \gets \mathcal{P} \cup \{(\mathbf{p}^i, c^i, \text{conf}^i)\}$
\EndFor
\State
\State \textbf{// 4. SAM2 segmentation}
\State $\mathcal{M} \gets \{\text{SAM2}(\mathbf{p}^i) : (\mathbf{p}^i, c^i, \text{conf}^i) \in \mathcal{P}\}$
\State
\State \textbf{// 5. IoU-based merge (same class)}
\State Sort $\mathcal{M}$ by confidence (descending)
\State $\mathcal{M}_{\text{kept}} \gets \emptyset$
\For{each mask $m \in \mathcal{M}$}
    \If{$\nexists m' \in \mathcal{M}_{\text{kept}}$ such that $c_m = c_{m'}$ and $\text{IoU}(m, m') > \tau_{\text{IoU}}$}
        \State $\mathcal{M}_{\text{kept}} \gets \mathcal{M}_{\text{kept}} \cup \{m\}$
    \EndIf
\EndFor
\State
\State \textbf{// 6. Cross-class conflict resolution}
\State $\hat{y} \gets \mathbf{0}$
\For{each mask $m \in \mathcal{M}_{\text{kept}}$ (sorted by confidence)}
    \State $\hat{y}[m] \gets c_m$ \Comment{Higher confidence overwrites}
\EndFor
\State \Return $\hat{y}$
\end{algorithmic}
\end{algorithm}

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.95\textwidth]{Imagenes/3.Bound.png}
\caption{Comparison between SCLIP dense prediction baseline and the proposed SCLIP-guided prompting approach. Four representative examples demonstrate key advantages: superior boundary quality through SAM2 integration, improved small object detection, and cleaner instance separation in cluttered scenes.}
\label{fig:comparison_baseline}
\end{figure}

% \subsection{Computational Optimizations}
% \label{sec:computational_optimizations}

% Beyond the algorithmic improvements (descriptor files, template strategies, intelligent prompting), several low-level computational optimizations significantly improve inference speed and memory efficiency without affecting accuracy. These optimizations are critical for practical deployment, especially on resource-constrained hardware.

% \subsubsection{Computational Complexity Analysis}

% Before discussing implementation optimizations, we analyze the computational complexity of the SCLIP-guided prompting approach compared to baseline methods.

% For an image of resolution $H \times W$ with $C$ classes and CLIP embedding dimension $D = 512$:

% \textbf{SCLIP dense prediction:} $\mathcal{O}(H \times W \times C \times D)$ for computing similarity between all pixels and all class embeddings.

% \textbf{Intelligent prompt extraction:} $\mathcal{O}(H \times W)$ for connected component analysis and centroid computation, dominated by the linear scan through confidence masks.

% \textbf{SAM2 segmentation:} $\mathcal{O}(N_{\text{prompts}} \times T_{\text{SAM2}})$ where $N_{\text{prompts}} \in [50, 300]$ is the number of extracted prompts and $T_{\text{SAM2}}$ is the per-prompt processing time.

% \textbf{Baseline grid sampling:} $\mathcal{O}(N_{\text{grid}} \times T_{\text{SAM2}})$ where $N_{\text{grid}} = 4096$ for a $64 \times 64$ grid.

% \textbf{Theoretical speedup:} $\frac{N_{\text{grid}}}{N_{\text{prompts}}} = \frac{4096}{50\text{-}300} \approx 13\text{-}80\times$

% In practice, the speedup is lower due to fixed costs (image encoding, feature extraction), but the intelligent prompting approach consistently achieves $3\text{-}4\times$ end-to-end speedup on PASCAL VOC 2012 images.

% \subsubsection{Mixed Precision Inference (FP16)}

% Modern GPUs achieve substantially higher throughput when operating on 16-bit floating-point (FP16) data compared to standard 32-bit (FP32). The implementation leverages PyTorch's automatic mixed precision (AMP) to accelerate inference while preserving numerical stability. Code details are available in Appendix \ref{sec:appendix_optimizations}.

% The \texttt{autocast} context automatically identifies operations that benefit from FP16 (e.g., matrix multiplications, convolutions) while keeping precision-sensitive operations (e.g., softmax, normalization) in FP32. This hybrid approach provides approximately $1.5\text{-}2\times$ faster inference on modern GPUs (Ampere, Ada Lovelace architectures), 50\% reduction in activation memory, and no measurable impact on segmentation quality.

% \subsubsection{Just-In-Time Compilation}

% PyTorch 2.0+ introduces \texttt{torch.compile()}, which fuses operations, eliminates Python overhead, and generates optimized CUDA kernels. The implementation applies compilation to both the CLIP vision encoder and text encoder using \texttt{mode="reduce-overhead"}. See Appendix \ref{sec:appendix_optimizations} for details.

% While compilation incurs a one-time overhead (10-20 seconds), subsequent inferences benefit from kernel fusion, memory optimization, and graph-level optimization, providing an additional $1.2\text{-}1.5\times$ improvement on top of FP16 optimizations.

% \subsubsection{Batched SAM2 Prompting}

% Standard SAM2 usage prompts one point at a time, requiring separate forward passes for each prompt. With 50-300 prompts per image, this creates significant overhead. The implementation processes multiple points simultaneously in batches (default size 32), exploiting GPU parallelism to achieve $2\text{-}4\times$ speedup over sequential prompting. Implementation details in Appendix \ref{sec:appendix_optimizations}.

% \subsubsection{Combined Impact}

% \begin{table}[!htbp]
% \centering
% \begin{tabular}{lcc}
% \hline
% \textbf{Configuration} & \textbf{Time/Image (s)} & \textbf{Relative Speedup} \\
% \hline
% Baseline (FP32, no compile, sequential) & 45-60 & 1.0× \\
% + FP16 & 25-35 & 1.5-1.8× \\
% + FP16 + torch.compile & 18-28 & 2.1-2.5× \\
% + FP16 + torch.compile + batching & 12-20 & 3.0-3.8× \\
% \hline
% \end{tabular}
% \caption{Cumulative impact of computational optimizations on inference time.}
% \label{tab:computational_optimizations}
% \end{table}

% These optimizations enable practical deployment on consumer hardware (e.g., NVIDIA GeForce GTX 1060 6GB) where the best configuration achieves 12-33 seconds per image for the complete SCLIP-guided SAM2 pipeline.

\section{Generative Editing Integration}

\subsection{Image Inpainting}

The segmentation masks produced by SCLIP-guided prompting enable text-driven image editing through integration with Stable Diffusion v2 Inpainting \cite{rombach2022high}. Figure~\ref{fig:inpainting_pipeline} illustrates the complete workflow: users select a class from the vocabulary (e.g., "person") and provide a text prompt describing the desired modification. The system extracts a binary mask for the selected class from the segmentation map and uses it to guide Stable Diffusion, which replaces the masked region according to the text prompt while preserving the surrounding image context.

Stable Diffusion v2 Inpainting is a latent diffusion model trained specifically for image completion tasks. Unlike standard diffusion models that operate directly on pixels, it works in a compressed latent space through a variational autoencoder, reducing computational cost while maintaining quality. The model receives three inputs: the original image, the binary mask indicating regions to modify, and the text prompt describing desired content. Through an iterative denoising process, the model generates new content for masked regions while maintaining semantic and visual coherence with surrounding unmasked areas. The conditioning mechanism ensures that generated content respects both the text description and the boundary context, producing realistic transitions between original and synthesized regions.

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.75\textwidth]{Imagenes/image_inpaint_pipeline.png}
\caption{Generative image inpainting pipeline. A vocabulary word extracts a binary mask from the segmentation, which combines with a user text prompt to guide Stable Diffusion in generating the final inpainted result.}
\label{fig:inpainting_pipeline}
\end{figure}

To ensure coherent results, mask boundaries are expanded by several pixels for smoother blending at edges, and user queries are reformulated into descriptive prompts suitable for the diffusion model. This integration demonstrates how open-vocabulary segmentation enables downstream generative editing tasks by automatically extracting masks from text vocabulary.

\FloatBarrier

\subsection{Video Inpainting}

The SCLIP-guided prompting approach extends to video processing by leveraging SAM2's temporal tracking capabilities. Figure~\ref{fig:video_inpainting_pipeline} illustrates the complete workflow. The pipeline begins by extracting and segmenting the first frame of the video using SCLIP-guided prompting. SAM2 then propagates the segmentation masks across all subsequent frames through its memory-based tracking mechanism, eliminating the need for per-frame CLIP inference. Once temporal segmentation is complete, the user selects a target class from the vocabulary to generate a binary mask spanning all frames. This mask, combined with the original video where masked regions are grayed out, is passed to WAN 2.1 VACE \cite{vace2025} along with a text prompt describing the desired content. VACE processes the entire video sequence to produce the final inpainted result with temporally consistent modifications.

WAN 2.1 VACE (Video Adaptive Content Enhancement) is a video inpainting model designed to maintain temporal consistency across frames. Video inpainting presents unique challenges compared to images: generated content must remain coherent not only spatially within each frame but also temporally across the sequence to avoid flickering or discontinuities. VACE addresses this through temporal attention mechanisms that propagate information across frames, ensuring that synthesized content maintains consistent appearance, motion, and style throughout the video. The model processes the masked video sequence along with the text prompt, generating new content that seamlessly integrates with the unmasked regions while preserving temporal dynamics. This approach is particularly effective for object removal and replacement scenarios where maintaining natural motion and lighting consistency is critical.

This integration enables zero-shot video editing workflows such as removing or replacing specific objects in videos using only natural language descriptions, automatically extracting masks from text vocabulary.

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.75\textwidth]{Imagenes/video_inpaint_pipeline.png}
\caption{Video inpainting pipeline. The first frame is segmented using SCLIP-guided prompting, then SAM2 propagates masks across frames. A selected vocabulary class generates a binary mask and grayed video, which VACE uses with a text prompt to produce the final inpainted video.}
\label{fig:video_inpainting_pipeline}
\end{figure}

\FloatBarrier
