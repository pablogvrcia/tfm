\vspace{2.5cm}

{\Large \bfseries Abstract}

\vspace{1.5cm}

Conventional semantic segmentation methods are limited to recognizing objects from a fixed set of categories, restricting their applicability in real-world scenarios where novel or unexpected concepts often arise. This thesis addresses the problem of open-vocabulary semantic segmentation through CLIP (Contrastive Language-Image Pre-training)-guided prompting, achieving 68.09\% mean Intersection over Union (mIoU) on PASCAL VOC 2012 while reducing computational cost by 96\% through intelligent semantic guidance.

The proposed approach builds upon SCLIP's (Self-attention CLIP) Cross-layer Self-Attention (CSA) mechanism for dense semantic prediction. Instead of using CLIP's predictions as the final output, the method extracts intelligent prompt points from high-confidence regions (50-300 points) to guide SAM 2's (Segment Anything Model 2) mask generation. This achieves 96\% reduction in prompts compared to blind grid sampling (4096 points) while outperforming recent training-free open-vocabulary methods on PASCAL VOC 2012 (68.09\% mIoU vs. ITACLIP's 67.9\%). Class labels are directly assigned from CLIP's predictions at each prompt location, avoiding the complexity of majority voting or multi-scale evaluation. This annotation-free approach enables segmentation of both discrete objects and amorphous ``stuff'' classes (sky, road, vegetation) without requiring any labeled training data.

The system is further enhanced through descriptor files that provide multi-term class representations (e.g., ``person in shirt'', ``person in jeans'') and optimized template strategies, improving robustness to intra-class variations. Computational optimizations (FP16, torch.compile, batched prompting) enable practical deployment on consumer hardware (6GB VRAM, 12-33 seconds per image).

Once segmentation masks are obtained, the system integrates Stable Diffusion v2 to enable text-driven image editing, creating a complete pipeline from language-based semantic understanding to image manipulation (object removal, replacement, style transfer). The approach extends to video segmentation by leveraging SAM 2's temporal tracking capabilities, enabling consistent object segmentation across video frames with CLIP analysis performed only on the first frame.

Comprehensive evaluation on PASCAL VOC 2012 demonstrates strong performance across 21 object categories, with exceptional background recognition (86.90\% IoU) and robust class-specific results (horse 78.49\%, cat 75.48\%, aeroplane 76.53\%). The intelligent prompting strategy provides both efficiency gains (96\% fewer prompts, 3-4Ã— speedup) and quality improvements through SAM 2's superior boundary delineation, demonstrating that semantic guidance outperforms exhaustive spatial sampling in both accuracy and efficiency for open-vocabulary segmentation on this benchmark.