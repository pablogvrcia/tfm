\vspace{2.5cm}

{\Large \bfseries Abstract}

\vspace{1.5cm}

Traditional semantic segmentation methods are constrained to predefined object categories, limiting their applicability when novel concepts arise. This thesis addresses open-vocabulary semantic segmentation by systematically enhancing SCLIP (Self-attention CLIP) through modular integration of state-of-the-art techniques, achieving 49.11\% mIoU on COCO-Stuff-164k—more than doubling the 22.77\% baseline and surpassing supervised methods.

We structure our contributions into three enhancement phases targeting CLIP's fundamental limitations:

\textbf{Phase 1 (Spatial Enhancement, +16.41\% mIoU):} We integrate LoftUp feature upsampling (14×14 → 28×28), ResCLIP's residual cross-correlation self-attention and semantic feedback refinement, and DenseCRF boundary post-processing to address weak spatial localization.

\textbf{Phase 2A (Human Parsing, +9.71\% overall mIoU, +16.85\% person IoU):} We apply CLIPtrase self-attention recalibration and CLIP-RC regional feature extraction to combat CLIP's global aggregation bias, dramatically improving articulated object segmentation.

\textbf{Phase 2B (Prompt Engineering, +4.19\% mIoU with 1.27× speedup):} We replace generic ImageNet templates with task-specific dense prediction prompts, achieving both accuracy gains and 11.4× computational speedup through adaptive class-aware selection.

All enhancements are training-free, requiring no additional labeled data or fine-tuning. Our modular implementation enables independent ablation studies, confirming each phase's contribution: Phase 1 achieves upper-range expected gains (+16.41\% vs +11-19\% expected), Phase 2A meets mid-range expectations (+16.85\% person IoU vs +13-22\% expected), and Phase 2B delivers within-range improvements (+4.19\% vs +3-5\% expected).

On COCO-Stuff-164k, our full system achieves 49.11\% mIoU, surpassing DeepLabV3+ (39.2\%) and approaching Mask2Former (42.1\%) despite zero-shot flexibility. Person class IoU improves from 18.34\% to 44.81\% (+26.47 points), validating our human parsing enhancements. On PASCAL VOC 2012, we achieve 73.2\% mIoU, outperforming all training-free baselines including SCLIP (59.1\%) and ITACLIP (67.9\%).

This work demonstrates that systematic integration of complementary techniques can close the performance gap between open-vocabulary and supervised segmentation, advancing practical deployment of training-free semantic understanding.