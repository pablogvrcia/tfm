\vspace{2.5cm}

{\Large \bfseries Abstract}

\vspace{1.5cm}

Conventional semantic segmentation methods are limited to recognizing objects from a fixed set of categories, restricting their applicability in real-world scenarios where novel or unexpected concepts often arise. This thesis tackles the problem of open-vocabulary semantic segmentation and integrates it with generative image editing techniques, enabling flexible object discovery and manipulation based solely on natural language prompts.

Our approach begins by using a promptable segmentation model (SAM2) to produce a broad set of candidate masks without relying on predefined classes. To identify which of these masks correspond to a user-specified concept, we leverage a vision-language model (CLIP) with multi-scale voting—evaluating masks at three resolutions (224px, 336px, 512px) with weighted averaging to improve robustness across object sizes. We further employ a multi-instance selection strategy that adaptively handles variable numbers of object instances, from single objects to multiple discrete instances to semantic parts, using overlap-based filtering and confidence thresholds. This process enables the model to segment objects not explicitly known during training while naturally handling complex scenes with multiple instances.

Once relevant masks are selected, we apply a state-of-the-art generative inpainting model (Stable Diffusion v2) to modify the segmented regions. This model, guided by text prompts, can remove, replace, or transform objects realistically, seamlessly integrating them into the scene. The resulting system transcends conventional fixed-label segmentation pipelines by empowering users to directly command both segmentation and image editing through natural language instructions.

We validate our method on PASCAL VOC 2012, achieving 69.3\% mIoU—a significant 13.2 percentage point improvement over existing open-vocabulary methods like MaskCLIP. The multi-scale CLIP voting alone contributes +6.8\% mIoU improvement over baseline single-scale scoring.

Additionally, this thesis explores dense CLIP-based segmentation by building upon SCLIP's (Self-attention CLIP) Cross-layer Self-Attention (CSA) mechanism. We extend this foundation with a novel SAM2-based mask refinement layer that uses majority voting to combine SCLIP's dense predictions with SAM2's high-quality boundaries. Our extended approach achieves 49.52\% mIoU on COCO-Stuff and 48.09\% mIoU on PASCAL VOC in a fully annotation-free setting, demonstrating significant improvements over baseline dense methods (38.4$\times$ over naive CLIP on COCO-Stuff, +83\% over state-of-the-art ITACLIP on COCO-Stuff) and providing insights into the complementary strengths of proposal-based versus dense prediction approaches.

Our results demonstrate the system's ability to handle novel concepts, produce accurate segmentation masks for multiple instances, and enable high-quality text-driven image modifications across both methodological approaches.