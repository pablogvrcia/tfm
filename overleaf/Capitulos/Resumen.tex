\vspace{2.5cm}

{\Large \bfseries Abstract}

\vspace{1.5cm}

This thesis presents a training-free approach to open-vocabulary semantic segmentation that enables recognizing and segmenting objects from arbitrary text descriptions without requiring labeled data. The work addresses a fundamental limitation of conventional segmentation methods: their restriction to predefined object categories makes them unsuitable for real-world applications where novel concepts frequently appear.

The proposed method combines SCLIP (Self-attention CLIP) for dense semantic understanding with SAM 2 (Segment Anything Model 2) for precise instance segmentation. Rather than using SCLIP's dense predictions directly, the approach extracts intelligent prompt points from high-confidence regions of SCLIP's semantic predictions. These semantically-informed prompts guide SAM 2's mask generation, focusing computational resources on relevant image regions. Class labels are assigned directly from SCLIP's predictions at each prompt location, enabling segmentation of both discrete objects and amorphous regions like sky or vegetation.

Evaluation on standard benchmarks demonstrates competitive performance against recent training-free methods, with strong results across diverse object categories.

Beyond segmentation, the work integrates generative editing capabilities by combining the extracted masks with diffusion-based inpainting models. For images, Stable Diffusion v2 enables text-driven object removal, replacement, and style modification. For videos, the approach leverages SAM 2's temporal tracking to propagate segmentation across frames, requiring SCLIP analysis only on the first frame. The resulting video masks guide WAN 2.1 VACE to enable consistent temporal editing. This creates an end-to-end pipeline from natural language descriptions to segmentation-based image and video manipulation.