\vspace{2.5cm}

{\Large \bfseries Abstract}

\vspace{1.5cm}

Conventional semantic segmentation methods are limited to recognizing objects from a fixed set of categories, restricting their applicability in real-world scenarios where novel or unexpected concepts often arise. This thesis tackles the problem of open-vocabulary semantic segmentation through CLIP-guided prompting, an efficient approach that uses CLIP's semantic understanding to intelligently guide SAM2 segmentation, enabling flexible semantic understanding based solely on natural language prompts.

Our approach builds upon SCLIP's (Self-attention CLIP) Cross-layer Self-Attention (CSA) mechanism for dense semantic prediction. Instead of using CLIP's predictions as the final output, we extract intelligent prompt points from high-confidence regions (50-300 points) and use these to guide SAM2's mask generation. This achieves 96\% reduction in prompts compared to blind grid sampling (4096 points) while maintaining competitive accuracy. Class labels are directly assigned from CLIP's predictions at each prompt location, avoiding the complexity of majority voting or multi-scale evaluation. This annotation-free approach enables the model to segment both discrete objects and amorphous ``stuff'' classes (sky, road, vegetation) without requiring any labeled training data.

Once segmentation masks are obtained, we integrate a state-of-the-art generative inpainting model (Stable Diffusion v2) to enable text-driven image editing. This model can remove, replace, or transform segmented regions realistically, creating a complete pipeline from language-based semantic understanding to image manipulation. Additionally, our approach extends to video segmentation by leveraging SAM2's temporal tracking capabilities, enabling consistent object segmentation across video frames with CLIP analysis performed only on the first frame.

We validate our CLIP-guided prompting approach on PASCAL VOC 2012, achieving 59.78\% mIoU. The intelligent prompting strategy provides both efficiency gains (96\% fewer prompts compared to blind grid sampling) and quality improvements through SAM2's superior boundary delineation. Our method demonstrates competitive performance in the open-vocabulary setting, where the model segments objects from arbitrary text descriptions without any category-specific training.

Our results demonstrate the effectiveness of CLIP-guided prompting for efficient open-vocabulary segmentation, advancing the state-of-the-art while enabling practical text-driven image editing applications.