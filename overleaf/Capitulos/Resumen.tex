\vspace{2.5cm}

{\Large \bfseries Abstract}

\vspace{1.5cm}

Conventional semantic segmentation methods are limited to recognizing objects from a fixed set of categories, restricting their applicability in real-world scenarios where novel or unexpected concepts often arise. This thesis tackles the problem of open-vocabulary semantic segmentation and integrates it with generative image editing techniques, enabling flexible object discovery and manipulation based solely on natural language prompts.

Our approach begins by using a promptable segmentation model (SAM2) to produce a broad set of candidate masks without relying on predefined classes. To identify which of these masks correspond to a user-specified concept, we leverage a vision-language model (CLIP) to compute similarity scores between text embeddings and image patches. By aggregating these patch-level similarities, we assign each mask a semantic relevance score and retain only those most closely matching the prompt. This process enables the model to segment objects not explicitly known during training.

Once relevant masks are selected, we apply a state-of-the-art generative inpainting model (Stable Diffusion v2) to modify the segmented regions. This model, guided by text prompts, can remove, replace, or transform objects realistically, seamlessly integrating them into the scene. The resulting system transcends conventional fixed-label segmentation pipelines by empowering users to directly command both segmentation and image editing through natural language instructions.

% We validate our method using standard benchmarks and custom test cases, demonstrating its ability to handle novel concepts, produce accurate segmentation masks, and achieve high-quality, text-driven image modifications. The proposed framework expands the potential of semantic segmentation and inpainting, pointing toward more intuitive, adaptive, and user-centric image editing tools.