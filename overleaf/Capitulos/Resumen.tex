\vspace{2.5cm}

{\Large \bfseries Abstract}

\vspace{1.5cm}

Conventional semantic segmentation methods are limited to recognizing objects from a fixed set of categories, restricting their applicability in real-world scenarios where novel or unexpected concepts often arise. This thesis tackles the problem of open-vocabulary semantic segmentation by building upon dense CLIP-based methods and integrating high-quality mask refinement, enabling flexible semantic understanding based solely on natural language prompts.

Our primary approach builds upon SCLIP's (Self-attention CLIP) Cross-layer Self-Attention (CSA) mechanism for dense semantic prediction. We extend this foundation with a novel SAM2-based mask refinement layer that intelligently combines SCLIP's semantic understanding with SAM2's superior boundary quality. The refinement process uses prompted segmentation guided by SCLIP's confidence scores: extracting representative points from high-confidence regions to guide SAM2's mask generation, then applying majority voting to combine SCLIP's dense predictions with SAM2's refined boundaries. This annotation-free approach enables the model to segment both discrete objects and amorphous ``stuff'' classes (sky, road, vegetation) without requiring any labeled training data.

Once segmentation masks are obtained, we integrate a state-of-the-art generative inpainting model (Stable Diffusion v2) to enable text-driven image editing. This model can remove, replace, or transform segmented regions realistically, creating a complete pipeline from language-based semantic understanding to image manipulation.

We validate our dense SCLIP + SAM2 refinement approach on standard benchmarks including COCO-Stuff and PASCAL VOC 2012, demonstrating substantial improvements over baseline dense prediction methods. The novel SAM2 refinement layer provides significant quality gains while maintaining the annotation-free advantage of dense approaches.

Additionally, we explore an alternative proposal-based approach using SAM2 mask generation followed by CLIP scoring with multi-scale voting. While this method achieves strong performance on discrete object segmentation, our analysis reveals complementary strengths: the proposal-based approach excels at well-defined objects, while our primary dense method better handles semantic ``stuff'' classes and provides more comprehensive scene understanding.

Our results demonstrate the effectiveness of combining dense semantic prediction with intelligent mask refinement, advancing the state of open-vocabulary segmentation while enabling practical text-driven image editing applications.