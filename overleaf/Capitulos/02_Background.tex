\chapter{Background and Related Work} 
\label{ch:no_lineal}

This chapter provides a comprehensive overview of the foundational concepts and related work relevant to this thesis. It delves into the core areas of semantic segmentation, language models for vision, open-vocabulary semantic segmentation, mask generation models, and generative AI models for inpainting. In addition, it discusses recent advances in prompt learning, captioning approaches leveraging vision-language models, and large-scale open-vocabulary segmentation frameworks, ensuring a broad context for the subsequent chapters.

\section{Semantic Segmentation}
Semantic segmentation is a fundamental problem in computer vision that involves assigning a semantic label to each pixel in an image. Unlike image-level classification or object detection, semantic segmentation aims to produce a dense, pixel-wise prediction, thereby providing a fine-grained understanding of the scene. Early attempts at semantic segmentation often relied on handcrafted features and probabilistic models \cite{shotton2009textonboost}, but these approaches struggled with complex scenes and diverse object appearances.

The introduction of deep learning techniques revolutionized the field. Fully Convolutional Networks (FCNs) \cite{long2015fully} repurposed classification backbones \cite{krizhevsky2012imagenet,simonyan2014very,he2016deep} to produce dense predictions. Encoder-decoder architectures like U-Net \cite{ronneberger2015u} and SegNet \cite{badrinarayanan2017segnet} recovered high-resolution details through skip connections and stored pooling indices, respectively. Subsequent models focused on capturing richer context and multi-scale representations. PSPNet \cite{zhao2017pyramid} aggregated global context using pyramid pooling, while DeepLab \cite{chen2018encoder} employed atrous convolutions for flexible receptive fields. HRNet \cite{sun2019deep} maintained high-resolution features throughout the network to preserve spatial details.

Standard datasets such as PASCAL VOC \cite{everingham2010pascal} and MS COCO \cite{lin2014microsoft} drove progress and benchmarked performance. Yet, these methods typically operated under a closed-set assumption, relying on predefined categories. As real-world applications demand models that recognize novel objects, the limitations of closed vocabularies became evident, motivating the shift toward open-vocabulary semantic segmentation.

\section{Language Models for Vision}
Integrating language understanding into vision models extends their applicability and flexibility. Early efforts, such as DeViSE \cite{frome2013devise}, aligned visual features with semantic word embeddings to enable zero-shot image classification. This idea evolved into more complex systems for image captioning \cite{karpathy2015deep}, which learned to describe images with natural language sentences.

A significant breakthrough came with large-scale vision-language pretraining. CLIP \cite{radford2021learning} learned powerful joint embeddings for images and text from massive unlabeled web data, enabling zero-shot classification and a flexible semantic interface. ALIGN \cite{jia2021scaling} further scaled this approach, while BLIP \cite{li2022blip} unified vision-language understanding and generation under a single pretraining framework. Flamingo \cite{alayrac2022flamingo} explored few-shot adaptation scenarios by integrating large language models with visual backbones.

Parallel to these efforts, research into prompt learning refined how language models interface with vision tasks. Zhou et al. \cite{zhou2022learning} introduced methods to learn optimal prompts for vision-language models, improving their adaptability to downstream tasks. Mokady et al. \cite{mokady2021clipcap} leveraged CLIP features to guide image captioning (CLIPCap), showcasing how text prefixes conditioned on CLIP embeddings could steer generation toward semantically aligned outputs.

These vision-language innovations laid the groundwork for open-vocabulary segmentation, allowing models to understand and respond to arbitrary, user-defined textual queries.

\section{Open-Vocabulary Semantic Segmentation}
Open-vocabulary semantic segmentation aims to move beyond fixed taxonomies, enabling segmentation of arbitrary concepts specified by language prompts. Early works \cite{bucher2019zero} connected pixels to semantic embeddings but lacked the representation power of modern vision-language systems.

The advent of CLIP enabled robust open-vocabulary segmentation. LSeg \cite{li2022language}, GroupViT \cite{xu2022groupvit}, and OpenSeg \cite{ghiasi2022scaling} demonstrated CLIP's effectiveness for language-driven segmentation through different strategies: adapting embeddings, emerging segmentation from text supervision, and scaling with image-level labels. Dense prediction methods followed: CLIPSeg \cite{luddecke2022clipseg} extended CLIP with transformer decoders, while MaskCLIP \cite{zhou2022extract} and MasQCLIP \cite{xu2023masqclip} extracted dense labels without additional training.

Recent training-free methods achieve competitive performance by modifying CLIP's architecture rather than fine-tuning. SCLIP \cite{sclip2024} replaces self-attention with correlative mechanisms for spatial-covariant features. NACLIP \cite{naclip2025} uses Gaussian windows for local coherence, CaR \cite{huang2022car} applies class-aware regularizations, and ITACLIP \cite{shao2024itaclip} combines architectural modifications with engineered prompts for state-of-the-art training-free performance. Advanced approaches like X-Decoder \cite{zou2023xdecoder}, ODISE \cite{xu2023odise}, CAT-Seg \cite{cho2024catseg}, and LMSeg \cite{LMSeg2024} further push boundaries through unified decoding, diffusion integration, and cost aggregation.

\subsection{Detection-Based Pipelines}
An alternative paradigm combines open-vocabulary detectors with promptable segmentation. GroundingDINO \cite{groundingdino2023} and OWL-ViT \cite{minderer2022owlvit}/OWLv2 \cite{minderer2023owlv2} detect objects from text prompts (achieving 52.5 AP on COCO zero-shot), which SAM \cite{kirillov2023segment} refines into precise masks. Grounded-SAM \cite{grounded_sam_2024} formalizes this two-stage pipeline (48.7 mAP on SegInW). While computationally efficient through sparse predictions, these approaches inherit detection limitations: missed objects, coarse localization, and inability to segment amorphous "stuff" classes. Dense prediction methods (explored in this thesis) avoid these issues by directly producing pixel-wise predictions, though at higher computational cost—motivating the intelligent prompting strategies developed in Chapter \ref{ch:methodology}.

\section{Large Model Approaches to Open-Vocabulary Segmentation}

A recent direction leverages Large Language Models (LLMs) and Large Multimodal Models (LMMs) for complex reasoning-based segmentation. LISA (Large Language Instructed Segmentation Assistant) \cite{lai2024lisa} integrates a large language model with a vision encoder and segmentation decoder, enabling referring segmentation through natural language reasoning. Unlike traditional open-vocabulary methods that primarily match text embeddings to visual features, LISA can process complex linguistic instructions involving spatial relationships, multiple objects, and contextual reasoning (e.g., "segment the person standing to the left of the red car"). PixelLM \cite{ren2023pixellm} employs a lightweight pixel decoder with a comprehensive segmentation codebook, achieving state-of-the-art results on pixel-level reasoning tasks while leveraging GPT-4V-aided data curation to create high-quality training datasets. F-LMM \cite{flmm2025} demonstrates that frozen large multimodal models can be grounded for referring expression segmentation while preserving conversational abilities, enabling complex tasks like reasoning segmentation and visual chain-of-thought reasoning.

While these large model approaches demonstrate impressive capabilities on complex reasoning queries and exhaustive instance detection, they require significantly more computational resources. LISA employs billion-parameter language models, while PixelLM and F-LMM similarly rely on large-scale multimodal backbones. This contrasts with lightweight vision-language approaches like CLIP-based methods, which operate with substantially smaller models and faster inference times.

The choice between large model and lightweight approaches depends on application requirements: complex reasoning tasks justify the computational overhead of LMMs, while efficiency-critical applications favor lighter architectures. This thesis explores the latter paradigm, developing intelligent prompting strategies that achieve competitive accuracy on standard benchmarks while maintaining practical deployment on consumer hardware.

\section{Mask Generation Models}
Mask generation models produce accurate object or region delineations and serve as a backbone for many segmentation systems. Mask R-CNN \cite{he2017mask} extended object detection frameworks to instance segmentation, while Mask2Former \cite{cheng2022mask2former} unified semantic, instance, and panoptic segmentation using a transformer-based design.

The Segment Anything Model (SAM) \cite{kirillov2023segment} marked a significant shift toward prompt-driven segmentation. Trained on a vast and diverse dataset (SA-1B with over 1 billion masks), SAM can segment virtually any object when provided with a suitable prompt—points, boxes, or text—making it particularly versatile for zero-shot generalization. Building upon this foundation, SAM 2 \cite{ravi2024sam2} extended these capabilities to video segmentation, introducing a memory mechanism that enables consistent object tracking across frames. SAM 2 achieves superior accuracy while requiring fewer interactions and operates at real-time speeds (approximately 44 frames per second), making it highly practical for both image and video applications.

By integrating SAM or SAM 2 with open-vocabulary embeddings from CLIP or related models, one can achieve promptable, zero-shot segmentation of arbitrary categories. This synergy of mask generation with vision-language models unlocks flexible and dynamic segmentation capabilities essential for downstream applications.

\section{Generative AI Models for Inpainting}
Generative inpainting models fill masked image regions with plausible, contextually coherent content. Before deep learning, patch-based methods \cite{criminisi2004region} searched for suitable patches to fill holes, but lacked semantic understanding. Context Encoders \cite{pathak2016context} introduced a learning-based approach, using convolutional neural networks and adversarial training to predict missing regions. Subsequent improvements like Partial Convolutions \cite{liu2018image}, Gated Convolutions \cite{yu2019free}, and attention-based models \cite{yu2018generative} enhanced robustness and image fidelity.

The latest generation of inpainting models leverages powerful generative architectures and large-scale training. Stable Diffusion \cite{rombach2022high} employs latent diffusion models to produce high-resolution, semantically consistent completions guided by textual prompts. DALL·E 2 \cite{ramesh2022hierarchical} similarly enables text-driven modifications, allowing users to describe desired changes in natural language. Integrating such generative models with open-vocabulary segmentation and promptable mask generation (e.g., SAM) enables unprecedented levels of interactivity: users can identify segments of interest and instruct the model to add, remove, or alter objects via textual commands.

This combination of open-vocabulary segmentation and generative inpainting lays the foundation for next-generation image editing tools, capable of fluidly responding to a broad range of user-defined concepts and transformations.

\section{Video Inpainting Models}
Video inpainting extends image inpainting to the temporal domain, requiring both semantic plausibility and temporal consistency across frames. The field has evolved through two main paradigms: flow-based propagation and diffusion-based generation.

Flow-based approaches propagate information from neighboring frames using optical flow. E2FGVI \cite{li2022towards} introduced end-to-end flow-guided video inpainting with dual-stream architecture (content hallucination and flow completion), achieving 15× speedup over prior methods. ProPainter \cite{zhou2023propainter} improved upon this with dual-domain propagation, combining image propagation with feature propagation through a mask-guided sparse video transformer. While computationally efficient, these methods struggle with large occlusions, complex motion, and long-term temporal consistency.

Diffusion-based approaches leverage generative priors from large-scale diffusion models. DiffuEraser \cite{diffueraser2025} and DiTPainter \cite{ditpainter2025} employ diffusion transformers for highly realistic and temporally coherent results, though at higher computational cost. VACE (Video All-in-one Creation and Editing) \cite{vace2025} represents the current state-of-the-art, unifying inpainting, editing, and generation in a single framework trained on large-scale video data. VACE enables text-driven video manipulation through natural language prompts, making it particularly suitable for integration with open-vocabulary segmentation.

The combination of SAM 2's temporal tracking for video segmentation with VACE's text-driven inpainting creates a complete pipeline for language-guided video editing: users can specify objects via text prompts (segmented by CLIP + SAM 2), and VACE handles their removal, replacement, or modification while maintaining temporal consistency across frames.