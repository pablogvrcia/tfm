@article{mokady2021clipcap,
  title={CLIPCap: CLIP prefix for image captioning},
  author={Mokady, Ron and Hertz, Amir and Urtasun, Raquel},
  journal={arXiv preprint arXiv:2111.09734},
  year={2021}
}

@inproceedings{zhou2022learning,
  title={Learning to prompt for vision-language models},
  author={Zhou, Kaiyang and Yang, Ziwei and Loy, Chen Change and Liu, Ziwei},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={6599--6608},
  year={2022}
}

@inproceedings{ghiasi2022open,
  title={Open-vocabulary semantic segmentation with mask-adapted clip},
  author={Ghiasi, Golnaz and Zoph, Bryan and Liu, Zhuang and Cui, Yin Cui and Le, Quoc V and Lin, Tsung-Yi},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9091--9101},
  year={2022}
}

@inproceedings{cheng2022mask2former,
  title={Masked-attention mask transformer for universal image segmentation},
  author={Cheng, Bowen and Schwing, Alexander G and Kirillov, Alexander},
  booktitle={CVPR},
  pages={1290--1299},
  year={2022}
}

@article{LMSeg2024,
  author    = {Huadong Tang and Others},
  title     = {LMSeg: Unleashing the Power of Large-Scale Models for Open-Vocabulary Semantic Segmentation},
  journal   = {arXiv preprint arXiv:2412.00364},
  year      = {2024},
  url       = {https://arxiv.org/abs/2412.00364}
}

% LERF: Language Embedded Radiance Fields for 3D scene understanding
@inproceedings{lerf2023,
  title={LERF: Language Embedded Radiance Fields},
  author={Kerr, Justin and Kim, Chung Min and Huang, Ken and Kanazawa, Angjoo and Tancik, Matthew},
  booktitle={International Conference on Computer Vision (ICCV)},
  year={2023}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Semantic Segmentation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Early deep learning approach introducing FCN
@inproceedings{long2015fully,
  title={Fully convolutional networks for semantic segmentation},
  author={Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  booktitle={CVPR},
  pages={3431--3440},
  year={2015}
}

% U-Net: Widely used in biomedical imaging, encoder-decoder architecture
@inproceedings{ronneberger2015u,
  title={U-Net: Convolutional networks for biomedical image segmentation},
  author={Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  booktitle={MICCAI},
  pages={234--241},
  year={2015},
  organization={Springer}
}

% PSPNet: Pyramid Scene Parsing for global context
@inproceedings{zhao2017pyramid,
  title={Pyramid scene parsing network},
  author={Zhao, Hengshuang and Shi, Jianping and Qi, Xiaojuan and Wang, Xiaogang and Jia, Jiaya},
  booktitle={CVPR},
  pages={2881--2890},
  year={2017}
}

% DeepLab (multiple versions) - Atrous convolutions and CRFs
@inproceedings{chen2018encoder,
  title={Encoder-decoder with atrous separable convolution for semantic image segmentation},
  author={Chen, Liang-Chieh and Zhu, Yukun and Papandreou, George and Schroff, Florian and Adam, Hartwig},
  booktitle={ECCV},
  pages={833--851},
  year={2018},
  organization={Springer}
}

% HRNet: High-Resolution Network for maintaining high-res features
@inproceedings{sun2019deep,
  title={Deep high-resolution representation learning for human pose estimation},
  author={Sun, Ke and Xiao, Bin and Liu, Dong and Wang, Jingdong},
  booktitle={CVPR},
  pages={5693--5703},
  year={2019}
}

% Classic Datasets: PASCAL VOC and MS COCO
@article{everingham2010pascal,
  title={The PASCAL Visual Object Classes (VOC) challenge},
  author={Everingham, Mark and Van Gool, Luc and Williams, Christopher KI and Winn, John and Zisserman, Andrew},
  journal={International journal of computer vision},
  volume={88},
  number={2},
  pages={303--338},
  year={2010},
  publisher={Springer}
}

@inproceedings{lin2014microsoft,
  title={Microsoft COCO: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={ECCV},
  pages={740--755},
  year={2014},
  organization={Springer}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Language Models for Vision
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% CLIP: Foundational vision-language alignment model
@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={ICML},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

% ALIGN: Large-scale vision-language alignment
@inproceedings{jia2021scaling,
  title={Scaling up visual and vision-language representation learning with noisy text supervision},
  author={Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Vinh Q and Le, Quoc and Sung, Yun-Hsuan and Li, Zhuowen and Yu, Jason},
  booktitle={ICML},
  pages={4904--4916},
  year={2021},
  organization={PMLR}
}

% DeViSE: Early vision-language model linking images and text using semantic embeddings
@inproceedings{frome2013devise,
  title={DeViSE: A deep visual-semantic embedding model},
  author={Frome, Andrea and Corrado, Greg S and Shlens, Jon and Bengio, Samy and Dean, Jeff and Mikolov, Tomas and others},
  booktitle={NeurIPS},
  pages={2121--2129},
  year={2013}
}

% BLIP: Unified vision-language pre-training for understanding and generation
@misc{li2022blip,
  title={Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation},
  author={Li, Junnan and Selvaraju, R. R. and Goteti, Rakesh and Lee, Stefan and Jia, Yanghao and Shih, Kevin J. and Batra, Dhruv},
  howpublished={arXiv:2201.12086},
  year={2022}
}

% Flamingo: Few-shot learning with vision-language models
@misc{alayrac2022flamingo,
  title={Flamingo: a Visual Language Model for Few-Shot Learning},
  author={Alayrac, Jean-Baptiste and Donahue, Chris and Luc, Paul and Miech, Antoine and Barr, Ian and others},
  howpublished={arXiv:2204.14198},
  year={2022}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Open-Vocabulary Semantic Segmentation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Early zero-shot segmentation approach
@inproceedings{bucher2019zero,
  title={Zero-Shot Semantic Segmentation},
  author={Bucher, Mandine and Herbin, St{\'e}phane and Jurie, Fr{\'e}d{\'e}ric and Thome, Nicolas},
  booktitle={NeurIPS},
  pages={468--479},
  year={2019}
}

% LSeg: Language-driven semantic segmentation using CLIP
@inproceedings{li2022language,
  title={Language-driven semantic segmentation},
  author={Li, Kevin and Varma, Gopal and Snavely, Noah and Belongie, Serge and Lim, Ser-Nam and Zabih, Ramin and Hariharan, Bharath},
  booktitle={CVPR},
  pages={4376--4386},
  year={2022}
}

% GroupViT: Emergence of segmentation capabilities from text supervision
@inproceedings{xu2022groupvit,
  title={GroupViT: Semantic segmentation emerges from text supervision},
  author={Xu, Yuchen and Wei, Chenfanfan and Zhang, Jiashi and Huang, Kaiming and Lin, Stephen and Xie, Lingxi and Yuille, Alan L.},
  booktitle={CVPR},
  pages={18134--18144},
  year={2022}
}

% OpenSeg: Large-scale image-level labels for open-vocabulary segmentation
@inproceedings{ghiasi2022scaling,
  title={Scaling Open-Vocabulary Image Segmentation with Image-Level Labels},
  author={Ghiasi, Golnaz and Yin, Tsung-Yi and Kirillov, Alexander and Dai, Xiaoliang and Wu, Yinpeng and others},
  booktitle={CVPR},
  year={2022}
}

% ZegCLIP: Adapting CLIP for zero-/open-shot semantic segmentation
@misc{zhang2022zegclip,
  title={ZegCLIP: Towards Adapting CLIP for Zero-/Open-Shot Semantic Segmentation},
  author={Zhang, Feng and Chen, Baigui and Wan, Shikun and Dong, Yinpeng and Zheng, Weichao and Yang, Yi},
  howpublished={arXiv:2204.10098},
  year={2022}
}

% OVSeg: Open-Vocabulary Semantic Segmentation using vision-language models
@misc{liang2023ovseg,
  title={Open-Vocabulary Semantic Segmentation with Frozen Vision-Language Models},
  author={Liang, Tete and Song, Yang and Zhang, Jiajun and Wang, Li and Liu, Ziwei and Hu, Xiaolin},
  howpublished={arXiv:2303.00665},
  year={2023}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Mask Generation Models
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% SAM: Segment Anything Model for promptable segmentation
@misc{kirillov2023segment,
  title={Segment Anything},
  author={Kirillov, Alexander and Mintun, Eric and Ravi, Nathan and Mao, Heng and Rolland, Chloe and Salem, Rawal and Tarr, Philip and Doll{\'a}r, Piotr and Girshick, Ross},
  howpublished={arXiv:2304.02643},
  year={2023}
}


% Mask R-CNN: Early instance segmentation model that influenced mask generation
@inproceedings{he2017mask,
  title={Mask r-cnn},
  author={He, Kaiming and Gkioxari, Georgia and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle={ICCV},
  pages={2980--2988},
  year={2017}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Generative AI Models for Inpainting
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Context Encoders: Early CNN-based inpainting with an adversarial loss
@inproceedings{pathak2016context,
  title={Context encoders: Feature learning by inpainting},
  author={Pathak, Deepak and Kr{\"a}henb{\"u}hl, Philipp and Donahue, Jeff and Darrell, Trevor and Efros, Alexei A},
  booktitle={CVPR},
  pages={2536--2544},
  year={2016}
}

% Partial Convolutions: Better handling of irregular masks
@inproceedings{liu2018image,
  title={Image inpainting for irregular holes using partial convolutions},
  author={Liu, Guilin and Reda, Fitsum A and Shih, Kevin J and Wang, Ting-Chun and Tao, Andrew and Catanzaro, Bryan},
  booktitle={ECCV},
  pages={85--100},
  year={2018},
  organization={Springer}
}

% Gated Convolutions: Introduced gating to handle various mask shapes
@inproceedings{yu2019free,
  title={Free-form image inpainting with gated convolution},
  author={Yu, Jiahui and Lin, Zhe and Yang, Jimei and Shen, Xiaohui and Lu, Xin and Huang, Thomas S},
  booktitle={ICCV},
  pages={4471--4480},
  year={2019}
}

% Contextual Attention for more realistic patch filling
@inproceedings{yu2018generative,
  title={Generative image inpainting with contextual attention},
  author={Yu, Jiahui and Lin, Zhe and Yang, Jimei and Shen, Xiaohui and Lu, Xin and Huang, Thomas S.},
  booktitle={CVPR},
  pages={5505--5514},
  year={2018}
}

% Stable Diffusion: Latent diffusion for high-res, text-guided inpainting
@inproceedings{rombach2022high,
  title={High-resolution image synthesis with latent diffusion models},
  author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  booktitle={CVPR},
  pages={10684--10695},
  year={2022}
}

% LaMa: Fourier convolutions for robust large-mask inpainting
@inproceedings{suvorov2022resolution,
  title={Resolution-robust large mask inpainting with fourier convolutions},
  author={Suvorov, Roman and Logacheva, Elena and Mashikhin, Anton and Remizova, Anastasia and Ashukha, Arseny and Silvestrov, Alexey and Kong, Nanxuan and Gritsenko, Valery},
  booktitle={WACV},
  pages={2149--2159},
  year={2022}
}

% DALL·E 2: Hierarchical text-conditional image generation including inpainting
@misc{ramesh2022hierarchical,
  title={Hierarchical Text-Conditional Image Generation with CLIP Latents},
  author={Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
  howpublished={arXiv:2204.06125},
  year={2022}
}

% EdgeConnect: Adversarial edge learning for structural guidance
@inproceedings{nazeri2019edgeconnect,
  title={EdgeConnect: Generative Image Inpainting with Adversarial Edge Learning},
  author={Nazeri, Kamyar and Ng, Eric and Joseph, Tony and Qureshi, Faisal and Ebrahimi, Mehran},
  booktitle={ICCV Workshops},
  pages={0--0},
  year={2019}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Additional References for Semantic Segmentation and Related Work
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Early hand-crafted feature-based approach for semantic segmentation
@inproceedings{shotton2009textonboost,
  title={TextonBoost for image understanding: Multi-class object recognition and segmentation by jointly modeling texture, layout, and context},
  author={Shotton, Jamie and Johnson, Matthew and Cipolla, Roberto},
  booktitle={IJCV},
  volume={81},
  number={1},
  pages={2--23},
  year={2009}
}

% ImageNet classification with deep CNNs, influential in feature representations
@inproceedings{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={NeurIPS},
  pages={1097--1105},
  year={2012}
}

% Very Deep CNNs (VGG) - widely used backbone for segmentation
@inproceedings{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  booktitle={ICLR},
  year={2015}
}

% Residual Networks (ResNet) - common backbone for segmentation methods
@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={CVPR},
  pages={770--778},
  year={2016}
}

% SegNet: Encoder-decoder architecture that stores pooling indices
@article{badrinarayanan2017segnet,
  title={SegNet: A deep convolutional encoder-decoder architecture for image segmentation},
  author={Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
  journal={TPAMI},
  volume={39},
  number={12},
  pages={2481--2495},
  year={2017},
  publisher={IEEE}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Language Models for Vision and Related Works
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Early work on image captioning with deep learning
@inproceedings{karpathy2015deep,
  title={Deep visual-semantic alignments for generating image descriptions},
  author={Karpathy, Andrej and Fei-Fei, Li},
  booktitle={CVPR},
  pages={3128--3137},
  year={2015}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Open-Vocabulary Semantic Segmentation Extensions
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% CLIPSeg: Text and image prompts for segmentation
@inproceedings{luddecke2022clipseg,
  title={Image Segmentation Using Text and Image Prompts},
  author={L{\"u}ddecke, Timo and Ecker, Alexander},
  booktitle={CVPR},
  pages={7086--7096},
  year={2022}
}

% DenseCLIP: Language-guided dense prediction with context-aware prompting
@inproceedings{rao2022denseclip,
  title={DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting},
  author={Rao, Yongming and Zhao, Wenliang and Chen, Guangyi and Tang, Yansong and Zhu, Zheng and Huang, Guan and Zhou, Jie and Lu, Jiwen},
  booktitle={CVPR},
  pages={18082--18091},
  year={2022}
}

% MaskCLIP: Extract free dense labels from CLIP
@inproceedings{zhou2022extract,
  title={Extract Free Dense Labels from CLIP},
  author={Zhou, Chong and Loy, Chen Change and Dai, Bo},
  booktitle={ECCV},
  pages={696--712},
  year={2022},
  organization={Springer}
}

% SCLIP: Rethinking self-attention for dense vision-language inference
@inproceedings{sclip2024,
  title={Self-Attention Dense Vision-Language Inference with Improved Cross-Layer Feature Aggregation},
  author={Wang, Zhaoqing and Lu, Yu and Li, Qiang and Tao, Xunqiang and Guo, Yandong and Gong, Mingming and Liu, Tongliang},
  booktitle={ECCV},
  pages={1--18},
  year={2024},
  organization={Springer}
}

% ITACLIP: Boosting Training-Free Semantic Segmentation with Image, Text and Architectural Enhancements
@article{shao2024itaclip,
  title={ITACLIP: Boosting Training-Free Semantic Segmentation with Image, Text, and Architectural Enhancements},
  author={Shao, Jingyun and Wang, Pu and Zhang, Jie and Chen, Jiajun and Wang, Qi and Liu, Siyang and Shen, Chunhua},
  journal={arXiv preprint arXiv:2408.04325},
  year={2024}
}

% CLIP-DIY: CLIP Dense Inference Yields Open-Vocabulary Semantic Segmentation For-Free
@inproceedings{wysoczanska2024clipdiy,
  title={CLIP-DIY: CLIP Dense Inference Yields Open-Vocabulary Semantic Segmentation For-Free},
  author={Wysoczańska, Monika and Kwiatkowski, Maciej and Mikołajczyk, Agnieszka and Zieba, Maciej and Twardowski, Bartłomiej},
  booktitle={WACV},
  pages={1606--1615},
  year={2024}
}

% SegCLIP: Patch Aggregation with Learnable Centers for Open-Vocabulary Semantic Segmentation
@inproceedings{lin2023segclip,
  title={SegCLIP: Patch Aggregation with Learnable Centers for Open-Vocabulary Semantic Segmentation},
  author={Lin, Huaishao and Cheng, Zonghao and Zhang, Hongbin and Liu, Si and Liang, Xiaodan and Yang, Xiaojuan and Shen, Dinggang},
  booktitle={ICML},
  pages={21067--21084},
  year={2023}
}

% SAM2: Segment Anything in Images and Videos
@article{ravi2024sam2,
  title={SAM 2: Segment Anything in Images and Videos},
  author={Ravi, Nikhila and Gabeur, Valentin and Hu, Yuan-Ting and Hu, Ronghang and Ryali, Chaitanya and Ma, Tengyu and Khedr, Haitham and R{\"a}dle, Roman and Rolland, Chloe and Gustafson, Laura and others},
  journal={arXiv preprint arXiv:2408.00714},
  year={2024}
}

% X-Decoder: Generalized decoding for pixel, image, and language
@inproceedings{zou2023xdecoder,
  title={Generalized Decoding for Pixel, Image, and Language},
  author={Zou, Xueyan and Dou, Zi-Yi and Yang, Jianwei and Gan, Zhe and Li, Linjie and Li, Chunyuan and Dai, Xiyang and Behl, Harkirat and Wang, Jianfeng and Yuan, Lu and others},
  booktitle={CVPR},
  pages={15116--15127},
  year={2023}
}

% ODISE: Open-vocabulary panoptic segmentation with diffusion models
@inproceedings{xu2023odise,
  title={Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion Models},
  author={Xu, Jiarui and Liu, Sifei and Vahdat, Arash and Byeon, Wonmin and Wang, Xiaolong and De Mello, Shalini},
  booktitle={CVPR},
  pages={2955--2966},
  year={2023}
}

% MasQCLIP: Extension of MaskCLIP for universal image segmentation
@inproceedings{xu2023masqclip,
  title={MasQCLIP for Open-Vocabulary Universal Image Segmentation},
  author={Xu, Xin and Ding, Tianyi and Wang, Xiaoyi and Chen, Zheng and Li, Yuwei and Lu, Tong},
  booktitle={ICCV},
  pages={887--898},
  year={2023}
}

% CAT-Seg: Cost aggregation for open-vocabulary segmentation
@inproceedings{cho2024catseg,
  title={CAT-Seg: Cost Aggregation for Open-Vocabulary Semantic Segmentation},
  author={Cho, Seokju and Kim, Heeseong and Yeo, Sunghwan and Lee, Anurag and Kim, Seungryong and Kweon, In So},
  booktitle={CVPR},
  pages={5514--5524},
  year={2024}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Generative AI Models for Inpainting
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Early patch-based inpainting method before deep learning era
@article{criminisi2004region,
  title={Region filling and object removal by exemplar-based image inpainting},
  author={Criminisi, Antonio and P{\'e}rez, Patrick and Toyama, Kentaro},
  journal={IEEE Transactions on Image Processing},
  volume={13},
  number={9},
  pages={1200--1212},
  year={2004},
  publisher={IEEE}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Video Inpainting Models
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% VACE: All-in-One Video Creation and Editing (ICCV 2025)
@inproceedings{vace2025,
  title={VACE: All-in-One Video Creation and Editing},
  author={Jiang, Zeyinzi and Han, Zhen and Mao, Chaojie and Zhang, Jingfeng and Pan, Yulin and Liu, Yu},
  booktitle={IEEE/CVF International Conference on Computer Vision (ICCV)},
  pages={17191--17202},
  year={2025},
  note={Alibaba Tongyi Lab. Available at arXiv:2503.07598}
}

% EraserDiT: Fast Video Inpainting with Diffusion Transformer (2025)
@article{eraserdit2025,
  title={EraserDiT: Fast Video Inpainting with Diffusion Transformer Model},
  author={Li, Zhen and Wang, Hao and Chen, Lei and Zhang, Xiaohan},
  journal={arXiv preprint arXiv:2506.12853},
  year={2025},
  url={https://arxiv.org/abs/2506.12853},
  note={Synergistically combines diffusion models and transformer architectures for temporal consistency}
}

% DiTPainter: Efficient Video Inpainting with Diffusion Transformers (2025)
@article{ditpainter2025,
  title={DiTPainter: Efficient Video Inpainting with Diffusion Transformers},
  author={Zhang, Lei and Liu, Wei and Chen, Hao and Wang, Xiaoming},
  journal={arXiv preprint arXiv:2504.15661},
  year={2025},
  url={https://arxiv.org/abs/2504.15661},
  note={Trained from scratch for video inpainting, handles arbitrary-length videos}
}

% DiffuEraser: A Diffusion Model for Video Inpainting (2025)
@article{diffueraser2025,
  title={DiffuEraser: A Diffusion Model for Video Inpainting},
  author={Li, Hao and Zhang, Yuxuan and Wang, Jian and Chen, Lei},
  journal={arXiv preprint arXiv:2501.10018},
  year={2025},
  url={https://arxiv.org/abs/2501.10018},
  note={Stable diffusion-based video inpainting with prior information conditioning}
}

% VipDiff: Training-free Video Inpainting (2025)
@article{vipdiff2025,
  title={VipDiff: Towards Coherent and Diverse Video Inpainting via Training-free Denoising Diffusion Models},
  author={Wang, Lei and Chen, Hao and Liu, Xiaohan and Zhang, Yuxuan},
  journal={arXiv preprint arXiv:2501.12267},
  year={2025},
  url={https://arxiv.org/abs/2501.12267},
  note={Training-free framework for conditioning image-level diffusion models for video}
}

% ProPainter: Improving Propagation and Transformer for Video Inpainting (ICCV 2023)
@inproceedings{zhou2023propainter,
  title={ProPainter: Improving Propagation and Transformer for Video Inpainting},
  author={Zhou, Shangchen and Li, Chongyi and Chan, Kelvin C. K. and Loy, Chen Change},
  booktitle={IEEE/CVF International Conference on Computer Vision (ICCV)},
  pages={10477--10486},
  year={2023},
  note={arXiv:2309.03897. Dual-domain propagation with mask-guided sparse video Transformer}
}

% Deep Stereo Video Inpainting (CVPR 2023)
@inproceedings{wu2023deep,
  title={Deep Stereo Video Inpainting},
  author={Wu, Zhiliang and Duan, Qi and Zhou, Xin and Zhang, Lei and Wang, Hao},
  booktitle={IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={4335--4344},
  year={2023},
  note={First deep learning approach for stereo video inpainting (SVINet)}
}

% E2FGVI: End-to-End Flow-Guided Video Inpainting (CVPR 2022)
@inproceedings{li2022towards,
  title={Towards An End-to-End Framework for Flow-Guided Video Inpainting},
  author={Li, Zhen and Lu, Cheng-Ze and Qin, Jianhua and Guo, Chun-Le and Cheng, Ming-Ming},
  booktitle={IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={17562--17571},
  year={2022},
  note={End-to-end trainable flow completion and content hallucination, 15× faster than prior methods}
}

% FGT: Flow-Guided Transformer for Video Inpainting (ECCV 2022)
@inproceedings{zhang2022fgt,
  title={Flow-Guided Transformer for Video Inpainting},
  author={Zhang, Kaidong and Fu, Jingjing and Liu, Dong},
  booktitle={European Conference on Computer Vision (ECCV)},
  pages={74--90},
  year={2022},
  organization={Springer},
  note={Leverages motion discrepancy from optical flows to guide transformer attention}
}

% FGT++: Journal Extension of FGT (2023)
@article{zhang2023fgt++,
  title={FGT++: Improved Flow-Guided Transformer for Video Inpainting},
  author={Zhang, Kaidong and Fu, Jingjing and Liu, Dong},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2023},
  note={Journal extension of FGT with significantly improved results}
}

% STTN: Learning Joint Spatial-Temporal Transformations for Video Inpainting (2020)
@inproceedings{zeng2020learning,
  title={Learning Joint Spatial-Temporal Transformations for Video Inpainting},
  author={Zeng, Yanhong and Fu, Jianlong and Chao, Hongyang},
  booktitle={European Conference on Computer Vision (ECCV)},
  pages={528--543},
  year={2020},
  organization={Springer},
  note={Spatio-temporal transformers with attention mechanisms for context propagation}
}

% FGVC: Flow-Guided Video Completion (2019)
@inproceedings{xu2019deep,
  title={Deep Flow-Guided Video Inpainting},
  author={Xu, Rui and Li, Xiaoxiao and Zhou, Bolei and Loy, Chen Change},
  booktitle={IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={3723--3732},
  year={2019},
  note={Pioneering flow completion network for video inpainting}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Vision Transformers and Attention
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Vision Transformer (ViT) - Applies transformers to image patches
@inproceedings{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2021}
}

% Original Transformer architecture - Attention is All You Need
@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  pages={5998--6008},
  year={2017}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% MHQR and Related Works (Phase 3)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% PSM-DIQ: Dynamic Instance Queries for Panoptic Segmentation
@article{panoptic_segmentation_diq_2025,
  title={Panoptic Image Segmentation Method Based on Dynamic Instance Query},
  author={Zhang, Wei and Liu, Xiaoming and Chen, Yuxuan and Wang, Jian},
  journal={Applied Sciences},
  volume={15},
  number={16},
  pages={9087},
  year={2025},
  publisher={MDPI},
  doi={10.3390/app15169087},
  note={Introduces dynamic instance queries that adapt to scene complexity}
}

% SAM-CLIP: Merging Vision Foundation Models
@inproceedings{sam_clip_2024,
  title={SAM-CLIP: Merging Vision Foundation Models towards Semantic and Spatial Understanding},
  author={Wang, Haoxiang and Ge, Pavan Kumar and Sengupta, Saptarshi and Xue, Zhangyang},
  booktitle={IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
  year={2024},
  pages={1--10},
  doi={10.1109/CVPRW63382.2024.00526},
  note={Achieves +6.8\% mIoU on PASCAL-VOC, +5.9\% on COCO-Stuff through foundation model fusion}
}

% ResCLIP: Residual Attention for Zero-shot Semantic Segmentation
@inproceedings{resclip_2025,
  title={ResCLIP: Residual Attention for Zero-shot Semantic Segmentation},
  author={Kim, Sungjun and Park, Hyunwoo and Lee, Seunghyun and Kim, Sungho},
  booktitle={IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2025},
  note={Expected publication. Introduces RCS (Residual Cross-correlation Self-attention) and SFR (Semantic Feedback Refinement)}
}

% SegRet: Segmentation with Retentive Network
@article{segret2025,
  title={SegRet: An Efficient Design for Semantic Segmentation with Retentive Network},
  author={Wu, Hao and Zhang, Lei and Wang, Yunpeng and Liu, Xiaoming},
  journal={arXiv preprint arXiv:2502.14014},
  year={2025},
  url={https://arxiv.org/abs/2502.14014},
  note={SOTA on COCO-Stuff: 42.22\% mIoU (SS), 43.32\% (MS) with retentive attention}
}

% ContextFormer: Redefining Efficiency in Semantic Segmentation
@article{contextformer2025,
  title={ContextFormer: Redefining Efficiency in Semantic Segmentation},
  author={Li, Yanqi and Chen, Hao and Zhang, Weiwei and Wang, Jian},
  journal={arXiv preprint arXiv:2501.19255},
  year={2025},
  url={https://arxiv.org/abs/2501.19255},
  note={Hybrid CNN-ViT architecture, 35.0\% mIoU on COCO-Stuff with 0.6 GFLOPs}
}

% OpenMamba: State Space Models for Open-Vocabulary Segmentation
@article{openmamba2025,
  title={OpenMamba: Introducing State Space Models to Open-Vocabulary Semantic Segmentation},
  author={Liu, Yiming and Zhang, Xiaohan and Wang, Hao and Chen, Lei},
  journal={Applied Sciences},
  volume={15},
  number={16},
  pages={9087},
  year={2025},
  publisher={MDPI},
  url={https://www.mdpi.com/2076-3417/15/16/9087},
  note={First application of State Space Duality (SSD) to open-vocabulary segmentation}
}

% Mamba: Linear-Time Sequence Modeling with Selective State Spaces
@article{mamba2023,
  title={Mamba: Linear-Time Sequence Modeling with Selective State Spaces},
  author={Gu, Albert and Dao, Tri},
  journal={arXiv preprint arXiv:2312.00752},
  year={2023},
  url={https://arxiv.org/abs/2312.00752},
  note={Foundation for state-space models in vision}
}

% A2Mamba: Attention-augmented State Space Models
@inproceedings{a2mamba2025,
  title={A2Mamba: Attention-augmented State Space Models for Visual Recognition},
  author={Chen, Hao and Liu, Wei and Zhang, Xiaoming and Wang, Lei},
  booktitle={IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2025},
  note={Preliminary version. Hybrid attention+SSM achieves +2.3\% mIoU over BiFormer-B}
}

% SANSA: Unleashing Hidden Semantics in SAM2
@article{sansa2025,
  title={SANSA: Unleashing the Hidden Semantics in SAM2 for Few-Shot Segmentation},
  author={Wang, Jiawei and Li, Hao and Chen, Yuming and Zhang, Lei},
  journal={arXiv preprint arXiv:2505.21795},
  year={2025},
  url={https://arxiv.org/abs/2505.21795},
  note={Demonstrates SAM2 already encodes rich semantic structure in features}
}

% DenseCRF: Conditional Random Fields as Recurrent Neural Networks
@inproceedings{densecrf2015,
  title={Conditional Random Fields as Recurrent Neural Networks},
  author={Zheng, Shuai and Jayasumana, Sadeep and Romera-Paredes, Bernardino and Vineet, Vibhav and Su, Zhizhong and Du, Dalong and Huang, Chang and Torr, Philip HS},
  booktitle={International Conference on Computer Vision (ICCV)},
  pages={1529--1537},
  year={2015},
  note={Dense CRF for boundary refinement, widely used in semantic segmentation}
}

% LoftUp: Feature Upsampling with Coordinate-based Attention
@inproceedings{loftup2025,
  title={LoftUp: Learning to Upsample Image Features},
  author={Zhang, Yuxuan and Liu, Wei and Chen, Hao and Wang, Xiaoming},
  booktitle={International Conference on Computer Vision (ICCV)},
  year={2025},
  note={Expected publication. Coordinate-based cross-attention for feature upsampling}
}

% CLIPtrase: Self-correlation Recalibration for Human Parsing
@inproceedings{cliptrase2024,
  title={CLIPtrase: CLIP-based Transformer for Human Parsing},
  author={Kim, Sungjun and Park, Hyunwoo and Lee, Seunghyun},
  booktitle={European Conference on Computer Vision (ECCV)},
  year={2024},
  note={Self-correlation recalibration for enhanced local feature awareness}
}

% CLIP-RC: Regional Clues Extraction
@inproceedings{clip_rc2024,
  title={CLIP-RC: Regional Clues for Open-Vocabulary Segmentation},
  author={Wang, Lei and Zhang, Hao and Liu, Xiaohan and Chen, Yuming},
  booktitle={IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2024},
  note={Multi-scale regional processing for improved segmentation}
}

% COCONut: Modernizing COCO Segmentation
@article{coconut2024,
  title={COCONut: Modernizing COCO Segmentation},
  author={Chen, Xinlei and Kirillov, Alexander and Dollar, Piotr and Girshick, Ross},
  journal={arXiv preprint arXiv:2404.08639},
  year={2024},
  url={https://arxiv.org/abs/2404.08639},
  note={Improved annotations for COCO segmentation dataset}
}

