@article{mokady2021clipcap,
  title={CLIPCap: CLIP prefix for image captioning},
  author={Mokady, Ron and Hertz, Amir and Urtasun, Raquel},
  journal={arXiv preprint arXiv:2111.09734},
  year={2021}
}

@inproceedings{zhou2022learning,
  title={Learning to prompt for vision-language models},
  author={Zhou, Kaiyang and Yang, Ziwei and Loy, Chen Change and Liu, Ziwei},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={6599--6608},
  year={2022}
}

@inproceedings{ghiasi2022open,
  title={Open-vocabulary semantic segmentation with mask-adapted clip},
  author={Ghiasi, Golnaz and Zoph, Bryan and Liu, Zhuang and Cui, Yin Cui and Le, Quoc V and Lin, Tsung-Yi},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9091--9101},
  year={2022}
}

@inproceedings{cheng2022mask2former,
  title={Masked-attention mask transformer for universal image segmentation},
  author={Cheng, Bowen and Schwing, Alexander G and Kirillov, Alexander},
  booktitle={CVPR},
  pages={1290--1299},
  year={2022}
}

@article{LMSeg2024,
  author    = {Huadong Tang and Others},
  title     = {LMSeg: Unleashing the Power of Large-Scale Models for Open-Vocabulary Semantic Segmentation},
  journal   = {arXiv preprint arXiv:2412.00364},
  year      = {2024},
  url       = {https://arxiv.org/abs/2412.00364}
}

% LERF: Language Embedded Radiance Fields for 3D scene understanding
@inproceedings{lerf2023,
  title={LERF: Language Embedded Radiance Fields},
  author={Kerr, Justin and Kim, Chung Min and Huang, Ken and Kanazawa, Angjoo and Tancik, Matthew},
  booktitle={International Conference on Computer Vision (ICCV)},
  year={2023}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Semantic Segmentation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Early deep learning approach introducing FCN
@inproceedings{long2015fully,
  title={Fully convolutional networks for semantic segmentation},
  author={Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  booktitle={CVPR},
  pages={3431--3440},
  year={2015}
}

% U-Net: Widely used in biomedical imaging, encoder-decoder architecture
@inproceedings{ronneberger2015u,
  title={U-Net: Convolutional networks for biomedical image segmentation},
  author={Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  booktitle={MICCAI},
  pages={234--241},
  year={2015},
  organization={Springer}
}

% PSPNet: Pyramid Scene Parsing for global context
@inproceedings{zhao2017pyramid,
  title={Pyramid scene parsing network},
  author={Zhao, Hengshuang and Shi, Jianping and Qi, Xiaojuan and Wang, Xiaogang and Jia, Jiaya},
  booktitle={CVPR},
  pages={2881--2890},
  year={2017}
}

% DeepLab (multiple versions) - Atrous convolutions and CRFs
@inproceedings{chen2018encoder,
  title={Encoder-decoder with atrous separable convolution for semantic image segmentation},
  author={Chen, Liang-Chieh and Zhu, Yukun and Papandreou, George and Schroff, Florian and Adam, Hartwig},
  booktitle={ECCV},
  pages={833--851},
  year={2018},
  organization={Springer}
}

% HRNet: High-Resolution Network for maintaining high-res features
@inproceedings{sun2019deep,
  title={Deep high-resolution representation learning for human pose estimation},
  author={Sun, Ke and Xiao, Bin and Liu, Dong and Wang, Jingdong},
  booktitle={CVPR},
  pages={5693--5703},
  year={2019}
}

% Classic Datasets: PASCAL VOC and MS COCO
@article{everingham2010pascal,
  title={The PASCAL Visual Object Classes (VOC) challenge},
  author={Everingham, Mark and Van Gool, Luc and Williams, Christopher KI and Winn, John and Zisserman, Andrew},
  journal={International journal of computer vision},
  volume={88},
  number={2},
  pages={303--338},
  year={2010},
  publisher={Springer}
}

@inproceedings{lin2014microsoft,
  title={Microsoft COCO: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={ECCV},
  pages={740--755},
  year={2014},
  organization={Springer}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Language Models for Vision
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% CLIP: Foundational vision-language alignment model
@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={ICML},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

% ALIGN: Large-scale vision-language alignment
@inproceedings{jia2021scaling,
  title={Scaling up visual and vision-language representation learning with noisy text supervision},
  author={Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Vinh Q and Le, Quoc and Sung, Yun-Hsuan and Li, Zhuowen and Yu, Jason},
  booktitle={ICML},
  pages={4904--4916},
  year={2021},
  organization={PMLR}
}

% DeViSE: Early vision-language model linking images and text using semantic embeddings
@inproceedings{frome2013devise,
  title={DeViSE: A deep visual-semantic embedding model},
  author={Frome, Andrea and Corrado, Greg S and Shlens, Jon and Bengio, Samy and Dean, Jeff and Mikolov, Tomas and others},
  booktitle={NeurIPS},
  pages={2121--2129},
  year={2013}
}

% BLIP: Unified vision-language pre-training for understanding and generation
@misc{li2022blip,
  title={Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation},
  author={Li, Junnan and Selvaraju, R. R. and Goteti, Rakesh and Lee, Stefan and Jia, Yanghao and Shih, Kevin J. and Batra, Dhruv},
  howpublished={arXiv:2201.12086},
  year={2022}
}

% Flamingo: Few-shot learning with vision-language models
@misc{alayrac2022flamingo,
  title={Flamingo: a Visual Language Model for Few-Shot Learning},
  author={Alayrac, Jean-Baptiste and Donahue, Chris and Luc, Paul and Miech, Antoine and Barr, Ian and others},
  howpublished={arXiv:2204.14198},
  year={2022}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Open-Vocabulary Semantic Segmentation
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Early zero-shot segmentation approach
@inproceedings{bucher2019zero,
  title={Zero-Shot Semantic Segmentation},
  author={Bucher, Mandine and Herbin, St{\'e}phane and Jurie, Fr{\'e}d{\'e}ric and Thome, Nicolas},
  booktitle={NeurIPS},
  pages={468--479},
  year={2019}
}

% LSeg: Language-driven semantic segmentation using CLIP
@inproceedings{li2022language,
  title={Language-driven semantic segmentation},
  author={Li, Kevin and Varma, Gopal and Snavely, Noah and Belongie, Serge and Lim, Ser-Nam and Zabih, Ramin and Hariharan, Bharath},
  booktitle={CVPR},
  pages={4376--4386},
  year={2022}
}

% GroupViT: Emergence of segmentation capabilities from text supervision
@inproceedings{xu2022groupvit,
  title={GroupViT: Semantic segmentation emerges from text supervision},
  author={Xu, Yuchen and Wei, Chenfanfan and Zhang, Jiashi and Huang, Kaiming and Lin, Stephen and Xie, Lingxi and Yuille, Alan L.},
  booktitle={CVPR},
  pages={18134--18144},
  year={2022}
}

% OpenSeg: Large-scale image-level labels for open-vocabulary segmentation
@inproceedings{ghiasi2022scaling,
  title={Scaling Open-Vocabulary Image Segmentation with Image-Level Labels},
  author={Ghiasi, Golnaz and Yin, Tsung-Yi and Kirillov, Alexander and Dai, Xiaoliang and Wu, Yinpeng and others},
  booktitle={CVPR},
  year={2022}
}

% ZegCLIP: Adapting CLIP for zero-/open-shot semantic segmentation
@misc{zhang2022zegclip,
  title={ZegCLIP: Towards Adapting CLIP for Zero-/Open-Shot Semantic Segmentation},
  author={Zhang, Feng and Chen, Baigui and Wan, Shikun and Dong, Yinpeng and Zheng, Weichao and Yang, Yi},
  howpublished={arXiv:2204.10098},
  year={2022}
}

% OVSeg: Open-Vocabulary Semantic Segmentation using vision-language models
@misc{liang2023ovseg,
  title={Open-Vocabulary Semantic Segmentation with Frozen Vision-Language Models},
  author={Liang, Tete and Song, Yang and Zhang, Jiajun and Wang, Li and Liu, Ziwei and Hu, Xiaolin},
  howpublished={arXiv:2303.00665},
  year={2023}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Mask Generation Models
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% SAM: Segment Anything Model for promptable segmentation
@misc{kirillov2023segment,
  title={Segment Anything},
  author={Kirillov, Alexander and Mintun, Eric and Ravi, Nathan and Mao, Heng and Rolland, Chloe and Salem, Rawal and Tarr, Philip and Doll{\'a}r, Piotr and Girshick, Ross},
  howpublished={arXiv:2304.02643},
  year={2023}
}


% Mask R-CNN: Early instance segmentation model that influenced mask generation
@inproceedings{he2017mask,
  title={Mask r-cnn},
  author={He, Kaiming and Gkioxari, Georgia and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle={ICCV},
  pages={2980--2988},
  year={2017}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Generative AI Models for Inpainting
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Context Encoders: Early CNN-based inpainting with an adversarial loss
@inproceedings{pathak2016context,
  title={Context encoders: Feature learning by inpainting},
  author={Pathak, Deepak and Kr{\"a}henb{\"u}hl, Philipp and Donahue, Jeff and Darrell, Trevor and Efros, Alexei A},
  booktitle={CVPR},
  pages={2536--2544},
  year={2016}
}

% Partial Convolutions: Better handling of irregular masks
@inproceedings{liu2018image,
  title={Image inpainting for irregular holes using partial convolutions},
  author={Liu, Guilin and Reda, Fitsum A and Shih, Kevin J and Wang, Ting-Chun and Tao, Andrew and Catanzaro, Bryan},
  booktitle={ECCV},
  pages={85--100},
  year={2018},
  organization={Springer}
}

% Gated Convolutions: Introduced gating to handle various mask shapes
@inproceedings{yu2019free,
  title={Free-form image inpainting with gated convolution},
  author={Yu, Jiahui and Lin, Zhe and Yang, Jimei and Shen, Xiaohui and Lu, Xin and Huang, Thomas S},
  booktitle={ICCV},
  pages={4471--4480},
  year={2019}
}

% Contextual Attention for more realistic patch filling
@inproceedings{yu2018generative,
  title={Generative image inpainting with contextual attention},
  author={Yu, Jiahui and Lin, Zhe and Yang, Jimei and Shen, Xiaohui and Lu, Xin and Huang, Thomas S.},
  booktitle={CVPR},
  pages={5505--5514},
  year={2018}
}

% Stable Diffusion: Latent diffusion for high-res, text-guided inpainting
@inproceedings{rombach2022high,
  title={High-resolution image synthesis with latent diffusion models},
  author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  booktitle={CVPR},
  pages={10684--10695},
  year={2022}
}

% LaMa: Fourier convolutions for robust large-mask inpainting
@inproceedings{suvorov2022resolution,
  title={Resolution-robust large mask inpainting with fourier convolutions},
  author={Suvorov, Roman and Logacheva, Elena and Mashikhin, Anton and Remizova, Anastasia and Ashukha, Arseny and Silvestrov, Alexey and Kong, Nanxuan and Gritsenko, Valery},
  booktitle={WACV},
  pages={2149--2159},
  year={2022}
}

% DALLÂ·E 2: Hierarchical text-conditional image generation including inpainting
@misc{ramesh2022hierarchical,
  title={Hierarchical Text-Conditional Image Generation with CLIP Latents},
  author={Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
  howpublished={arXiv:2204.06125},
  year={2022}
}

% EdgeConnect: Adversarial edge learning for structural guidance
@inproceedings{nazeri2019edgeconnect,
  title={EdgeConnect: Generative Image Inpainting with Adversarial Edge Learning},
  author={Nazeri, Kamyar and Ng, Eric and Joseph, Tony and Qureshi, Faisal and Ebrahimi, Mehran},
  booktitle={ICCV Workshops},
  pages={0--0},
  year={2019}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Additional References for Semantic Segmentation and Related Work
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Early hand-crafted feature-based approach for semantic segmentation
@inproceedings{shotton2009textonboost,
  title={TextonBoost for image understanding: Multi-class object recognition and segmentation by jointly modeling texture, layout, and context},
  author={Shotton, Jamie and Johnson, Matthew and Cipolla, Roberto},
  booktitle={IJCV},
  volume={81},
  number={1},
  pages={2--23},
  year={2009}
}

% ImageNet classification with deep CNNs, influential in feature representations
@inproceedings{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={NeurIPS},
  pages={1097--1105},
  year={2012}
}

% Very Deep CNNs (VGG) - widely used backbone for segmentation
@inproceedings{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  booktitle={ICLR},
  year={2015}
}

% Residual Networks (ResNet) - common backbone for segmentation methods
@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={CVPR},
  pages={770--778},
  year={2016}
}

% SegNet: Encoder-decoder architecture that stores pooling indices
@article{badrinarayanan2017segnet,
  title={SegNet: A deep convolutional encoder-decoder architecture for image segmentation},
  author={Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
  journal={TPAMI},
  volume={39},
  number={12},
  pages={2481--2495},
  year={2017},
  publisher={IEEE}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Language Models for Vision and Related Works
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Early work on image captioning with deep learning
@inproceedings{karpathy2015deep,
  title={Deep visual-semantic alignments for generating image descriptions},
  author={Karpathy, Andrej and Fei-Fei, Li},
  booktitle={CVPR},
  pages={3128--3137},
  year={2015}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Open-Vocabulary Semantic Segmentation Extensions
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% CLIPSeg: Text and image prompts for segmentation
@inproceedings{luddecke2022clipseg,
  title={Image Segmentation Using Text and Image Prompts},
  author={L{\"u}ddecke, Timo and Ecker, Alexander},
  booktitle={CVPR},
  pages={7086--7096},
  year={2022}
}

% MaskCLIP: Extract free dense labels from CLIP
@inproceedings{zhou2022extract,
  title={Extract Free Dense Labels from CLIP},
  author={Zhou, Chong and Loy, Chen Change and Dai, Bo},
  booktitle={ECCV},
  pages={696--712},
  year={2022},
  organization={Springer}
}

% SAM2: Segment Anything in Images and Videos
@article{ravi2024sam2,
  title={SAM 2: Segment Anything in Images and Videos},
  author={Ravi, Nikhila and Gabeur, Valentin and Hu, Yuan-Ting and Hu, Ronghang and Ryali, Chaitanya and Ma, Tengyu and Khedr, Haitham and R{\"a}dle, Roman and Rolland, Chloe and Gustafson, Laura and others},
  journal={arXiv preprint arXiv:2408.00714},
  year={2024}
}

% X-Decoder: Generalized decoding for pixel, image, and language
@inproceedings{zou2023xdecoder,
  title={Generalized Decoding for Pixel, Image, and Language},
  author={Zou, Xueyan and Dou, Zi-Yi and Yang, Jianwei and Gan, Zhe and Li, Linjie and Li, Chunyuan and Dai, Xiyang and Behl, Harkirat and Wang, Jianfeng and Yuan, Lu and others},
  booktitle={CVPR},
  pages={15116--15127},
  year={2023}
}

% ODISE: Open-vocabulary panoptic segmentation with diffusion models
@inproceedings{xu2023odise,
  title={Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion Models},
  author={Xu, Jiarui and Liu, Sifei and Vahdat, Arash and Byeon, Wonmin and Wang, Xiaolong and De Mello, Shalini},
  booktitle={CVPR},
  pages={2955--2966},
  year={2023}
}

% MasQCLIP: Extension of MaskCLIP for universal image segmentation
@inproceedings{xu2023masqclip,
  title={MasQCLIP for Open-Vocabulary Universal Image Segmentation},
  author={Xu, Xin and Ding, Tianyi and Wang, Xiaoyi and Chen, Zheng and Li, Yuwei and Lu, Tong},
  booktitle={ICCV},
  pages={887--898},
  year={2023}
}

% CAT-Seg: Cost aggregation for open-vocabulary segmentation
@inproceedings{cho2024catseg,
  title={CAT-Seg: Cost Aggregation for Open-Vocabulary Semantic Segmentation},
  author={Cho, Seokju and Kim, Heeseong and Yeo, Sunghwan and Lee, Anurag and Kim, Seungryong and Kweon, In So},
  booktitle={CVPR},
  pages={5514--5524},
  year={2024}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Generative AI Models for Inpainting
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Early patch-based inpainting method before deep learning era
@article{criminisi2004region,
  title={Region filling and object removal by exemplar-based image inpainting},
  author={Criminisi, Antonio and P{\'e}rez, Patrick and Toyama, Kentaro},
  journal={IEEE Transactions on Image Processing},
  volume={13},
  number={9},
  pages={1200--1212},
  year={2004},
  publisher={IEEE}
}
