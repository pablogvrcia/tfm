# Open-Vocabulary Semantic Segmentation for Generative AI

Master's Thesis Implementation by Pablo GarcÃ­a GarcÃ­a

This repository implements an open-vocabulary semantic segmentation pipeline that integrates SAM 2, CLIP, and Stable Diffusion for flexible, language-driven image understanding and manipulation.

## ğŸ¯ What is This?

This system allows you to:
- **Segment any object** using natural language descriptions (even objects never seen during training)
- **Remove objects** from images realistically
- **Replace objects** with AI-generated alternatives
- **Apply style transfer** to specific regions

**Example:** You can say "the red car in the background" and the system will find it, segment it, and remove/replace it - all without any manual selection!

## ğŸ—ï¸ Architecture

The pipeline combines four state-of-the-art models:

1. **SAM 2** (Meta): Generates high-quality segmentation masks
2. **CLIP** (OpenAI): Aligns visual regions with text descriptions
3. **Mask Alignment**: Scores masks based on semantic similarity
4. **Stable Diffusion v2**: Performs realistic inpainting and generation

```
Input Image + Text Prompt
    â†“
SAM 2: Generate 100-300 mask candidates
    â†“
CLIP: Extract dense vision-language features
    â†“
Alignment: Score masks against text prompt
    â†“
Select top-K masks
    â†“
Stable Diffusion: Edit selected regions
    â†“
Output Image
```

## ğŸš€ Quick Start

### Option 1: Automated Setup (Recommended)

```bash
cd /home/pablo/tfm/code
./setup.sh
```

This will:
- Create a virtual environment
- Install all dependencies
- Download SAM 2 checkpoints
- Verify everything works

### Option 2: Manual Setup

```bash
# Create virtual environment
python3 -m venv venv
source venv/bin/activate

# Install dependencies
pip install --upgrade pip
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
pip install -r requirements.txt

# Download SAM 2 checkpoints
python scripts/download_sam2_checkpoints.py --model sam2_hiera_large
```

See [SETUP.md](SETUP.md) for detailed instructions.

## ğŸ“¦ Requirements

- **GPU**: NVIDIA GTX 1060 6GB or better
- **CUDA**: 11.8 or 12.x
- **Python**: 3.10+
- **Disk Space**: ~20GB (for models and checkpoints)
- **RAM**: 16GB recommended

## ğŸ¨ Usage Examples

### 1. Segment Objects

Find and highlight objects matching a text description:

```bash
python main.py \
  --image photo.jpg \
  --prompt "person wearing red jacket" \
  --mode segment \
  --top-k 5 \
  --visualize
```

**Output**: `output/segmentation.png` with top 5 matching masks highlighted

### 2. Remove Objects

Remove objects from the scene:

```bash
python main.py \
  --image photo.jpg \
  --prompt "traffic cone" \
  --mode remove \
  --visualize
```

**Output**: `output/edited.png` with the object removed and background filled naturally

### 3. Replace Objects

Replace one object with another:

```bash
python main.py \
  --image photo.jpg \
  --prompt "old television" \
  --mode replace \
  --edit "modern 4K smart TV" \
  --visualize
```

**Output**: `output/edited.png` with the old TV replaced by a modern one

### 4. Style Transfer

Apply artistic styles to specific regions:

```bash
python main.py \
  --image photo.jpg \
  --prompt "building facade" \
  --mode style \
  --edit "impressionist painting style, like Monet" \
  --visualize
```

**Output**: `output/edited.png` with the building rendered in impressionist style

### 5. Benchmark Performance

Test the pipeline's performance:

```bash
python main.py --image photo.jpg --mode benchmark
```

## âš™ï¸ Configuration

Three quality presets are available:

| Preset | Speed | Quality | Best For |
|--------|-------|---------|----------|
| `fast` | Fastest | Good | Testing, iteration |
| `balanced` | Medium | Very Good | General use (default) |
| `quality` | Slowest | Best | Final results, thesis figures |

Usage:
```bash
python main.py --image photo.jpg --prompt "dog" --config quality
```

## ğŸ“Š Performance (GTX 1060 6GB)

Expected processing times:

- **SAM 2 mask generation**: 3-6 seconds
- **CLIP alignment**: 0.2-0.5 seconds
- **Stable Diffusion inpainting**: 8-15 seconds
- **Total pipeline**: 15-30 seconds (segmentation + editing)

## ğŸ­ Project Structure

```
code/
â”œâ”€â”€ models/                    # Model implementations
â”‚   â”œâ”€â”€ sam2_segmentation.py  # SAM 2 mask generation
â”‚   â”œâ”€â”€ clip_features.py      # CLIP feature extraction
â”‚   â”œâ”€â”€ mask_alignment.py     # Mask-text alignment
â”‚   â””â”€â”€ inpainting.py         # Stable Diffusion inpainting
â”œâ”€â”€ scripts/                   # Utility scripts
â”‚   â””â”€â”€ download_sam2_checkpoints.py
â”œâ”€â”€ checkpoints/               # SAM 2 model weights
â”œâ”€â”€ output/                    # Generated results
â”œâ”€â”€ main.py                    # CLI entry point
â”œâ”€â”€ pipeline.py                # Complete pipeline
â”œâ”€â”€ config.py                  # Configuration presets
â”œâ”€â”€ utils.py                   # Helper functions
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ setup.sh                   # Automated setup script
â”œâ”€â”€ SETUP.md                   # Detailed setup guide
â””â”€â”€ README.md                  # This file
```

## ğŸ”§ Troubleshooting

### CUDA Out of Memory

```bash
# Use smaller SAM 2 model
python scripts/download_sam2_checkpoints.py --model sam2_hiera_tiny

# Or reduce image size
python main.py --image large.jpg --prompt "car" --mode segment
# Pipeline automatically resizes if needed
```

### SAM 2 Checkpoint Not Found

```bash
# Download the checkpoint
python scripts/download_sam2_checkpoints.py --model sam2_hiera_large

# Verify
ls -lh checkpoints/
```

### Slow Inference

- Use `--config fast`
- Use smaller SAM 2 model (`sam2_hiera_tiny` or `sam2_hiera_small`)
- Close other GPU applications

See [SETUP.md](SETUP.md) for more troubleshooting tips.

## ğŸ“š Thesis Context

This implementation supports the Master's thesis:

**"Open-Vocabulary Semantic Segmentation for Generative AI"**

Key contributions:
1. Integration of SAM 2, CLIP, and Stable Diffusion into unified pipeline
2. Multi-scale CLIP feature extraction strategy (+4.2% mIoU improvement)
3. Zero-shot segmentation of arbitrary objects via natural language
4. Practical system achieving 15-30s end-to-end latency

See the thesis document (in `/home/pablo/tfm/overleaf`) for:
- Detailed methodology (Chapter 3)
- Experimental results (Chapter 4)
- Ablation studies and analysis

## ğŸ“– Citation

```bibtex
@mastersthesis{garcia2025openvocab,
  title={Open-Vocabulary Semantic Segmentation for Generative AI},
  author={GarcÃ­a GarcÃ­a, Pablo},
  year={2025},
  school={Universidad de Zaragoza}
}
```

## ğŸ”— References

- **SAM 2**: [Segment Anything in Images and Videos](https://github.com/facebookresearch/segment-anything-2)
- **CLIP**: [Learning Transferable Visual Models](https://github.com/openai/CLIP)
- **Stable Diffusion**: [High-Resolution Image Synthesis](https://github.com/Stability-AI/stablediffusion)

## ğŸ“ License

This code is for academic and research purposes as part of a Master's thesis.

## ğŸ¤ Acknowledgments

- Thesis supervisors: Alejandro PÃ©rez Yus, MarÃ­a Santos Villafranca
- Universidad de Zaragoza, Escuela de IngenierÃ­a y Arquitectura
- Meta AI (SAM 2), OpenAI (CLIP), Stability AI (Stable Diffusion)

---

For detailed setup instructions, see [SETUP.md](SETUP.md)

For usage examples and troubleshooting, run `python main.py --help`
